{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=../figures/Brown_logo.svg width=50%>\n",
    "\n",
    "# Data-Driven Design & Analyses of Structures & Materials (3dasm)\n",
    "\n",
    "## Lecture 1b: Finding gradients\n",
    "\n",
    "### Suryanarayanan M. S. | <a href = \"mailto: s.manojsanu@tudelft.nl\">s.manojsanu@tudelft.nl</a>  | PhD Candidate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outline for today's lecture\n",
    "* Finite differencing\n",
    "* Symbolic differentiation\n",
    "* Automatic differentiation\n",
    "\n",
    "**References:**\n",
    "* J. R. R. Martins & Andrew Ning, Engineering Design Optimization, 2021 - Chapter-6\n",
    "* Extras:\n",
    "    * Nocedal, Jorge, and Stephen J. Wright. Numerical optimization. Springer Science & Business Media, 2006. - Chapter 8\n",
    "    * Automatic Differentiation in Machine Learning: a Survey, https://arxiv.org/abs/1502.05767"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Four main methods\n",
    "\n",
    "- Hand calculation (Analytical)\n",
    "- Finite differences \n",
    "- Symbolic differentiation \n",
    "- Automatic differentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. Hand calculations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Trusting your high-school calculus     \n",
    "        \n",
    "* Chain-rule for derivatives\n",
    "    * $y = f(x) \\Rightarrow \\frac{dy}{dx} = f^{'}(x)$\n",
    "    * $z = g(y) \\Rightarrow \\frac{dz}{dy} = g^{'}(x)$\n",
    "    <img align=right src=./data/hand_rule_graph.png width=40%>\n",
    "    * $\\frac{dz}{dx} = ?$\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{dz}{dx} = \\frac{dz}{dy} \\times \\frac{dy}{dx}$$\n",
    "<img align=center src=./data/hand_rule_sliders.png width=40%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slighly more complicated\n",
    "* $f: \\mathcal{R} \\rightarrow \\mathcal{R}^2$ - A vector valued function\n",
    "* $g: \\mathcal{R}^2 \\rightarrow \\mathcal{R}$\n",
    "<img align=center src=./data/hand_with_branches.png width=40%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{dz}{dx} = \\Big(\\frac{\\partial z}{\\partial y_1} \\times \\frac{d y_1}{dx} \\Big) + \\Big(\\frac{\\partial z}{\\partial y_2} \\times \\frac{d y_2}{dx}\\Big)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For a one layer neural network\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Input = $x$\n",
    "* Linear layer ($l$), $z = Wx + b$\n",
    "* Non-linearity, $y = \\sigma(z)$\n",
    "* MSE loss, $L = 0.5 * (y - \\bar{y})^2$$\n",
    "\n",
    "<img align=center src=./data/nn_hand.png width=40%>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise \n",
    "* Find $\\frac{\\partial L}{\\partial W}$ and $\\frac{\\partial L}{\\partial b}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Derivation:\n",
    "* $\\frac{\\partial L}{\\partial y} = (y-\\bar{y})$\n",
    "* $\\frac{dy}{dz} = \\sigma^{'}$\n",
    "* $\\frac{\\partial z}{\\partial W} = x$\n",
    "* $\\frac{\\partial z}{\\partial b} = 1$\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial W} = \\frac{\\partial L}{\\partial y} \\times \\frac{dy}{dz} \\times  \\frac{\\partial z}{\\partial W}$$\n",
    "$$= (y-\\bar{y}).\\sigma^{'}.x$$\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial b} = \\frac{\\partial L}{\\partial y} \\times \\frac{dy}{dz} \\times  \\frac{\\partial z}{\\partial b}$$\n",
    "$$= (y-\\bar{y}).\\sigma^{'}.1$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Pros:\n",
    "    * Fast\n",
    "    * Exact\n",
    "    * No special software needed\n",
    "* Cons:\n",
    "    * *Trust* your skills\n",
    "        * error-prone\n",
    "    * Time-consuming\n",
    "    * Redundancies!\n",
    "        * $(y-\\bar{y}).\\sigma^{'}.\\{\\}$ - Common"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- Formula: $$\\frac{df(x)}{dx} = \\frac{f(x + h) - f(x)}{h}$$\n",
    "- Standard way to check evey other method!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. Finite differencing\n",
    "\n",
    "* Approximate the derivative by a small change in the input\n",
    "    * Definition of derivative\n",
    "    \n",
    "**Taylor series makes a comeback**\n",
    "* From 1D Taylor series\n",
    "$$f(x + h) = f(x) + h\\frac{df}{dx} + \\mathcal{O}(h^2)$$\n",
    "$$f(x + h) - f(x) = h\\frac{df}{dx} + \\mathcal{O}(h^2)$$\n",
    "$$\\frac{df}{dx} = \\frac{f(x + h) - f(x)}{h} + \\mathcal{O}(h)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In n-dimensions**\n",
    "\n",
    "We saw that Taylor series is expanded along a direction!\n",
    "$$ f(\\vec{x} + \\alpha \\vec{p})|_{x=\\vec{x}_0} \\approx f(\\vec{x}_0) + \\alpha \\nabla f(\\vec{x}_0)^T \\vec{p} + \\frac{1} {2}\\alpha^2 \\vec{p}^T   \\mathbf{H}(\\vec{x}_0)   \\vec{p}$$\n",
    "\n",
    "For finite differencing, the directions are the unit-vectors along a coordinate axis.\n",
    "$$f(\\vec{x} + h \\hat{e}_j) = f(\\vec{x}) + h \\frac{\\partial f}{ \\partial x_j} +  \\mathcal{O}(h^2)$$\n",
    "$$\\frac{\\partial f}{ \\partial x_j} = \\frac{(\\vec{x} + h \\hat{e}_j) - f(\\vec{x})}{h} + \\mathcal{O}(h)$$\n",
    "$$ j= 1, 2, 3, ..., n$$\n",
    "where $n$ is the dimensionality of $\\vec{x}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Strictly,\n",
    "* This is the forward finite difference methods\n",
    "* This are reverse and central finite difference methods as well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes\n",
    "In 2D, it is easier to see how the first formula gets transformed into the second.\n",
    "* If $\\vec{x}  = \\begin{bmatrix} x_1  \\\\ x_2 \\end{bmatrix} \\in \\mathcal{R}^2$\n",
    "* Then we need two derivatives to form the gradient of f i.e.\n",
    "    $$\\nabla f = \\begin{bmatrix} \\frac{\\partial f}{\\partial x_1} \\\\ \\frac{\\partial f}{\\partial x_2}\\end{bmatrix}$$\n",
    "* We have two coordinate directions as well\n",
    "$$\\vec{p} = \\begin{bmatrix} 1  \\\\ 0 \\end{bmatrix} \\quad \\text{or} \\quad \\begin{bmatrix} 0  \\\\ 1 \\end{bmatrix}$$\n",
    "\n",
    "* For getting the first component of the gradient, we use the corresponding direction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pros:**\n",
    "* Simple to implement\n",
    "* Works for any function (black-box)\n",
    "* Used to check other methods\n",
    "\n",
    "**Cons:**\n",
    "- Not exact - depends on the choice of h\n",
    "    * $h$ needs to be small for accurate gradients [From definition] - Truncation error\n",
    "    * If $h$ is too small, finite precision errors creep in - Roundoff error\n",
    "- Slow - requires multiple function evaluations for a simple gradient\n",
    "    - E.g. For a neural network, we need to evaluate the function for each parameter\n",
    "        - What if we have 1 million parameters? or if the function is a simulation!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C. Symbolic differentiation\n",
    "- Use symbolic data types to represent mathematical expressions\n",
    "- Use equations of calculus\n",
    "    - Basically what you did with hand but now automatic!\n",
    "\n",
    "<img align=right src=./data/expression_swell.png width=40%>\n",
    "\n",
    "**Pros:**\n",
    "- Exact\n",
    "- The formula identifies problem structure!\n",
    "\n",
    "**Cons:**\n",
    "- Slow due to redundancies\n",
    "- Not scalable - expression swell\n",
    "- Wasteful\n",
    "    - We need the gradient's value at a point and not a formula!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## D. Automatic differentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* aka Algorithmic differentiation (or autodiff)\n",
    "* Combines the best aspect of symbolic and numerical differentiation\n",
    "* a. Use on any function (like Finite Differences)\n",
    "    - ifs, loops, conditionals, etc\n",
    "* b. Exact (like Symbolic differentiation)\n",
    "    - No truncation error\n",
    "* c. Fast and scalable (unlike both)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Core idea**\n",
    "* Every function is made from elementary operations (in a computer!).\n",
    "* We know the derivative of each elementary operation.\n",
    "* So, we can compute the derivative of the entire function by applying the chain rule repeatedly!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For implementation:\n",
    "- Function as a computational graph!\n",
    "    - Obtained usually by tracing\n",
    "- Look up the rules for each operation somewhere\n",
    "\n",
    "E.g. computational graph of our one-layer NN!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two modes of automatic differentiation: -> Traversing the graph!\n",
    "- Forward accumulation\n",
    "- Reverse accumulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analogy:\n",
    "- Computational graph is like a connected pipe system\n",
    "- Forward accumulation is like pouring water from the input to the output\n",
    "    - Put water in one of the inputs and you see its influence on all the outputs\n",
    "- Reverse accumulation is like pouring water from the output to the input\n",
    "    - Put water in one of the outputs and you see its influence on all the inputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many inputs and few outputs -> Forward accumulation\n",
    "\n",
    "Few inputs and many outputs -> Reverse accumulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mathematically\n",
    "- Notation:\n",
    "    - Upstream [dots]\n",
    "    - Downstream [bars]\n",
    "- Forward: \n",
    "    - Dual numbers!\n",
    "    - Based on JVPs\n",
    "    - Pushforward\n",
    "- Reverse:\n",
    "    - Based on VJPs\n",
    "    - Pullback\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why VJPS and JVPs matter?\n",
    "- Explain with KU=F [matrix-vector products are easier to store than matrices!] \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementation example (using JAX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The end!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
