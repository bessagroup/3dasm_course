{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=../figures/Brown_logo.svg width=50%>\n",
    "\n",
    "# Data-Driven Design & Analyses of Structures & Materials (3dasm)\n",
    "\n",
    "## Lecture 1b: Finding gradients\n",
    "\n",
    "### Suryanarayanan M. S. | <a href = \"mailto: s.manojsanu@tudelft.nl\">s.manojsanu@tudelft.nl</a>  | PhD Candidate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outline for today's lecture\n",
    "* Finite differencing\n",
    "* Symbolic differentiation\n",
    "* Automatic differentiation\n",
    "\n",
    "**References:**\n",
    "* J. R. R. Martins & Andrew Ning, Engineering Design Optimization, 2021 - Chapter-6\n",
    "* Extras:\n",
    "    * Nocedal, Jorge, and Stephen J. Wright. Numerical optimization. Springer Science & Business Media, 2006. - Chapter 8\n",
    "    * Automatic Differentiation in Machine Learning: a Survey, https://arxiv.org/abs/1502.05767"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Four main methods\n",
    "\n",
    "- Hand calculation (Analytical)\n",
    "- Finite differences \n",
    "- Symbolic differentiation \n",
    "- Automatic differentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. Hand calculations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Trusting your high-school calculus     \n",
    "        \n",
    "* Chain-rule for derivatives\n",
    "    * $y = f(x) \\Rightarrow \\frac{dy}{dx} = f^{'}(x)$\n",
    "    * $z = g(y) \\Rightarrow \\frac{dz}{dy} = g^{'}(x)$\n",
    "    <img align=right src=./data/hand_rule_graph.png width=40%>\n",
    "    * $\\frac{dz}{dx} = ?$\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{dz}{dx} = \\frac{dz}{dy} \\times \\frac{dy}{dx}$$\n",
    "<img align=center src=./data/hand_rule_sliders.png width=40%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slighly more complicated\n",
    "<img align=center src=./data/hand_with_branches.png width=40%>\n",
    "\n",
    "* $f: \\mathcal{R} \\rightarrow \\mathcal{R}^2$ - A vector valued function\n",
    "* $g: \\mathcal{R}^2 \\rightarrow \\mathcal{R}$\n",
    "\n",
    "$$\\frac{dz}{dx} = \\Big(\\frac{\\partial z}{\\partial y_1} \\times \\frac{d y_1}{dx} \\Big) + \\Big(\\frac{\\partial z}{\\partial y_2} \\times \\frac{d y_2}{dx}\\Big)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For a one layer neural network\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Input = $x$\n",
    "* Linear layer ($l$), $z = Wx + b$\n",
    "* Non-linearity, $y = \\sigma(z)$\n",
    "* MSE loss, $L = 0.5 * (y - \\bar{y})^2$$\n",
    "\n",
    "<img align=center src=./data/nn_hand.png width=40%>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise \n",
    "* Find $\\frac{\\partial L}{\\partial W}$ and $\\frac{\\partial L}{\\partial b}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Derivation:\n",
    "* $\\frac{\\partial L}{\\partial y} = (y-\\bar{y})$\n",
    "* $\\frac{dy}{dz} = \\sigma^{'}$\n",
    "* $\\frac{\\partial z}{\\partial W} = x$\n",
    "* $\\frac{\\partial z}{\\partial b} = 1$\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial W} = \\frac{\\partial L}{\\partial y} \\times \\frac{dy}{dz} \\times  \\frac{\\partial z}{\\partial W}$$\n",
    "$$= (y-\\bar{y}).\\sigma^{'}.x$$\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial b} = \\frac{\\partial L}{\\partial y} \\times \\frac{dy}{dz} \\times  \\frac{\\partial z}{\\partial b}$$\n",
    "$$= (y-\\bar{y}).\\sigma^{'}.1$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Pros:\n",
    "    * Fast\n",
    "    * Exact\n",
    "    * No special software needed\n",
    "* Cons:\n",
    "    * *Trust* your skills\n",
    "        * error-prone\n",
    "    * Time-consuming\n",
    "    * Redundancies!\n",
    "        * $(y-\\bar{y}).\\sigma^{'}.\\{\\}$ - Common"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- Formula: $$\\frac{df(x)}{dx} = \\frac{f(x + h) - f(x)}{h}$$\n",
    "- Standard way to check evey other method!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. Finite differencing\n",
    "\n",
    "* Approximate the derivative by a small change in the input\n",
    "    * Definition of derivative\n",
    "    \n",
    "**Taylor series makes a comeback**\n",
    "* From 1D Taylor series\n",
    "$$f(x + h) = f(x) + h\\frac{df}{dx} + \\mathcal{O}(h^2)$$\n",
    "$$f(x + h) - f(x) = h\\frac{df}{dx} + \\mathcal{O}(h^2)$$\n",
    "$$\\frac{df}{dx} = \\frac{f(x + h) - f(x)}{h} + \\mathcal{O}(h)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In n-dimensions**\n",
    "\n",
    "We saw that Taylor series is expanded along a direction!\n",
    "$$ f(\\vec{x} + \\alpha \\vec{p})|_{x=\\vec{x}_0} \\approx f(\\vec{x}_0) + \\alpha \\nabla f(\\vec{x}_0)^T \\vec{p} + \\frac{1} {2}\\alpha^2 \\vec{p}^T   \\mathbf{H}(\\vec{x}_0)   \\vec{p}$$\n",
    "\n",
    "For finite differencing, the directions are the unit-vectors along a coordinate axis.\n",
    "$$f(\\vec{x} + h \\hat{e}_j) = f(\\vec{x}) + h \\frac{\\partial f}{ \\partial x_j} +  \\mathcal{O}(h^2)$$\n",
    "$$\\frac{\\partial f}{ \\partial x_j} = \\frac{(\\vec{x} + h \\hat{e}_j) - f(\\vec{x})}{h} + \\mathcal{O}(h)$$\n",
    "$$ j= 1, 2, 3, ..., n$$\n",
    "where $n$ is the dimensionality of $\\vec{x}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Strictly,\n",
    "* This is the forward finite difference methods\n",
    "* This are reverse and central finite difference methods as well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes\n",
    "In 2D, it is easier to see how the first formula gets transformed into the second.\n",
    "* If $\\vec{x}  = \\begin{bmatrix} x_1  \\\\ x_2 \\end{bmatrix} \\in \\mathcal{R}^2$\n",
    "* Then we need two derivatives to form the gradient of f i.e.\n",
    "    $$\\nabla f = \\begin{bmatrix} \\frac{\\partial f}{\\partial x_1} \\\\ \\frac{\\partial f}{\\partial x_2}\\end{bmatrix}$$\n",
    "* We have two coordinate directions as well\n",
    "$$\\vec{p} = \\begin{bmatrix} 1  \\\\ 0 \\end{bmatrix} \\quad \\text{or} \\quad \\begin{bmatrix} 0  \\\\ 1 \\end{bmatrix}$$\n",
    "\n",
    "* For getting the first component of the gradient, we use the corresponding direction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pros:**\n",
    "* Simple to implement\n",
    "* Works for any function (black-box)\n",
    "* Used to check other methods\n",
    "\n",
    "**Cons:**\n",
    "- Not exact - depends on the choice of h\n",
    "    * $h$ needs to be small for accurate gradients [From definition] - Truncation error\n",
    "    * If $h$ is too small, finite precision errors creep in - Roundoff error\n",
    "- Slow - requires multiple function evaluations for a simple gradient\n",
    "    - E.g. For a neural network, we need to evaluate the function for each parameter\n",
    "        - What if we have 1 million parameters? or if the function is a simulation!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C. Symbolic differentiation\n",
    "- Use symbolic data types to represent mathematical expressions\n",
    "- Use equations of calculus\n",
    "    - Basically what you did with hand but now automatic!\n",
    "\n",
    "<img align=right src=./data/expression_swell.png width=40%>\n",
    "\n",
    "**Pros:**\n",
    "- Exact\n",
    "- The formula identifies problem structure!\n",
    "\n",
    "**Cons:**\n",
    "- Slow due to redundancies\n",
    "- Not scalable - expression swell\n",
    "- Wasteful\n",
    "    - We need the gradient's value at a point and not a formula!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## D. Automatic differentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* aka Algorithmic differentiation (or autodiff)\n",
    "* Combines the best aspect of symbolic and numerical differentiation\n",
    "* a. Use on any function (like Finite Differences)\n",
    "    - ifs, loops, conditionals, etc\n",
    "* b. Exact (like Symbolic differentiation)\n",
    "    - No truncation error\n",
    "* c. Fast and scalable (unlike both)\n",
    "* d. Modular"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Core idea**\n",
    "* Every function is made from elementary operations (in a computer!).\n",
    "* We know the derivative of each elementary operation.\n",
    "* So, we can compute the derivative of the entire function by applying the chain rule repeatedly!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For implementation:**\n",
    "<img align=right src=./data/comp_graph.png width=40%>\n",
    "- A program as an acyclic computational graph!\n",
    "    - Variables connected by operations (as nodes)\n",
    "    - Any function can be an operation\n",
    "    - Obtained usually by tracing variables\n",
    "- Look up the rules for each operation somewhere\n",
    "    - E.g. `def sin_derivative(x): return cos(x)`\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Two modes of automatic differentiation\n",
    "\n",
    "- Direction of traversing the graph\n",
    "<img align=right src=./data/forward_vs_backward.png width=40%>\n",
    "- Forward accumulation\n",
    "    - Derivatives \"flow\" along with the program execution\n",
    "    - Inputs to outputs\n",
    "- Reverse accumulation\n",
    "    - Derivative computation is done once the execution is over\n",
    "    - Similar to how we did by hand [Working backwards]\n",
    "    - Outputs to inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analogy: Thinking of autodiff as a pipe-system**\n",
    "\n",
    "<img align=right src=./data/pipe_ad.png width=20%>\n",
    "\n",
    "- Input channels ($n$)\n",
    "- Output channels ($m$)\n",
    "- Many many intermediate operations\n",
    "    - Connects the inputs to the outputs\n",
    "- Unconnected parts have no influence on one another\n",
    "    - Derivatives = 0\n",
    "- Cyclic dependencies\n",
    "    - Causes flow stagnation\n",
    "    - A big NO!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Forward-mode \n",
    "\n",
    "<img align=right src=./data/forward_1.png width=30%>\n",
    "\n",
    "* If you want to know the partial derivative w.r.t one of inputs\n",
    "    * You start from that variable\n",
    "        * aka Seeding\n",
    "    * Flow through the graph\n",
    "    * Get \"accumulated\" at the outputs\n",
    "* It does not matter how many outputs you have\n",
    "    * You get the effect of the input on all the outputs!\n",
    "* If $x_1$ is seeded, we will get ($m \\times 1$) matrix\n",
    "$$\\begin{bmatrix}\\frac{\\partial y_1}{\\partial x_1} \\\\ \\frac{\\partial y_2}{\\partial x_1} \\\\ \\frac{\\partial y_3}{\\partial x_1} \\\\ ... \\end{bmatrix}$$\n",
    "\n",
    "**We get the influence of $x_1$ on all outputs in one go!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reverse-mode\n",
    "\n",
    "<img align=right src=./data/reverse_1.png width=15%>\n",
    "\n",
    "* If you want *all* partial derivatives of *one* of the outputs!\n",
    "    * Seed the output you are interested in\n",
    "    * Flow through the graph\n",
    "    * This is essentially the gradient ($1 \\times n$) matrix\n",
    "\n",
    "$$\\begin{bmatrix}\\frac{\\partial y_1}{\\partial x_1} & \\frac{\\partial y_1}{\\partial x_2} & \\frac{\\partial y_1}{\\partial x_3} &... \\end{bmatrix}$$\n",
    "\n",
    "* It does not matter how many inputs there are\n",
    "    * You will get the effect of all of them on a single output    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mathematics behind autodiff\n",
    "\n",
    "* Consider a general function $f: \\mathcal{R}^n \\rightarrow \\mathcal{R}^m$\n",
    "    * Domain = $\\mathcal{R}^n$; $n$ inputs $\\begin{bmatrix}x_1 \\\\ x_2 \\\\... \\\\x_n \\end{bmatrix}$\n",
    "    * Range = $\\mathcal{R}^m$; $m$ outputs $\\begin{bmatrix}y_1 \\\\ y_2 \\\\... \\\\y_m \\end{bmatrix}$\n",
    "    * $\\vec{y} = f(\\vec{x})$\n",
    "* We can define the jacobian matrix ($\\mathcal{J}$) as:\n",
    "$$\\mathcal{J} = \\begin{bmatrix} \\frac{\\partial y_1}{\\partial x_1} & \\frac{\\partial y_1}{\\partial x_2} & ... & \\frac{\\partial y_1}{\\partial x_n} \\\\\n",
    "... & ... & ... & ...\\\\\n",
    "... & ... & ... & ...\\\\\n",
    "\\frac{\\partial y_m}{\\partial x_1} & \\frac{\\partial y_m}{\\partial x_2} & ... & \\frac{\\partial y_m}{\\partial x_n}\n",
    "\\end{bmatrix}_{\\quad m\\times n}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why jacobians matter?\n",
    "\n",
    "<img align=right src=./data/jacobian.png width=50%>\n",
    "\n",
    "* Forward-mode\n",
    "    * Gave us one column of the jacobian\n",
    "    * To construct the entire jacobian\n",
    "        * $n$ forward accumulation steps are needed\n",
    "        * One per input\n",
    "    * Useful when the function has more outputs than inputs\n",
    "* Reverse-mode\n",
    "    * Gave us one row of the jacobian\n",
    "    * To construct the entire jacobian\n",
    "        * $m$ reverse accumulation steps are needed\n",
    "        * One per output\n",
    "    * Useful when the function has more inputs than outputs\n",
    "    \n",
    "   \n",
    "**IN ML, most of the times, number of inputs (model parameters) >>>>> Number of outputs (loss value). So, Reverse-mode is better and is known as back propagation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autodiff's modularity\n",
    "\n",
    "<img align=right src=./data/modularity.png width=30%>\n",
    "\n",
    "* Each operation works independently\n",
    "* Each operation needs to propagate information\n",
    "    * Either from inputs to outputs (Upstream to downstream)\n",
    "    * Or from outputs to inputs (Downstream to upstream)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For forward-mode**\n",
    "\n",
    "<img align=right src=./data/tangents_primals.png width=30%>\n",
    "\n",
    "* Propagation of primals and tangents through a function\n",
    "    * Primals (Primary values)\n",
    "        * Inputs to outputs\n",
    "    * Tangents (Derivatives)\n",
    "        * Geometrically, tangent to a curve = derivative!\n",
    "        * $\\mathcal{J}_f$ is the function's jacobian at the given input and output\n",
    "* The jacobian times a vector is propagated forward\n",
    "    * This is called `jacobian-vector product` or `jvp`\n",
    "    * Jacobian times an input vector = output vector\n",
    "* This means we dont have to store anything!\n",
    "    * Forward-mode is independent of the depth of the graph\n",
    "    * E.g. Think extremely deep neural networks\n",
    "\n",
    "**For forward-mode autodiff to work, `jvp` rules have to written for all operations!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In pseudo-code\n",
    "# Some frameworks may not support forward-mode autodiff\n",
    "\n",
    "\n",
    "def unknown_function(x, y):\n",
    "    \"\"\" A black-box function.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x, y\n",
    "        Scalars, Arrays, or matrices of VALUES.\n",
    "        Unlike symbolic differentiation, remember that we work with numerical values!\n",
    "    \"\"\"\n",
    "    # Do something blackboxy!\n",
    "    ...\n",
    "    return z\n",
    "\n",
    "def unknown_function_jvp_rule(up_primals, up_tangents):\n",
    "    \"\"\" Tells the autodiff program how to differentiate the above function.\n",
    "\n",
    "    Briefly tell the software how to propagate the derivatives!\n",
    "    See how modular this is. User doesnot need to know the computational graph.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    up_primals\n",
    "        Upstream primals = Inputs to the original function\n",
    "        i.e. x & y\n",
    "    up_tangents\n",
    "        Upstream tangents = Derivative information accumulated in the primals\n",
    "        i.e. x_dot & y_dot\n",
    "        Imagine the flow of water!\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    down_primals\n",
    "        Primals to pass on downstream = Outputs of the function\n",
    "        z = unknown_function(x, y)\n",
    "    down_tangents\n",
    "        Downstream tangents = Derivative information incorporting the up_tangents\n",
    "        and the function's jacobian\n",
    "        z_dot = f(x_dot, y_dot, Jf)\n",
    "    \"\"\"\n",
    "    x, y = up_primals\n",
    "    x_dot, y_dot = up_tangents\n",
    "    down_primals = z = unknown_function(x, y)\n",
    "    # Compute the jacobian of the unknown_function =  Jf\n",
    "    down_tangents =  XXX # Compute the jvp [Jacobian times x_dot & y_dot]\n",
    "    return down_primals, down_tangents\n",
    "\n",
    "# Register the jvp rule\n",
    "\n",
    "# YOUR_FRAMEWORK.register_jvp(func=unknown_function,\n",
    "#                             jvp_rule=unknown_function_jvp_rule)\n",
    "\n",
    "# Now this function can be differentiated by the system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notes:\n",
    "* Upstream derivatives are denoted with \"dots\" notation i.e. $\\dot{\\vec{x}}$\n",
    "* Forward-mode also has a nice interpretation using *Dual numbers*\n",
    "    * Dual numbers are of the form $a + \\epsilon b$, where $a$ and $b$ $\\in \\mathcal{R}^n$\n",
    "    * $\\epsilon$ is a hypothetical number having the property $\\epsilon^2 = 0$ and   $\\epsilon \\neq 0$ [I know its weird]\n",
    "    * They are represented as $(a, b)$\n",
    "    * If you take the Taylor series expansion of any function at $a$ along $b$\n",
    "    $$f\\Big((a, b)\\Big) = f(a + \\epsilon b) = f(a) + \\epsilon b * f^{'}(a) + 0 + 0 + 0 + ... \\\\\n",
    "    = c + \\epsilon d = (c, d)\\\\\n",
    "    \\text{where,} \\; c = f(a) \\\\\n",
    "    d = b* f^{'}(a)$$\n",
    "* If $(a, b) = (\\vec{x}, \\dot{\\vec{x}})$\n",
    "    * For any function, $\\vec{y} = f(\\vec{x})$\n",
    "    * $f\\Big((\\vec{x}, \\dot{\\vec{x}})\\Big) = (\\vec{y}, \\dot{\\vec{y}})$\n",
    "    * Where, $\\dot{\\vec{y}} = \\mathcal{J}_f*\\dot{\\vec{x}}$ = JVP!\n",
    "    * i.e. Any function, evaluated on dual numbers, propagates outputs and derivatives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For Reverse-mode**\n",
    "* First, we have to go from the start to end of the graph\n",
    "    * Propagate the primals only\n",
    "* Next, we start traversing backwards\n",
    "    * Propagate the downstream derivatives [from outputs to inputs]\n",
    "    * Forward pass stores required values to help with this\n",
    "* What is needed are not `JVP` rules\n",
    "    * We need `VJP`s or `vector-jacobian` products\n",
    "    * We are \"pulling\" tangents backwards\n",
    "    * Ouput vector times the Jacobian = input vector\n",
    "* Memory scales with graph depth!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notes**\n",
    "* JVP = $\\mathcal{J} \\times \\vec{v}$, where $\\vec{v}$ is from the input of the function \n",
    "* VJP = $\\vec{w} \\times \\mathcal{J}$, where $\\vec{w}$ is from the output of the function\n",
    "    *  = $\\mathcal{J}^T \\times \\vec{w}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In pseudo-code\n",
    "\n",
    "def unknown_function(x, y):\n",
    "    \"\"\" A black-box function.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x, y\n",
    "        Scalars, Arrays, or matrices of VALUES.\n",
    "        Unlike symbolic differentiation, remember that we work with numerical values!\n",
    "    \"\"\"\n",
    "    # Do something blackboxy!\n",
    "    ...\n",
    "    return z\n",
    "\n",
    "# VJP rule - Needs two functions unlike forward-mode!\n",
    "\n",
    "def unknown_function_forward(up_primals):\n",
    "    \"\"\"Forward pass during reverse-mode.\n",
    "\n",
    "    This is very similar to the original function except that you can store (cache)\n",
    "    values for the backward pass.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    up_primals\n",
    "        The inputs of the original function\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    down_primals\n",
    "        Output of the original function\n",
    "    stuff_to_store\n",
    "        Residual values needed for backward pass\n",
    "    \"\"\"\n",
    "\n",
    "    down_primals = z = unknown_function(x, y)\n",
    "\n",
    "    stuff_to_store = (x, y)  # People call this as residual [Leftovers from evaluating the function]\n",
    "\n",
    "    return down_primals, stuff_to_store\n",
    "\n",
    "def unknown_function_backward(stored_stuff, down_tangents):\n",
    "    \"\"\"This is executed only after the entire graph has been traversed.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    stored_stuff\n",
    "        The stuff you stored during the forward pass (A long time ago)\n",
    "    down_tangents\n",
    "        Downstream tangents (or derivatives) accumulated so far\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    up_tangents\n",
    "        The derivatives propagated through the function to its inputs\n",
    "    \"\"\"\n",
    "    x, y = stored_stuff\n",
    "    z_bar = down_tangents\n",
    "\n",
    "    # We need to propagate z_bar to get x_bar and y_bar\n",
    "    x_bar = z_bar * XXX # The vector-jacobian product [w.r.t x]\n",
    "    y_bar = z_bar * YYY # The vector-jacobian product [w.r.t y]\n",
    "\n",
    "    up_tangents = (x_bar, y_bar)\n",
    "    return up_tangents\n",
    "\n",
    "\n",
    "# Register the vjp rule\n",
    "\n",
    "# YOUR_FRAMEWORK.register_vjp(func=unknown_function,\n",
    "#                             func_forward=unknown_function_forward,\n",
    "#                             func_backward=unknown_func_backward)\n",
    "\n",
    "# Now this function can be differentiated by the system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The end!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
