{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding gradients!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Four main methods\n",
    "- Hand calculation (Analytical)\n",
    "- Finite differences \n",
    "- Symbolic differentiation \n",
    "- Automatic differentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Hand calulations - Analytical derivations (5 min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Use your calculus and chain rules!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pros:\n",
    "- Exact\n",
    "- Fast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example, \n",
    "Lets calculate the gradient for a one layer neural network by hand\n",
    "\n",
    "$$output = \\sigma(wx + b)$$\n",
    "$$Loss, L = 0.5 * (output - target)**2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cons:\n",
    "- Error prone\n",
    "- Time consuming\n",
    "- Not scalable [Redundancies]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Finite differences (10 min)\n",
    "\n",
    "- Approximate the derivative by a small change in the input\n",
    "- Formula: $$\\frac{df(x)}{dx} = \\frac{f(x + h) - f(x)}{h}$$\n",
    "- Standard way to check evey other method!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pros:\n",
    "- Simple to implement\n",
    "- Works for any function\n",
    "\n",
    "Cons:\n",
    "- Not exact - depends on the choice of h - truncation error vs roundoff error - Taylor series!\n",
    "- Slow - requires multiple function evaluations\n",
    "    - E.g. For the neural nwtwork example, we need to evaluate the function for each parameter\n",
    "    - What if we have 1 million parameters? or if the function is a simulation!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. Symbolic differentiation (5 min)\n",
    "- Use symbolic data types to represent mathematical expressions\n",
    "- Use equations of calculus -\n",
    "- Basically what you did with hand but now automatic!\n",
    "\n",
    "Pros:\n",
    "- Exact\n",
    "- The formula identifies problem structure!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cons:\n",
    "- Slow due to redundcies\n",
    "- Not scalable - expression swell\n",
    "- Wasteful - We only the the value of the gradient at a point and not the entre expression!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D. Automatic differentiation (20 min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Combine the best aspect of symbolic and numerical differentiation\n",
    "- a. Use on any function (like Finite Differences)\n",
    "    - ifs, loops, conditionals, etc\n",
    "- b. Exact (like Symbolic differentiation)\n",
    "    - No truncation error\n",
    "- c. Fast and scalable (unlike both)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Core idea: Every function is made from elementary operations (in a computer!). We know the derivative of each elementary operation. So, we can compute the derivative of the entire function by applying the chain rule repeatedly!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For implementation:\n",
    "- Function as a computational graph!\n",
    "    - Obtained usually by tracing\n",
    "- Look up the rules for each operation somewhere\n",
    "\n",
    "E.g. computational graph of our one-layer NN!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two modes of automatic differentiation: -> Traversing the graph!\n",
    "- Forward accumulation\n",
    "- Reverse accumulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analogy:\n",
    "- Computational graph is like a connected pipe system\n",
    "- Forward accumulation is like pouring water from the input to the output\n",
    "    - Put water in one of the inputs and you see its influence on all the outputs\n",
    "- Reverse accumulation is like pouring water from the output to the input\n",
    "    - Put water in one of the outputs and you see its influence on all the inputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many inputs and few outputs -> Forward accumulation\n",
    "\n",
    "Few inputs and many outputs -> Reverse accumulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mathematically\n",
    "- Notation:\n",
    "    - Upstream [dots]\n",
    "    - Downstream [bars]\n",
    "- Forward: \n",
    "    - Dual numbers!\n",
    "    - Based on JVPs\n",
    "    - Pushforward\n",
    "- Reverse:\n",
    "    - Based on VJPs\n",
    "    - Pullback\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why VJPS and JVPs matter?\n",
    "- Explain with KU=F [matrix-vector products are easier to store than matrices!] \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementation example (using JAX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The end!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
