{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=../figures/Brown_logo.svg width=50%>\n",
    "\n",
    "## Data-Driven Design & Analyses of Structures & Materials (3dasm)\n",
    "\n",
    "## Lecture 13\n",
    "\n",
    "### Miguel A. Bessa | <a href = \"mailto: miguel_bessa@brown.edu\">miguel_bessa@brown.edu</a>  | Associate Professor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "**What:** A lecture of the \"3dasm\" course\n",
    "\n",
    "**Where:** This notebook comes from this [repository](https://github.com/bessagroup/3dasm_course)\n",
    "\n",
    "**Reference for entire course:** Murphy, Kevin P. *Probabilistic machine learning: an introduction*. MIT press, 2022. Available online [here](https://probml.github.io/pml-book/book1.html)\n",
    "\n",
    "**How:** We try to follow Murphy's book closely, but the sequence of Chapters and Sections is different. The intention is to use notebooks as an introduction to the topic and Murphy's book as a resource.\n",
    "* If working offline: Go through this notebook and read the book.\n",
    "* If attending class in person: listen to me (!) but also go through the notebook in your laptop at the same time. Read the book.\n",
    "* If attending lectures remotely: listen to me (!) via Zoom and (ideally) use two screens where you have the notebook open in 1 screen and you see the lectures on the other. Read the book."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "**Optional reference (the \"bible\" by the \"bishop\"... pun intended ðŸ˜†) :** Bishop, Christopher M. *Pattern recognition and machine learning*. Springer Verlag, 2006.\n",
    "\n",
    "**References/resources to create this notebook:**\n",
    "* This simple tutorial is still based on a script I created for this article: https://imechanica.org/node/23957\n",
    "* It follows from some examples provided by the scikit-learn user guide, which seem to have originated from Mathieu Blondel, Jake Vanderplas, Vincent Dubourg, and Jan Hendrik Metzen.\n",
    "\n",
    "Apologies in advance if I missed some reference used in this notebook. Please contact me if that is the case, and I will gladly include it here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "## **OPTION 1**. Run this notebook **locally in your computer**:\n",
    "1. Confirm that you have the '3dasm' mamba (or conda) environment (see Lecture 1).\n",
    "2. Go to the 3dasm_course folder in your computer and pull the last updates of the [repository](https://github.com/bessagroup/3dasm_course):\n",
    "```\n",
    "git pull\n",
    "```\n",
    "    - Note: if you can't pull the repo due to conflicts (and you can't handle these conflicts), use this command (with **caution**!) and your repo becomes the same as the one online:\n",
    "        ```\n",
    "        git reset --hard origin/main\n",
    "        ```\n",
    "3. Open command window and load jupyter notebook (it will open in your internet browser):\n",
    "```\n",
    "jupyter notebook\n",
    "```\n",
    "5. Open notebook of this Lecture and choose the '3dasm' kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "## **OPTION 2**. Use **Google's Colab** (no installation required, but times out if idle):\n",
    "\n",
    "1. go to https://colab.research.google.com\n",
    "2. login\n",
    "3. File > Open notebook\n",
    "4. click on Github (no need to login or authorize anything)\n",
    "5. paste the git link: https://github.com/bessagroup/3dasm_course\n",
    "6. click search and then click on the notebook for this Lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": true,
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "# Basic plotting tools needed in Python.\n",
    "\n",
    "import matplotlib.pyplot as plt # import plotting tools to create figures\n",
    "import numpy as np # import numpy to handle a lot of things!\n",
    "from IPython.display import display, Math # to print with Latex math\n",
    "\n",
    "%config InlineBackend.figure_format = \"retina\" # render higher resolution images in the notebook\n",
    "plt.rcParams[\"figure.figsize\"] = (8,4) # rescale figure size appropriately for slides"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Outline for today\n",
    "\n",
    "* Introduction to Gaussian Processes\n",
    "    - Using Scikit-learn for Gaussian Process Regression (noiseless and noisy datasets)\n",
    "\n",
    "**Reading material**: This notebook + (GPs in Section 17.2 of book)\n",
    "\n",
    "**Optional reading material (GPs bible)**: Rasmussen, Carl E. *Gaussian Processes for Machine Learning*. MIT press, 2006. Available online [here](https://gaussianprocess.org/gpml/chapters/RW.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Gaussian processes\n",
    "\n",
    "Gaussian processes are a type of machine learning algorithm categorized as a kernel method.\n",
    "\n",
    "Kernel machines can be very powerful, but also have important limitations..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Understanding Gaussian Processes\n",
    "\n",
    "We will derive Gaussian Processes (GPs) in weight-space, instead of function-space (the results are the same).\n",
    "\n",
    "* This way, we will see that Gaussian Processes are a very natural generalization of Bayesian linear regression!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Recap of PPD for Bayesian linear regression\n",
    "\n",
    "Last lecture we found the PPD of Bayesian linear regression to be:\n",
    "\n",
    "$$\\require{color}\n",
    "{\\color{orange}p(y^*|\\mathbf{x}^*, \\mathcal{D})} = \\mathcal{N}\\left(y^* \\mid \\boldsymbol{\\phi}(\\mathbf{x}^*)^T \\overset{\\scriptscriptstyle >}{\\boldsymbol{\\Sigma}}_w \\left( \\overset{\\scriptscriptstyle <}{\\boldsymbol{\\Sigma}}_w^{-1} \\overset{\\scriptscriptstyle <}{\\boldsymbol{\\mu}}_w + \\frac{1}{\\sigma^2}\\boldsymbol{\\Phi}^T\\mathbf{y}\\right) \\,,\\, \\sigma^2 + \\boldsymbol{\\phi}(\\mathbf{x}^*)^T \\overset{\\scriptscriptstyle >}{\\boldsymbol{\\Sigma}}_w \\boldsymbol{\\phi}(\\mathbf{x}^*)\\right)\n",
    "$$\n",
    "\n",
    "with the previously determined covariance: $\\overset{\\scriptscriptstyle >}{\\boldsymbol{\\Sigma}}_w = \\left( \\overset{\\scriptscriptstyle <}{\\boldsymbol{\\Sigma}}_w^{-1} + \\frac{1}{\\sigma^2}\\boldsymbol{\\Phi}^T\\boldsymbol{\\Phi}\\right)^{-1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "This long expression of the PPD is **often simplified** by considering a prior with zero mean, i.e. $\\overset{\\scriptscriptstyle <}{\\boldsymbol{\\mu}}_w = \\mathbf{0}$:\n",
    "\n",
    "$$\n",
    "{\\color{orange}p(y^*|\\mathbf{x}^*, \\mathcal{D})} = \\mathcal{N}\\left(y^* \\mid \\boldsymbol{\\phi}(\\mathbf{x}^*)^T \\overset{\\scriptscriptstyle >}{\\boldsymbol{\\Sigma}}_w \\frac{1}{\\sigma^2}\\boldsymbol{\\Phi}^T\\mathbf{y} \\,,\\, \\sigma^2 + \\boldsymbol{\\phi}(\\mathbf{x}^*)^T \\overset{\\scriptscriptstyle >}{\\boldsymbol{\\Sigma}}_w \\boldsymbol{\\phi}(\\mathbf{x}^*)\\right)\n",
    "$$\n",
    "\n",
    "We will consider this simplification, without loss of generality (it makes the expressions slightly shorter)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Furthermore, we will also abbreviate the PPD expression by using the notation:\n",
    "\n",
    "$\\boldsymbol{\\phi}^*=\\boldsymbol{\\phi}(\\mathbf{x}^*)$\n",
    "\n",
    "from which the PPD can be rewritten as:\n",
    "\n",
    "$$\n",
    "{\\color{orange}p(y^*|\\mathbf{x}^*, \\mathcal{D})} = \\mathcal{N}\\left(y^* \\mid {\\boldsymbol{\\phi}^*}^T \\overset{\\scriptscriptstyle >}{\\boldsymbol{\\Sigma}}_w \\frac{1}{\\sigma^2}\\boldsymbol{\\Phi}^T\\mathbf{y} \\,,\\, \\sigma^2 + {\\boldsymbol{\\phi}^*}^T \\overset{\\scriptscriptstyle >}{\\boldsymbol{\\Sigma}}_w \\boldsymbol{\\phi}^*\\right)\n",
    "$$\n",
    "\n",
    "with the previously determined covariance: $\\overset{\\scriptscriptstyle >}{\\boldsymbol{\\Sigma}}_w = \\left( \\overset{\\scriptscriptstyle <}{\\boldsymbol{\\Sigma}}_w^{-1} + \\frac{1}{\\sigma^2}\\boldsymbol{\\Phi}^T\\boldsymbol{\\Phi}\\right)^{-1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Also recall that $\\boldsymbol{\\Phi}$ is where we group all $N$ evaluations of the basis functions into the $N\\times M$ matrix:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{\\Phi} = \\begin{bmatrix} \\phi_0(\\mathbf{x}_1) & \\phi_1(\\mathbf{x}_1) & \\cdots & \\phi_{M-1}(\\mathbf{x}_1) \\\\\n",
    "\\phi_0(\\mathbf{x}_2) & \\phi_1(\\mathbf{x}_2) & \\cdots & \\phi_{M-1}(\\mathbf{x}_2) \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "\\phi_0(\\mathbf{x}_N) & \\phi_1(\\mathbf{x}_N) & \\cdots & \\phi_{M-1}(\\mathbf{x}_N) \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "where $N$ is the number of training points and $M$ is the number of basis functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Moreover, if we define the matrix:\n",
    "\n",
    "$\\mathbf{K}=\\boldsymbol{\\Phi} \\overset{\\scriptscriptstyle <}{\\boldsymbol{\\Sigma}}_w\\boldsymbol{\\Phi}^T$\n",
    "\n",
    "We can show that the PPD can be rewritten as:\n",
    "\n",
    "$$\n",
    "{\\color{orange}p(y^*|\\mathbf{x}^*, \\mathcal{D})} = \\mathcal{N}\\left(y^* \\mid {\\boldsymbol{\\phi}^*}^T \\overset{\\scriptscriptstyle <}{\\boldsymbol{\\Sigma}}_w \\boldsymbol{\\Phi}^T(\\mathbf{K}+\\sigma^2\\mathbf{I}_N)^{-1}\\mathbf{y} \\,,\\, \\sigma^2 + {\\boldsymbol{\\phi}^*}^T \\overset{\\scriptscriptstyle <}{\\boldsymbol{\\Sigma}}_w \\boldsymbol{\\phi}^* - {\\boldsymbol{\\phi}^*}^T \\overset{\\scriptscriptstyle <}{\\boldsymbol{\\Sigma}}_w \\boldsymbol{\\Phi}^T (\\mathbf{K}+\\sigma^2\\mathbf{I}_N)^{-1} \\boldsymbol{\\Phi}\\overset{\\scriptscriptstyle <}{\\boldsymbol{\\Sigma}}_w \\boldsymbol{\\phi}^* \\right)\n",
    "$$\n",
    "\n",
    "I know... You don't think that this simplified anything... Be patient!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Equivalency of these two PPD expressions for Bayesian linear regression\n",
    "\n",
    "**The notes below** show that the PPD we found in the last lecture for Bayesian linear regression:\n",
    "\n",
    "$$\n",
    "{\\color{orange}p(y^*|\\mathbf{x}^*, \\mathcal{D})} = \\mathcal{N}\\left(y^* \\mid {\\boldsymbol{\\phi}^*}^T \\overset{\\scriptscriptstyle >}{\\boldsymbol{\\Sigma}}_w \\frac{1}{\\sigma^2}\\boldsymbol{\\Phi}^T\\mathbf{y} \\,,\\, \\sigma^2 + {\\boldsymbol{\\phi}^*}^T \\overset{\\scriptscriptstyle >}{\\boldsymbol{\\Sigma}}_w \\boldsymbol{\\phi}^*\\right)\n",
    "$$\n",
    "\n",
    "with the previously determined covariance: $\\overset{\\scriptscriptstyle >}{\\boldsymbol{\\Sigma}}_w = \\left( \\overset{\\scriptscriptstyle <}{\\boldsymbol{\\Sigma}}_w^{-1} + \\frac{1}{\\sigma^2}\\boldsymbol{\\Phi}^T\\boldsymbol{\\Phi}\\right)^{-1}$\n",
    "\n",
    "is equivalent to the following expression (shown in 2 lines due to length):\n",
    "\n",
    "$$\\begin{align}\n",
    "{\\color{orange}p(y^*| \\mathbf{x}^*, \\mathcal{D})} = \\mathcal{N}(y^* \\mid & {\\boldsymbol{\\phi}^*}^T \\overset{\\scriptscriptstyle <}{\\boldsymbol{\\Sigma}}_w \\boldsymbol{\\Phi}^T(\\mathbf{K}+\\sigma^2\\mathbf{I}_N)^{-1}\\mathbf{y} \\,, \\\\\n",
    "& \\sigma^2 + {\\boldsymbol{\\phi}^*}^T \\overset{\\scriptscriptstyle <}{\\boldsymbol{\\Sigma}}_w \\boldsymbol{\\phi}^* - {\\boldsymbol{\\phi}^*}^T \\overset{\\scriptscriptstyle <}{\\boldsymbol{\\Sigma}}_w \\boldsymbol{\\Phi}^T (\\mathbf{K}+\\sigma^2\\mathbf{I}_N)^{-1} \\boldsymbol{\\Phi}\\overset{\\scriptscriptstyle <}{\\boldsymbol{\\Sigma}}_w \\boldsymbol{\\phi}^* )\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where $\\mathbf{K}=\\boldsymbol{\\Phi} \\overset{\\scriptscriptstyle <}{\\boldsymbol{\\Sigma}}_w\\boldsymbol{\\Phi}^T$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "### Proof of PPD mean equivalency\n",
    "\n",
    "We want to show that the two expressions for the **mean** of the PPD are equivalent, i.e.:\n",
    "\n",
    "$$\n",
    "\\overset{\\scriptscriptstyle <}{\\boldsymbol{\\Sigma}}_w \\boldsymbol{\\Phi}^T(\\mathbf{K}+\\sigma^2\\mathbf{I}_N)^{-1} \\equiv \\overset{\\scriptscriptstyle >}{\\boldsymbol{\\Sigma}}_w \\frac{1}{\\sigma^2}\\boldsymbol{\\Phi}^T\n",
    "$$\n",
    "\n",
    "If you multiply from the left by $\\overset{\\scriptscriptstyle >}{\\boldsymbol{\\Sigma}}_w$, and from the right by $(\\mathbf{K}+\\sigma^2\\mathbf{I}_N)$ we get:\n",
    "\n",
    "$$\n",
    "\\overset{\\scriptscriptstyle >}{\\boldsymbol{\\Sigma}}_w^{-1}\\overset{\\scriptscriptstyle <}{\\boldsymbol{\\Sigma}}_w \\boldsymbol{\\Phi}^T \\equiv \\frac{1}{\\sigma^2}\\boldsymbol{\\Phi}^T (\\mathbf{K}+\\sigma^2\\mathbf{I}_N)\n",
    "$$\n",
    "\n",
    "Focusing on the LHS and knowing that: $\\overset{\\scriptscriptstyle >}{\\boldsymbol{\\Sigma}}_w^{-1} = \\left[ \\left( \\overset{\\scriptscriptstyle <}{\\boldsymbol{\\Sigma}}_w^{-1} + \\frac{1}{\\sigma^2}\\boldsymbol{\\Phi}^T\\boldsymbol{\\Phi}\\right)^{-1} \\right]^{-1} = \\overset{\\scriptscriptstyle <}{\\boldsymbol{\\Sigma}}_w^{-1} + \\frac{1}{\\sigma^2}\\boldsymbol{\\Phi}^T\\boldsymbol{\\Phi}$\n",
    "\n",
    "$$\n",
    "\\left(\\overset{\\scriptscriptstyle <}{\\boldsymbol{\\Sigma}}_w^{-1} + \\frac{1}{\\sigma^2}\\boldsymbol{\\Phi}^T\\boldsymbol{\\Phi}\\right) \\overset{\\scriptscriptstyle <}{\\boldsymbol{\\Sigma}}_w \\boldsymbol{\\Phi}^T \\equiv \\frac{1}{\\sigma^2}\\boldsymbol{\\Phi}^T (\\mathbf{K}+\\sigma^2\\mathbf{I}_N)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\underbrace{\\boldsymbol{\\Phi}^T}_{\\boldsymbol{\\Phi}^T\\frac{1}{\\sigma^2}\\sigma^2\\mathbf{I}_N} + \\frac{1}{\\sigma^2}\\boldsymbol{\\Phi}^T\\underbrace{\\boldsymbol{\\Phi}\\overset{\\scriptscriptstyle <}{\\boldsymbol{\\Sigma}}_w \\boldsymbol{\\Phi}^T}_{\\mathbf{K}}  \\equiv \\frac{1}{\\sigma^2}\\boldsymbol{\\Phi}^T (\\mathbf{K}+\\sigma^2\\mathbf{I}_N)\n",
    "\\quad \\text{q.e.d.}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "### Proof of PPD variance equivalency\n",
    "\n",
    "We want to show that the two expressions for the **variance** of the PPD are equivalent, i.e.:\n",
    "\n",
    "$$\n",
    "\\sigma^2 + {\\boldsymbol{\\phi}^*}^T \\overset{\\scriptscriptstyle <}{\\boldsymbol{\\Sigma}}_w \\boldsymbol{\\phi}^* - {\\boldsymbol{\\phi}^*}^T \\overset{\\scriptscriptstyle <}{\\boldsymbol{\\Sigma}}_w \\boldsymbol{\\Phi}^T (\\mathbf{K}+\\sigma^2\\mathbf{I}_N)^{-1} \\boldsymbol{\\Phi}\\overset{\\scriptscriptstyle <}{\\boldsymbol{\\Sigma}}_w \\boldsymbol{\\phi}^* \\equiv \\sigma^2 + {\\boldsymbol{\\phi}^*}^T \\overset{\\scriptscriptstyle >}{\\boldsymbol{\\Sigma}}_w \\boldsymbol{\\phi}^*\n",
    "$$\n",
    "\n",
    "We can prove this from teh matrix inversion Lemma defined for matrices $\\mathbf{Z}$, $\\mathbf{U}$, $\\mathbf{W}$, and $\\mathbf{V}$:\n",
    "\n",
    "$$\n",
    "\\left(\\mathbf{Z}+\\mathbf{U}\\mathbf{W}\\mathbf{V}^T\\right)^{-1} = \\mathbf{Z}^{-1} - \\mathbf{Z}^{-1}\\mathbf{U}\\left(\\mathbf{W}^{-1} + \\mathbf{V}^T\\mathbf{Z}^{-1}\\mathbf{U} \\right)^{-1}\\mathbf{V}^T\\mathbf{Z}^{-1}\n",
    "$$\n",
    "\n",
    "So, if we consider $\\mathbf{Z}^{-1}=\\overset{\\scriptscriptstyle <}{\\boldsymbol{\\Sigma}}_w$, $\\mathbf{W}^{-1}=\\sigma^2\\mathbf{I}_N$ and $\\mathbf{V}=\\mathbf{U}=\\boldsymbol{\\Phi}^T$ then we get:\n",
    "\n",
    "$$\n",
    "\\left(\\underbrace{\\overset{\\scriptscriptstyle <}{\\boldsymbol{\\Sigma}}_w^{-1} + \\frac{1}{\\sigma^2}\\boldsymbol{\\Phi}^T\\boldsymbol{\\Phi}}_{\\overset{\\scriptscriptstyle >}{\\boldsymbol{\\Sigma}}_w^{-1}}\\right)^{-1} = \\overset{\\scriptscriptstyle <}{\\boldsymbol{\\Sigma}}_w - \\overset{\\scriptscriptstyle <}{\\boldsymbol{\\Sigma}}_w \\boldsymbol{\\Phi}^T \\left(\\sigma^2\\mathbf{I}_N + \\underbrace{\\boldsymbol{\\Phi}\\overset{\\scriptscriptstyle <}{\\boldsymbol{\\Sigma}}_w \\boldsymbol{\\Phi}^T}_{\\mathbf{K}} \\right)^{-1} \\boldsymbol{\\Phi} \\overset{\\scriptscriptstyle <}{\\boldsymbol{\\Sigma}}_w\n",
    "$$\n",
    "\n",
    "Thus, if we multiply on the left by ${\\boldsymbol{\\phi}^*}^T$ and on the right by $\\boldsymbol{\\phi}^*$ for both sides we get:\n",
    "\n",
    "$$\n",
    "{\\boldsymbol{\\phi}^*}^T \\overset{\\scriptscriptstyle >}{\\boldsymbol{\\Sigma}}_w \\boldsymbol{\\phi}^*\\equiv {\\boldsymbol{\\phi}^*}^T \\overset{\\scriptscriptstyle <}{\\boldsymbol{\\Sigma}}_w \\boldsymbol{\\phi}^* - {\\boldsymbol{\\phi}^*}^T \\overset{\\scriptscriptstyle <}{\\boldsymbol{\\Sigma}}_w \\boldsymbol{\\Phi}^T (\\mathbf{K}+\\sigma^2\\mathbf{I}_N)^{-1} \\boldsymbol{\\Phi}\\overset{\\scriptscriptstyle <}{\\boldsymbol{\\Sigma}}_w \\boldsymbol{\\phi}^*\n",
    "\\quad \\text{q.e.d.}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## This result is incredibly powerful!\n",
    "\n",
    "Rewriting the PPD for Bayesian linear regression into this form:\n",
    "\n",
    "$$\\begin{align}\n",
    "{\\color{orange}p(y^*|\\mathbf{x}^*, \\mathcal{D})} = \\mathcal{N}( y^* \\mid &\n",
    "{\\boldsymbol{\\phi}^*}^T \\overset{\\scriptscriptstyle <}{\\boldsymbol{\\Sigma}}_w \\boldsymbol{\\Phi}^T(\\mathbf{K}+\\sigma^2\\mathbf{I}_N)^{-1}\\mathbf{y} \\,, \\\\\n",
    "& \\, \\sigma^2 + {\\boldsymbol{\\phi}^*}^T \\overset{\\scriptscriptstyle <}{\\boldsymbol{\\Sigma}}_w \\boldsymbol{\\phi}^* - {\\boldsymbol{\\phi}^*}^T \\overset{\\scriptscriptstyle <}{\\boldsymbol{\\Sigma}}_w \\boldsymbol{\\Phi}^T (\\mathbf{K}+\\sigma^2\\mathbf{I}_N)^{-1} \\boldsymbol{\\Phi}\\overset{\\scriptscriptstyle <}{\\boldsymbol{\\Sigma}}_w \\boldsymbol{\\phi}^* )\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where $\\mathbf{K}=\\boldsymbol{\\Phi} \\overset{\\scriptscriptstyle <}{\\boldsymbol{\\Sigma}}_w\\boldsymbol{\\Phi}^T$,\n",
    "\n",
    "is very interesting because the following **three terms of the same form** arise:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\n",
    "\\boldsymbol{\\Phi}\\overset{\\scriptscriptstyle <}{\\boldsymbol{\\Sigma}}_w \\boldsymbol{\\Phi}^T \\quad , \\quad {\\boldsymbol{\\phi}^*}^T\\overset{\\scriptscriptstyle <}{\\boldsymbol{\\Sigma}}_w \\boldsymbol{\\Phi}^T \\quad , \\quad {\\boldsymbol{\\phi}^*}^T\\overset{\\scriptscriptstyle <}{\\boldsymbol{\\Sigma}}_w \\boldsymbol{\\phi}^*\n",
    "$$\n",
    "\n",
    "**<font color=blue>Let's pause for a minute and observe the PPD expression carefully...</font>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "For simplicity of argument (but without loss of generality), consider for the time being that $\\overset{\\scriptscriptstyle <}{\\boldsymbol{\\Sigma}}_w = \\overset{\\scriptscriptstyle <}{\\sigma}_w^2 \\mathbf{I}_N$.\n",
    "\n",
    "In this case, we can simplify each of the above terms in a very easy form:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{\\Phi}\\overset{\\scriptscriptstyle <}{\\boldsymbol{\\Sigma}}_w \\boldsymbol{\\Phi}^T = \\overset{\\scriptscriptstyle <}{\\sigma}_w^2\\boldsymbol{\\Phi} \\boldsymbol{\\Phi}^T\n",
    "$$\n",
    "\n",
    "$$\n",
    "{\\boldsymbol{\\phi}^*}^T\\overset{\\scriptscriptstyle <}{\\boldsymbol{\\Sigma}}_w \\boldsymbol{\\Phi}^T = \\overset{\\scriptscriptstyle <}{\\sigma}_w^2 {\\boldsymbol{\\phi}^*}^T \\boldsymbol{\\Phi}^T\n",
    "$$\n",
    "\n",
    "$$\n",
    "{\\boldsymbol{\\phi}^*}^T\\overset{\\scriptscriptstyle <}{\\boldsymbol{\\Sigma}}_w \\boldsymbol{\\phi}^* = \\overset{\\scriptscriptstyle <}{\\sigma}_w^2 {\\boldsymbol{\\phi}^*}^T \\boldsymbol{\\phi}^*\n",
    "$$\n",
    "\n",
    "Where we see that the covariance matrix of the prior are simple \"coefficients\" for the basis functions (hyperparameters!). This holds even if we did not simplify the covariance matrix to be diagonal, but it just makes this argument easier to make."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Therefore, we have to analyze what it means to have pairs of basis functions being multiplied together.\n",
    "\n",
    "Let's start with the term $\\overset{\\scriptscriptstyle <}{\\sigma}_w^2\\boldsymbol{\\Phi} \\boldsymbol{\\Phi}^T$. The $N\\times M$ basis function matrix is:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{\\Phi} = \\begin{bmatrix} \\phi_0(\\mathbf{x}_1) & \\phi_1(\\mathbf{x}_1) & \\cdots & \\phi_{M-1}(\\mathbf{x}_1) \\\\\n",
    "\\phi_0(\\mathbf{x}_2) & \\phi_1(\\mathbf{x}_2) & \\cdots & \\phi_{M-1}(\\mathbf{x}_2) \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "\\phi_0(\\mathbf{x}_N) & \\phi_1(\\mathbf{x}_N) & \\cdots & \\phi_{M-1}(\\mathbf{x}_N) \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "So, the matrix multiplication $\\overset{\\scriptscriptstyle <}{\\sigma}_w^2\\boldsymbol{\\Phi} \\boldsymbol{\\Phi}^T$ creates an $N \\times N$ matrix (let's call it $\\mathbf{A}=\\overset{\\scriptscriptstyle <}{\\sigma}_w^2\\boldsymbol{\\Phi} \\boldsymbol{\\Phi}^T$ for now) where each element is given by:\n",
    "\n",
    "$$\n",
    "A_{ij} = \\overset{\\scriptscriptstyle <}{\\sigma}_w^2\\phi_m(\\mathbf{x}_i)\\phi_m(\\mathbf{x}_j)\n",
    "$$\n",
    "\n",
    "Note that I am using Einstein's notation where repeated indices indicate a summation, i.e. $\\phi_m(\\mathbf{x}_i)\\phi_m(\\mathbf{x}_j) = \\sum_{m=1}^M \\phi_m(\\mathbf{x}_i)\\phi_m(\\mathbf{x}_j)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Similarly, the remaining two terms have the same construction.\n",
    "\n",
    "Focusing on the term $\\overset{\\scriptscriptstyle <}{\\sigma}_w^2{\\boldsymbol{\\phi}^*}^T \\boldsymbol{\\Phi}^T$, it leads to an $1\\times N$ vector (let's call it $\\mathbf{b} = \\overset{\\scriptscriptstyle <}{\\sigma}_w^2\\boldsymbol{\\Phi} \\boldsymbol{\\Phi}^T$ for now) whose elements are:\n",
    "\n",
    "$$\n",
    "b_j = \\overset{\\scriptscriptstyle <}{\\sigma}_w^2 \\phi_m(\\mathbf{x}^*)\\phi_m(\\mathbf{x}_j) \\quad \\text{where } j=1,..., N \\text{ and } m=0,\\ldots,M-1\n",
    "$$\n",
    "\n",
    "and where you should recall that $\\mathbf{x}^*$ highlights that this is a point not seen in training (point where you want to make a prediction).\n",
    "\n",
    "And, finally, the last term $\\overset{\\scriptscriptstyle <}{\\sigma}_w^2 {\\boldsymbol{\\phi}^*}^T \\boldsymbol{\\phi}^*$ leads to a scalar (let's call it $c=\\overset{\\scriptscriptstyle <}{\\sigma}_w^2 {\\boldsymbol{\\phi}^*}^T \\boldsymbol{\\phi}^*$ for now) given by:\n",
    "\n",
    "$$\n",
    "c = \\overset{\\scriptscriptstyle <}{\\sigma}_w^2 \\phi_m(\\mathbf{x}^*)\\phi_m(\\mathbf{x}^*) \\quad \\text{where } m=0,\\ldots,M-1\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Hello kernels!\n",
    "\n",
    "So, we are seeing that all these terms that are in the PPD of Bayesian linear regression:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{\\Phi}\\overset{\\scriptscriptstyle <}{\\boldsymbol{\\Sigma}}_w \\boldsymbol{\\Phi}^T \\quad , \\quad {\\boldsymbol{\\phi}^*}^T\\overset{\\scriptscriptstyle <}{\\boldsymbol{\\Sigma}}_w \\boldsymbol{\\Phi}^T \\quad , \\quad {\\boldsymbol{\\phi}^*}^T\\overset{\\scriptscriptstyle <}{\\boldsymbol{\\Sigma}}_w \\boldsymbol{\\phi}^*\n",
    "$$\n",
    "\n",
    "are associated to a <font color='red'>single mathematical entity</font>:\n",
    "\n",
    "$$\n",
    "k(\\mathbf{x}_i,\\mathbf{x}_j) = \\sigma_{ij}^2\\phi_m(\\mathbf{x}_i)\\phi_m(\\mathbf{x}_j)  \\quad \\text{where } m=0,\\ldots,M-1\n",
    "$$\n",
    "    \n",
    "which is called a **kernel function** (a.k.a. Mercer kernel)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "For example, if we consider $N$ input points $x_n$ **in one-dimension**, if we choose a quadratic polynomial basis function without the bias term, $\\boldsymbol{\\phi}(x_n)=[x_n , x_n^2]$, and assume the hyperparameter $\\sigma_{ij}=1$, then we construct a quadratic kernel:\n",
    "\n",
    "$$\n",
    "k(x_i,x_j) = \\phi_m(x_i)\\phi_m(x_j) = x_i^2 x_j^2 + 2 x_i x_j\n",
    "$$\n",
    "\n",
    "for every pair of points $x_i$ and $x_j$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "At this point you may say: \"What the hell, Miguel? Why do we care???\"\n",
    "\n",
    "* I admit it, if you just use the same basis functions (polynomial with a given degree, i.e. a finite $M$), then writing Bayesian linear regression using the \"kernel\" formalism can be a waste of time..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "BUT, here's something that will blow your mind ðŸ¤¯:\n",
    "\n",
    "* Ask yourself this question: do you know what happens if we make $M \\rightarrow \\infty$ ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We can demonstrate that if $M \\rightarrow \\infty$ then the kernel function becomes a **Gaussian kernel** (a.k.a. **Radial Basis Function** or RBF kernel, a.k.a. **squared exponential kernel** or SE kernel):\n",
    "\n",
    "$$\n",
    "k(\\mathbf{x}_i,\\mathbf{x}_j) = s^2 \\exp\\left( - \\frac{||\\mathbf{x}_i-\\mathbf{x}_j||^2}{2 l^2} \\right)\n",
    "$$\n",
    "\n",
    "where $l$ is called the length scale of the kernel, and $s^2$ is often interpreted as the overall variance. However, there are more general versions of the RBF kernel (e.g. with anisotropic length scales).\n",
    "\n",
    "Note that from our derivation, we can understand that $l$ can be related to the variance of the prior if we assume $\\overset{\\scriptscriptstyle <}{\\boldsymbol{\\Sigma}}_w=\\overset{\\scriptscriptstyle <}{\\sigma}_w^2 \\mathbf{I}$. But you don't need to think about it that way. Just think about $l$ as a hyperparameter (just like $\\overset{\\scriptscriptstyle <}{\\boldsymbol{\\Sigma}}_w$ was a matrix of hyperparameters).\n",
    "\n",
    "* (Want to see the proof for this?? Check the notes below!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "#### Note: Proof that RBF comes from inner product of Infinite Polynomial Basis Functions\n",
    "\n",
    "The mathematical notation for a kernel (disregarding hyperparameters) is:\n",
    "\n",
    "$\n",
    "k(\\mathbf{x}_i,\\mathbf{x}_j) = \\langle \\phi(\\mathbf{x}_i),  \\phi(\\mathbf{x}_j) \\rangle\n",
    "$\n",
    "\n",
    "where the bra-ket notation refers to the inner-product between two projected vectors $\\phi(\\mathbf{x_i})$, i.e. basis functions.\n",
    "\n",
    "Focusing on the RBF kernel we can write the following (without loss of generality, consider $l=1$):\n",
    "\n",
    "$$\\begin{align}\n",
    "k(\\mathbf{x}_i,\\mathbf{x}_j) &= \\exp\\left[ - \\frac{||\\mathbf{x}_i-\\mathbf{x}_j||^2}{2} \\right]\\\\\n",
    "&= \\exp\\left[ - \\frac{1}{2} \\langle \\mathbf{x}_i-\\mathbf{x}_j, \\mathbf{x}_i-\\mathbf{x}_j  \\rangle \\right]\\\\\n",
    "&= \\exp\\left[ - \\frac{1}{2} \\left(\\langle \\mathbf{x}_i, \\mathbf{x}_i-\\mathbf{x}_j  \\rangle - \\langle \\mathbf{x}_j, \\mathbf{x}_i-\\mathbf{x}_j  \\rangle \\right) \\right]\\\\\n",
    "&= \\exp\\left[ - \\frac{1}{2} \\left(\\langle \\mathbf{x}_i, \\mathbf{x}_i  \\rangle - \\langle \\mathbf{x}_i, \\mathbf{x}_j  \\rangle - \\langle \\mathbf{x}_j, \\mathbf{x}_i  \\rangle + \\langle \\mathbf{x}_j, \\mathbf{x}_j  \\rangle \\right) \\right]\\\\\n",
    "&= \\exp\\left[ - \\frac{1}{2} \\left(||\\mathbf{x}_i||^2 - 2\\langle \\mathbf{x}_j, \\mathbf{x}_i  \\rangle  +||\\mathbf{x}_j||^2 \\right) \\right]\\\\\n",
    "&= \\underbrace{\\exp\\left[ - \\frac{1}{2}||\\mathbf{x}_i||^2 - \\frac{1}{2}||\\mathbf{x}_j||^2  \\right]}_{\\text{Constant}} \\exp\\left[ \\langle \\mathbf{x}_j, \\mathbf{x}_i  \\rangle \\right]\\\\\n",
    "&= C e^{\\langle \\mathbf{x}_j, \\mathbf{x}_i  \\rangle}\\\\\n",
    "&= C \\sum_{m=0}^{\\infty}\\frac{\\langle \\mathbf{x}_j, \\mathbf{x}_i  \\rangle^n}{m!} \\quad \\quad \\text{Taylor expansion of $e^x$} \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Concluding that the RBF kernel function **results from an infinite sum over polynomial kernels** (inner-product of polynomial basis functions)!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Gaussian process regression\n",
    "\n",
    "In conclusion, we found that the Gaussian process method is simply Bayesian linear regression using nonlinear kernel functions, instead of linear basis functions.\n",
    "\n",
    "So, we fully derived the simplest PPD for a Gaussian process:\n",
    "\n",
    "$$\n",
    "{\\color{orange}p(y^*|\\mathbf{x}^*, \\mathcal{D})} = \\mathcal{N}\\left( y^* \\mid \n",
    "{\\mathbf{k}^*}^T (\\mathbf{K}+\\sigma^2\\mathbf{I}_N)^{-1}\\mathbf{y} \\,,\\, \\sigma^2 + k(\\mathbf{x}^*, \\mathbf{x}^*) - {\\mathbf{k}^*}^T (\\mathbf{K}+\\sigma^2\\mathbf{I}_N)^{-1} \\mathbf{k}^*\\right)\n",
    "$$\n",
    "\n",
    "where it is assuming **homoscedastic noise** $\\sigma^2$, i.e. we assumed constant noise (uncertainty) in the data.\n",
    "\n",
    "* The homoscedastic noise that we consider here is a type of uncertainty that is called **aleatoric uncertainty** (a.k.a. data uncertainty) because even if we had infinite amounts of data we could not reduce the uncertainty below $\\sigma^2$ at every point.\n",
    "\n",
    "* The predicted uncertainty, i.e. the remaining terms in the PPD variance, is the **epistemic uncertainty** (a.k.a. model uncertainty) because it quantifies the error we make from our model predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "For completeness, let's also explicitly write the kernel function:\n",
    "\n",
    "$$\n",
    "k(\\mathbf{x}^*, \\mathbf{x}^*) \\quad \\text{where $k$ is a chosen kernel function}\n",
    "$$\n",
    "\n",
    "the kernel vector:\n",
    "\n",
    "$$\n",
    "{\\mathbf{k}^*}^T = [k(\\mathbf{x}_1, \\mathbf{x}^*), k(\\mathbf{x}_2, \\mathbf{x}^*), \\ldots, k(\\mathbf{x}_N, \\mathbf{x}^*) ] \\quad \\text{where $N$ is the number of training points}\n",
    "$$\n",
    "\n",
    "\n",
    "and the kernel matrix, that is usually called **Covariance matrix**:  \n",
    "\n",
    "$$\n",
    "\\mathbf{K} = \\begin{bmatrix} k(\\mathbf{x}_1, \\mathbf{x}_1) & k(\\mathbf{x}_1, \\mathbf{x}_2) & \\cdots & k(\\mathbf{x}_1, \\mathbf{x}_N) \\\\\n",
    "k(\\mathbf{x}_2, \\mathbf{x}_1) & k(\\mathbf{x}_2, \\mathbf{x}_2) & \\cdots & k(\\mathbf{x}_2, \\mathbf{x}_N) \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "k(\\mathbf{x}_N, \\mathbf{x}_1) & k(\\mathbf{x}_N, \\mathbf{x}_2) & \\cdots & k(\\mathbf{x}_N, \\mathbf{x}_N) \\\\\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Kernels and more kernels...\n",
    "\n",
    "The freedom that we get from chosing kernels is the best asset of Gaussian processes. The kernel choice can be viewed as a hyperparameter itself (that has additional hyperparameters).\n",
    "\n",
    "The overwhelming majority of people simply use the RBF kernel with two hyperparameters that we introduced above:\n",
    "\n",
    "$$\n",
    "k(\\mathbf{x}_i,\\mathbf{x}_j) = s^2 \\exp\\left( - \\frac{||\\mathbf{x}_i-\\mathbf{x}_j||^2}{2 l^2} \\right)\n",
    "$$\n",
    "\n",
    "But there are many more kernels and they can be very useful!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Here's some important basic kernels:\n",
    "\n",
    "* ARD kernel (generalization of RBF kernel)\n",
    "* Matern kernel (less smooth than RBF, as it has limited degree of differentiability)\n",
    "* Periodic kernel (captures repeating structure of the data)\n",
    "\n",
    "You can see their expressions in Murphy's book (Chapter 17.1).\n",
    "\n",
    "You can see interesting plots of these kernels [here](https://distill.pub/2019/visual-exploration-gaussian-processes/#MultipleKernels).\n",
    "\n",
    "* In fact, the complete [distill.pub article](https://distill.pub/2019/visual-exploration-gaussian-processes) where those plots are included is a great resource because it is interactive. Highly recommended!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Kernels are very interesting entities...\n",
    "\n",
    "Not every funciton is a kernel! They have to obbey some rules, but there's a lot of flexibility in defining a kernel.\n",
    "\n",
    "One of the most interesting aspects of kernels is that given two valid kernels $k_1(\\mathbf{x}_i, \\mathbf{x}_j)$ and $k_2(\\mathbf{x}_i, \\mathbf{x}_j)$, we can create a new kernel using any of the following methods:\n",
    "\n",
    "$\n",
    "k(\\mathbf{x}_i, \\mathbf{x}_j) = c k_1(\\mathbf{x}_i, \\mathbf{x}_j), \\quad \\text{for any constant $c>0$}\n",
    "$\n",
    "\n",
    "$\n",
    "k(\\mathbf{x}_i, \\mathbf{x}_j) = f(\\mathbf{x}_i) k_1(\\mathbf{x}_i, \\mathbf{x}_j)f(\\mathbf{x}_j), \\quad \\text{for any function $f$}\n",
    "$\n",
    "\n",
    "$\n",
    "k(\\mathbf{x}_i, \\mathbf{x}_j) = q \\left[k_1(\\mathbf{x}_i, \\mathbf{x}_j)\\right], \\quad \\text{for any function polynomial $q$ with non-negative coefficients}\n",
    "$\n",
    "\n",
    "$\n",
    "k(\\mathbf{x}_i, \\mathbf{x}_j) = \\exp\\left[k(\\mathbf{x}_i, \\mathbf{x}_j)\\right]\n",
    "$\n",
    "\n",
    "$\n",
    "k(\\mathbf{x}_i, \\mathbf{x}_j) = \\mathbf{x}_i^T \\mathbf{A} \\mathbf{x}_j, \\quad \\text{for any positive semi-definite matrix $\\mathbf{A}$}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Even more interestingly, kernels can also be combined using **addition** or **multiplication**:\n",
    "\n",
    "$\n",
    "k(\\mathbf{x}_i, \\mathbf{x}_j) = k_1(\\mathbf{x}_i, \\mathbf{x}_j) + k_2(\\mathbf{x}_i, \\mathbf{x}_j)\n",
    "$\n",
    "\n",
    "* Adding two positive-definite kernels together always results in another positive definite kernel. This is a way to get a disjunction of the individual properties of each kernel.\n",
    "\n",
    "$\n",
    "k(\\mathbf{x}_i, \\mathbf{x}_j) = k_1(\\mathbf{x}_i, \\mathbf{x}_j) \\times k_2(\\mathbf{x}_i, \\mathbf{x}_j)\n",
    "$\n",
    "\n",
    "* Multiplying two positive-definite kernels together always results in another positive definite kernel. This is a way to get a conjunction of the individual properties of each kernel.\n",
    "\n",
    "\n",
    "A great resource discussing the creation of non-trivial kernels is in [Duvenaud's webpage](https://www.cs.toronto.edu/~duvenaud/cookbook/). Highly recommended!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now that we know all of this, we can write a general expression for the PPD of a Gaussian process becomes:\n",
    "\n",
    "$$\n",
    "{\\color{orange}p(y^*|\\mathbf{x}^*, \\mathcal{D})} = \\mathcal{N}\\left( y^* \\mid \\mu^* + \n",
    "{\\mathbf{k}^*}^T (\\mathbf{K}+\\mathbf{R})^{-1}\\mathbf{y} \\,,\\, {\\sigma^*}^2 + k(\\mathbf{x}^*, \\mathbf{x}^*) - {\\mathbf{k}^*}^T (\\mathbf{K}+\\mathbf{R})^{-1} \\mathbf{k}^*\\right)\n",
    "$$\n",
    "\n",
    "* where we added a mean function $\\mu^* = \\mu(\\mathbf{x}^*)$, instead of assuming it to be zero (like we did in the beginning of the lecture, for conciseness)\n",
    "* where the noise $\\sigma^*=\\sigma(\\mathbf{x}^*)$ is also allowed to change for each point, i.e. we are assuming **heteroscedcastic noise**\n",
    "* and where the noise matrix $\\mathbf{R}$ is (usually) diagonal but where each entry is the noise level at data point $\\mathbf{x}_i$ (this at least can be measured at every training point):\n",
    "\n",
    "    $R_{ij} = \\sigma_i^2\\delta_{ij}$\n",
    "\n",
    "    where $\\sigma_i \\equiv \\sigma(\\mathbf{x}_i)$ is the noise assumed (or measured) at each training point $\\mathbf{x}_i$, and $\\delta_{ij}$ is the Kronecker delta (Identity matrix)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summary of Gaussian Processes\n",
    "\n",
    "In fact, the previous PPD expression for Gaussian processes can be generalized by absorbing terms into the kernel (because the kernels have all those fancy properties we just talked about!):\n",
    "\n",
    "$$\n",
    "{\\color{orange}p(y^*|\\mathbf{x}^*, \\mathcal{D})} = \\mathcal{N}\\left( y^* \\mid \\mu^* + {\\mathbf{k}^*}^T \\mathbf{K}^{-1}\\mathbf{y} \\,,\\, k(\\mathbf{x}^*, \\mathbf{x}^*) - {\\mathbf{k}^*}^T \\mathbf{K}^{-1} \\mathbf{k}^*\\right)\n",
    "$$\n",
    "\n",
    "* where $\\mu^*$ <font color='red'>is usually zero</font> (it depends on the prior mean $\\overset{\\scriptscriptstyle <}{\\boldsymbol{\\mu}}_w$, but often we choose the prior mean as zero, leading to $\\mu^*=0$).\n",
    "* and where the noise (uncertainty) assumed in the observation distribution (previously, we assumed it to be constant: $\\sigma^2$) can be incorporated in the kernel (and can even change at each input point!).\n",
    "\n",
    "    For example, by adding a White Kernel to an RBF kernel:\n",
    "\n",
    "    $k(\\mathbf{x}_i, \\mathbf{x}_j) = \\sigma_i^2 \\delta_{ij} + s^2 \\exp\\left( - \\frac{||\\mathbf{x}_i-\\mathbf{x}_j||^2}{2 l^2} \\right)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### You will explore these and other things in Homework 5\n",
    "\n",
    "In the next two lectures and in Homework 5 you are going to visualize and explore some of the effects of the hyperparameter and kernel choices.\n",
    "\n",
    "Have fun!"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "hide_code_all_hidden": false,
  "kernelspec": {
   "display_name": "Python [conda env:3dasm]",
   "language": "python",
   "name": "conda-env-3dasm-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
