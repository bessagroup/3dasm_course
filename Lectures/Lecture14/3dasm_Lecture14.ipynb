{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=../figures/Brown_logo.svg width=50%>\n",
    "\n",
    "## Data-Driven Design & Analyses of Structures & Materials (3dasm)\n",
    "\n",
    "## Lecture 14\n",
    "\n",
    "### Miguel A. Bessa | <a href = \"mailto: miguel_bessa@brown.edu\">miguel_bessa@brown.edu</a>  | Associate Professor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "**What:** A lecture of the \"3dasm\" course\n",
    "\n",
    "**Where:** This notebook comes from this [repository](https://github.com/bessagroup/3dasm_course)\n",
    "\n",
    "**Reference for entire course:** Murphy, Kevin P. *Probabilistic machine learning: an introduction*. MIT press, 2022. Available online [here](https://probml.github.io/pml-book/book1.html)\n",
    "\n",
    "**How:** We try to follow Murphy's book closely, but the sequence of Chapters and Sections is different. The intention is to use notebooks as an introduction to the topic and Murphy's book as a resource.\n",
    "* If working offline: Go through this notebook and read the book.\n",
    "* If attending class in person: listen to me (!) but also go through the notebook in your laptop at the same time. Read the book.\n",
    "* If attending lectures remotely: listen to me (!) via Zoom and (ideally) use two screens where you have the notebook open in 1 screen and you see the lectures on the other. Read the book."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "**Optional reference (the \"bible\" by the \"bishop\"... pun intended ðŸ˜†) :** Bishop, Christopher M. *Pattern recognition and machine learning*. Springer Verlag, 2006.\n",
    "\n",
    "**References/resources to create this notebook:**\n",
    "* This simple tutorial is still based on a script I created for this article: https://imechanica.org/node/23957\n",
    "* It follows from some examples provided by the scikit-learn user guide, which seem to have originated from Mathieu Blondel, Jake Vanderplas, Vincent Dubourg, and Jan Hendrik Metzen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "## **OPTION 1**. Run this notebook **locally in your computer**:\n",
    "1. Confirm that you have the '3dasm' mamba (or conda) environment (see Lecture 1).\n",
    "2. Go to the 3dasm_course folder in your computer and pull the last updates of the [repository](https://github.com/bessagroup/3dasm_course):\n",
    "```\n",
    "git pull\n",
    "```\n",
    "    - Note: if you can't pull the repo due to conflicts (and you can't handle these conflicts), use this command (with **caution**!) and your repo becomes the same as the one online:\n",
    "```\n",
    "git reset --hard origin/main\n",
    "```\n",
    "3. Open command window and load jupyter notebook (it will open in your internet browser):\n",
    "```\n",
    "jupyter notebook\n",
    "```\n",
    "5. Open notebook of this Lecture and choose the '3dasm' kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "## **OPTION 2**. Use **Google's Colab** (no installation required, but times out if idle):\n",
    "\n",
    "1. go to https://colab.research.google.com\n",
    "2. login\n",
    "3. File > Open notebook\n",
    "4. click on Github (no need to login or authorize anything)\n",
    "5. paste the git link: https://github.com/bessagroup/3dasm_course\n",
    "6. click search and then click on the notebook for this Lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": true,
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "# Basic plotting tools needed in Python.\n",
    "\n",
    "import matplotlib.pyplot as plt # import plotting tools to create figures\n",
    "import numpy as np # import numpy to handle a lot of things!\n",
    "from IPython.display import display, Math # to print with Latex math\n",
    "\n",
    "%config InlineBackend.figure_format = \"retina\" # render higher resolution images in the notebook\n",
    "plt.rcParams[\"figure.figsize\"] = (8,4) # rescale figure size appropriately for slides"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Outline for today\n",
    "\n",
    "* Tutorial on using Gaussian Processes (typically this notebook is covered in 2 lectures)\n",
    "    - Gaussian Process Regression for noiseless and noisy datasets using [scikit-learn](https://scikit-learn.org)\n",
    "\n",
    "**Reading material**: This notebook + (GPs in Section 17.2 of book)\n",
    "\n",
    "**Optional reading material (GPs bible)**: Rasmussen, Carl E. *Gaussian Processes for Machine Learning*. MIT press, 2006. Available online [here](https://gaussianprocess.org/gpml/chapters/RW.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Gaussian processes\n",
    "\n",
    "Last Lecture we saw that Gaussian processes (GPs) are a kernel machine learning method.\n",
    "\n",
    "Today we will see GPs in action!\n",
    "\n",
    "* We will develop an intuition for the influence of the different hyperparameters in GPs\n",
    "\n",
    "* You will discover that the hyperparameters of GPs are not just pre-assumed values that we impose in the prior! Instead, GP \"training\" includes hyperparameter optimization... Next lecture we will explain how GP packages optimize the hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Today, we are focusing on learning how to do Gaussian Process regression for one-dimensional datasets.\n",
    "\n",
    "We will consider the function to be learned the same that we used in previous lectures $f(x) = x \\sin(x)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Function to \"learn\"\n",
    "def f(x):\n",
    "    return x * np.sin(x)\n",
    "\n",
    "n_data = 50 # number of points in our dataset\n",
    "testset_ratio = 0.90 # ratio of test set points from the dataset\n",
    "x_data = np.linspace(0, 10, n_data) # create dataset with uniformly spaced points\n",
    "y_data = f(x_data) # function values at x_data\n",
    "\n",
    "X_data = np.reshape(x_data,(-1,1)) # a 2D array that scikit-learn likes\n",
    "\n",
    "seed = 1987 # set a random seed so that everyone gets the same result\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Let's split into 10% training points and the rest for testing:\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_data,\n",
    "                                    y_data, test_size=testset_ratio,\n",
    "                                    random_state=seed)\n",
    "\n",
    "x_train = X_train.ravel() # just for plotting later\n",
    "x_test = X_test.ravel() # just for plotting later\n",
    "\n",
    "print(\"Here's a print of X_train:\\n\", X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Gaussian processes with <font color='red'>homoscedastic noise</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Let's recap the PPD of Gaussian processes assuming **homoscedastic noise** we found in the last Lecture:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\\require{color}\n",
    "{\\color{orange}p(y^*|\\mathbf{x}^*, \\mathcal{D})} = \\mathcal{N}\\left( y^* \\mid \\mu^* +\n",
    "{\\mathbf{k}^*}^T (\\mathbf{K}+\\sigma^2\\mathbf{I}_N)^{-1}\\mathbf{y} \\,,\\, {\\sigma^*}^2 + k(\\mathbf{x}^*, \\mathbf{x}^*) - {\\mathbf{k}^*}^T (\\mathbf{K}+{\\color{red}\\sigma^2}\\mathbf{I}_N)^{-1} \\mathbf{k}^*\\right)\n",
    "$$\n",
    "\n",
    "However, usually most ML packages assume a zero mean function $\\mu^*=\\mu(\\mathbf{x}^*)=0$ (usually also assuming a prior on the \"weights\" with zero mean), and a zero variance function ${\\sigma^*}^2=\\sigma(\\mathbf{x}^*)^2=0$.\n",
    "\n",
    "In the case of <font color='red'>homoscedastic noise</font>, this means that ${\\color{red}\\sigma^2}$ is constant for all $N$ training points, i.e. $\\sigma(\\mathbf{x}_n) = \\sigma^2$. So, the usual PPD for homoscedastic noise that most ML packages assume is:\n",
    "\n",
    "$$\n",
    "{\\color{orange}p(y^*|\\mathbf{x}^*, \\mathcal{D})} = \\mathcal{N}\\left( y^* \\mid \n",
    "{\\mathbf{k}^*}^T (\\mathbf{K}+\\sigma^2\\mathbf{I}_N)^{-1}\\mathbf{y} \\,,\\, k(\\mathbf{x}^*, \\mathbf{x}^*) - {\\mathbf{k}^*}^T (\\mathbf{K}+\\sigma^2\\mathbf{I}_N)^{-1} \\mathbf{k}^*\\right)\n",
    "$$\n",
    "\n",
    "* In the scikit-learn package, they call ${\\color{red}\\alpha}$ to $\\sigma^2$, i.e. $\\sigma^2 = \\alpha$. In a non-Bayesian perspective (of models like Ridge regression), sometimes you see people calling $\\alpha$ the Tikhonov regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "For completeness, remember that the PPD of GPs depends on the **kernel function**:\n",
    "\n",
    "$$\n",
    "k(\\mathbf{x}^*, \\mathbf{x}^*) \\quad \\text{where $k$ is a chosen kernel function}\n",
    "$$\n",
    "\n",
    "the **kernel vector**:\n",
    "\n",
    "$$\n",
    "{\\mathbf{k}^*}^T = [k(\\mathbf{x}_1, \\mathbf{x}^*), k(\\mathbf{x}_2, \\mathbf{x}^*), \\ldots, k(\\mathbf{x}_N, \\mathbf{x}^*) ] \\quad \\text{where $N$ is the number of training points}\n",
    "$$\n",
    "\n",
    "\n",
    "and the kernel matrix a.k.a. **Covariance matrix**:  \n",
    "\n",
    "$$\n",
    "\\mathbf{K} = \\begin{bmatrix} k(\\mathbf{x}_1, \\mathbf{x}_1) & k(\\mathbf{x}_1, \\mathbf{x}_2) & \\cdots & k(\\mathbf{x}_1, \\mathbf{x}_N) \\\\\n",
    "k(\\mathbf{x}_2, \\mathbf{x}_1) & k(\\mathbf{x}_2, \\mathbf{x}_2) & \\cdots & k(\\mathbf{x}_2, \\mathbf{x}_N) \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "k(\\mathbf{x}_N, \\mathbf{x}_1) & k(\\mathbf{x}_N, \\mathbf{x}_2) & \\cdots & k(\\mathbf{x}_N, \\mathbf{x}_N) \\\\\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "By far the most common kernel used in GPs is a kernel that results from multiplying a Constant kernel (with hyperparameter $s^2$) by the RBF kernel (with hyperparameter $l$):\n",
    "\n",
    "$$\n",
    "k(x_i,x_j) = {\\color{red}s}^2\\exp{\\left(-\\frac{||x_i-x_j||^2}{2{\\color{red}l}^2}\\right)}\n",
    "$$\n",
    "\n",
    "In this case, the entire PPD only depends on **two hyperparameters** ($s^2$ and $l$) and it does not explicitly depend on any parameter! This is also why GPs are often called a **non-parametric** ML model, although they have hyperparameters!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "I am showing the PPD for homoscedastic noise here again for you to confirm this:\n",
    "\n",
    "$$\n",
    "{\\color{orange}p(y^*|\\mathbf{x}^*, \\mathcal{D})} = \\mathcal{N}\\left( y^* \\mid \n",
    "{\\mathbf{k}^*}^T (\\mathbf{K}+\\sigma^2\\mathbf{I}_N)^{-1}\\mathbf{y} \\,,\\, k(\\mathbf{x}^*, \\mathbf{x}^*) - {\\mathbf{k}^*}^T (\\mathbf{K}+\\sigma^2\\mathbf{I}_N)^{-1} \\mathbf{k}^*\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Tutorial on 1D regression with Gaussian Processes on <font color='red'>noiseless data</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Gaussian Process Regression (GPR) for noiseless datasets\n",
    "\n",
    "Let's make our first prediction using Gaussian processes for a noiseless dataset.\n",
    "\n",
    "We will use a kernel that results from multiplying a Constant kernel (with hyperparameter $s^2$) by the RBF kernel (with hyperparameter $l$):\n",
    "\n",
    "$$\n",
    "k(x_i,x_j) = {\\color{red}s}^2\\exp{\\left(-\\frac{||x_i-x_j||^2}{2{\\color{red}l}^2}\\right)}\n",
    "$$\n",
    "\n",
    "with an initial guess for the hyperparameters as: $s = 1$ and $l = 10$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, Matern, ExpSineSquared, ConstantKernel, WhiteKernel\n",
    "\n",
    "# Define points used for plotting\n",
    "n_plot = 500 # number of points used for plotting the model\n",
    "x_plot = np.linspace(0, 10, n_plot) # create dataset with uniformly spaced points\n",
    "#x_plot = np.linspace(-10, 20, n_plot) # For in-class example\n",
    "X_plot = np.reshape(x_plot,(-1,1)) # a 2D array that scikit-learn likes\n",
    "\n",
    "# Define the kernel function\n",
    "kernel = ConstantKernel(1.0**2, (1e-3, 1e3)) * RBF(10.0, (1e-2, 1e2)) # This is the very common Constant*RBF kernel\n",
    "#kernel = 1.0**2 * RBF(10, (1e-2, 1e2)) # Same kernel as above (scikit-learn assumes constant s^2 if you just\n",
    "                                     # write RBF and multiply by a scalar, or it uses the pure RBF kernel\n",
    "                                     # (without s^2) if you do not multiply by a scalar.\n",
    "\n",
    "# Other examples of kernels:\n",
    "#kernel = ExpSineSquared(length_scale=3.0, periodicity=3.14,\n",
    "#                       length_scale_bounds=(0.1, 10.0),\n",
    "#                       periodicity_bounds=(0.1, 10)) * RBF(3.0, (1e-2, 1e2))\n",
    "#kernel = Matern(length_scale=1.0, length_scale_bounds=(1e-2, 1e2),nu=1.5)\n",
    "\n",
    "gp_model = GaussianProcessRegressor(kernel=kernel, alpha=1e-10, optimizer='fmin_l_bfgs_b', n_restarts_optimizer=20)\n",
    "#gp_model = GaussianProcessRegressor(kernel=kernel, alpha=1e-10, optimizer=None, n_restarts_optimizer=20)\n",
    "\n",
    "# Fit to data to determine parameters\n",
    "gp_model.fit(X_train, y_train)\n",
    "\n",
    "# Make the prediction on the entire dataset (for plotting)\n",
    "y_plot_pred, sigma_plot = gp_model.predict(X_plot, return_std=True) # also output the uncertainty (std)\n",
    "\n",
    "# Predict for test set (for error metric)\n",
    "y_pred, sigma = gp_model.predict(X_test, return_std=True) # also output the uncertainty (std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Plot the function, the prediction and the 95% confidence interval\n",
    "fig1, ax1 = plt.subplots()\n",
    "\n",
    "ax1.plot(x_plot, f(x_plot), 'r:', label=u'ground truth: $f(x) = x sin(x)$') # function to learn\n",
    "\n",
    "ax1.plot(x_plot, y_plot_pred, 'b-', label=\"GPR prediction\")\n",
    "ax1.fill(np.concatenate([x_plot, x_plot[::-1]]), # note: list[<start>:<stop>:<step>] where ommission uses default\n",
    "         np.concatenate([y_plot_pred - 1.9600 * sigma_plot,\n",
    "                        (y_plot_pred + 1.9600 * sigma_plot)[::-1]]),\n",
    "         alpha=.5, fc='b', ec='None', label='95% confidence interval')\n",
    "\n",
    "ax1.plot(x_train, y_train, 'ro', markersize=6, label=\"training points\") # noiseless data\n",
    "ax1.plot(x_test, y_test, 'kX', markersize=6, label=\"testing points\") # Plot test points\n",
    "\n",
    "ax1.set_xlabel('$x$', fontsize=20)\n",
    "ax1.set_ylabel('$f(x)$', fontsize=20)\n",
    "ax1.set_title(\"Posterior kernel: %s\"\n",
    "              % gp_model.kernel_, fontsize=20) # Show in the title the value of the hyperparameters\n",
    "#ax1.set_ylim(-10, 15) # just to provide more space for the legend\n",
    "ax1.legend(loc='upper left', fontsize=15)\n",
    "fig1.set_size_inches(8,8)\n",
    "plt.close(fig1) # close the plot to see it in next cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "fig1 # plot figure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## In-class example\n",
    "\n",
    "Let's play with the previous cells to understand the GP approximation for the noiseless case by recreating the above plot considering different hyperparameters, different domain size etc.\n",
    "\n",
    "1. Recreate the above plot considering the same kernel (ConstantKernel*RBF) but changing the domain bounds from $x \\in (0, 10)$ to $x \\in (-10, 20)$ so that you can see the model extrapolating."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "2. Now see what happens when you consider non-negligible aleatoric uncertainty for the training data, i.e. considering that the noise at each training point is $\\alpha = \\sigma^2 = 2.5^2$.\n",
    "\n",
    "* This should help you see that scikit-learn is only considering constant **aleatoric uncertainty** for the input points of $\\sigma^2$ **that we are imposing** via $\\alpha$ because in the extrapolation part of the domain (no training points) you only see the effect of the epistemic uncertainty $s^2$ (which is modeled by the constant kernel hyperparameter, as we will discuss more in a moment)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "3. Revert back to the case with negligible aleatoric uncertainty at the input points, i.e. $\\alpha = 1e-10$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## In-class example (continued)\n",
    "\n",
    "4. Turn off the hyperparameter optimizer (optimizer = None), and see the effect of particular choices of hyperparameter $l$ for the following kernel:\n",
    "\n",
    "$$\n",
    "k(x_i,x_j) = {\\color{red}s}^2\\exp{\\left(-\\frac{||x_i-x_j||^2}{2{\\color{red}l}^2}\\right)}\n",
    "$$\n",
    "\n",
    "which corresponds to a Constant kernel $s^2$ times the RBF kernel (with hyperparameter $l$).\n",
    "\n",
    "4.1. For fixed hyperparameters $s^2=1.0$ and $l=10$.\n",
    "\n",
    "4.2. For fixed hyperparameters $s^2=1.0$ and $l=1$.\n",
    "\n",
    "4.3. For fixed hyperparameters $s^2=1.0$ and $l=0.1$.\n",
    "\n",
    "* This should help you understand why $l$ is called the length scale hyperparameter (it is associated to the \"roughness\" of the response)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## In-class example (continued)\n",
    "\n",
    "5. Still considering the hyperparameter optimizer turned off (optimizer = None), now investigate the effect of the constant kernel (hyperparameter $s^2$):\n",
    "\n",
    "5.1. For fixed hyperparamaters $s^2=1.0$ and $l=1$.\n",
    "\n",
    "5.2. For fixed hyperparamaters $s^2=2.5^2$ and $l=1$.\n",
    "\n",
    "5.3. For fixed hyperparamaters $s^2=4^2$ and $l=1$.\n",
    "\n",
    "* This should help you understand why $s^2$ is called the variance hyperparameter (it is the variance of the **epistemic uncertainty**, i.e. the prediction far away from the training data.\n",
    "    * You can see this in plot 5.1 (where $s^2=1.0$) because the 95% confidence interval is $\\pm 1.96\\times 1.0 \\approx \\pm 2$, for 5.2 (where $s^2=2.5^2$) it is $\\pm 1.96\\times2.5 \\approx \\pm 5$ and for 5.3 (where $s^2=4.0^2$) it is $\\pm 1.96\\times4.0 \\approx \\pm 8$.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## In-class example (continued)\n",
    "\n",
    "6. Now \"turn on\" the hyperparameter optimization again (optimizer='fmin_l_bfgs_b') and train the model.\n",
    "\n",
    "* You can see that the quality of interpolation is fantastic, but the extrapolation is quite bad... This behavior is typical of state-of-ther-art ML models.\n",
    "\n",
    "* You should also observe that the Gaussian process reverts back to the zero mean function far away from the training points. That is the consequence of assuming a prior with zero mean."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## In-class example (continued)\n",
    "\n",
    "7. Finally, consider different kernels and observe the quality of the approximation:\n",
    "\n",
    "7.1. Use the Exponential-Sine-Squared kernel (uncomment the code in appropriate line).\n",
    "\n",
    "7.2. Use the Matern kernel (uncomment the code in appropriate line).\n",
    "\n",
    "* In this case, the $s^2 \\times \\text{RBF}$ kernel led to better results. Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### In-class Exercise 1\n",
    "\n",
    "Fit a polynomial of degree 4 (Linear Least Squares with polynomial basis functions of degree 4) and compute the appropriate regression error metrics (R$^2$ and $\\text{MSE}$) for that model as well as for the Gaussian process model using the RBF kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Write your code for Exercise 1:\n",
    "\n",
    "# until here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Gaussian Process Regression approximates the function much better...\n",
    "\n",
    "* However, note that the **choice of kernel** used in GPR affects the quality of the prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Gaussian Process regression for <font color='red'>noisy datasets</font>\n",
    "\n",
    "Let's recreate the noisy dataset from $f(x)=x\\sin{x}$, as we did in Lecture 10:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We concluded that the PPD for a Gaussian process with **heteroscedastic noise** is:\n",
    "\n",
    "$$\\require{color}\n",
    "{\\color{orange}p(y^*|\\mathbf{x}^*, \\mathcal{D})} = \\mathcal{N}\\left( y^* \\mid \\mu^* +\n",
    "{\\mathbf{k}^*}^T (\\mathbf{K}+\\mathbf{R})^{-1}\\mathbf{y} \\,,\\, {\\sigma^*}^2 + k(\\mathbf{x}^*, \\mathbf{x}^*) - {\\mathbf{k}^*}^T (\\mathbf{K}+\\mathbf{R})^{-1} \\mathbf{k}^*\\right)\n",
    "$$\n",
    "\n",
    "where usually $\\mu^*\\equiv\\mu(\\mathbf{x}^*)$ and $\\sigma^* \\equiv \\sigma(\\mathbf{x}^*)$ are **assumed to be zero**, and where the aleatoric uncertainty at training points (noise matrix $\\mathbf{R}$) is often considered a diagonal matrix where each entry is the noise level at data point $\\mathbf{x}_i$ (sometimes noise can be measured at every training point $\\mathbf{x}_i$, but usually this information is not known):\n",
    "\n",
    "$R_{ij} = \\sigma_i^2\\delta_{ij}$\n",
    "\n",
    "where $\\sigma_i \\equiv \\sigma(\\mathbf{x}_i)$ is the noise assumed (or measured) at each training point $\\mathbf{x}_i$, and $\\delta_{ij}$ is the Kronecker delta (Identity matrix).\n",
    "\n",
    "* In scikit-learn $\\boldsymbol{\\alpha}$ can also be a **vector**, i.e. $\\alpha_i = \\sigma_i^2$. This is one way of modeling heteroscedasticity (if you know the variance for your aleatoric uncertainty at each training point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Now let's also create the noisy dataset:\n",
    "noise_std = 0.2 + 1.0 * np.random.random(y_data.shape) # np.random.random returns random number between [0.0, 1.0)\n",
    "noise_samples = np.random.normal(0, noise_std) # sample vector from Gaussians with random standard deviation\n",
    "y_noisy_data = y_data + noise_samples # Perturb every y_data point with Gaussian noise\n",
    "\n",
    "# Pair up points with their associated noise level (because of train_test_split):\n",
    "Y_noisy_data = np.column_stack((y_noisy_data,noise_std)) # note that we use \"noise_std\" (not \"noise_samples\")\n",
    "\n",
    "# Split into training points and the rest for testing (according to testset_ratio):\n",
    "X_train, X_test, Y_noisy_train, Y_noisy_test = train_test_split(X_data,\n",
    "                                    Y_noisy_data, test_size=testset_ratio,\n",
    "                                    random_state=seed) # \"noisy_train\" is a great name for a variable, hein?\n",
    "# NOTE: since we are using the same seed and we do train_test_split on the same X_data and since y_noisy_data\n",
    "#       is just y_data + noise, we are splitting the dataset exactly in the same way! This is nice because we\n",
    "#       want to keep the comparison as fair as possible.\n",
    "\n",
    "# Finally, for plotting purposes, let's convert the 2D arrays into 1D arrays (vectors):\n",
    "x_train = X_train.ravel()\n",
    "x_test = X_test.ravel()\n",
    "y_noisy_train = Y_noisy_train[:,0]\n",
    "noise_std_train = Y_noisy_train[:,1]\n",
    "y_noisy_test = Y_noisy_test[:,0]\n",
    "noise_std_test = Y_noisy_test[:,1]\n",
    "\n",
    "print(\"Note that X_train and X_test are the same data that we used for the noiseless case.\")\n",
    "print(\"Also note that noise in training data is coming from a Gaussian with standard deviation:\\n\", noise_std_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now let's train a GP but considering that we know the heteroscedastic noise at every training data point\n",
    "\n",
    "* Knowing the noise at each training point is not common in practice. However, if you can do multiple measurements at each data point $x_i$ to determine the standard deviation of $y_i$ at that data point, then you can estimate the noise and include it in the \"alpha\" parameter of scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Instanciate a Gaussian Process model with an RBF kernel + White kernel to learn the noise (not given)\n",
    "kernel = ConstantKernel(1.0, (1e-3, 1e3)) * RBF(10, (1e-2, 1e2))\n",
    "\n",
    "# Fitting for noisy data, if we have access to the uncertainty at the training points (usually we don't!), then\n",
    "# we can include the noise level at the alpha parameter\n",
    "gp_model = GaussianProcessRegressor(kernel=kernel, alpha=noise_std_train**2,\n",
    "                                    optimizer='fmin_l_bfgs_b', n_restarts_optimizer=5)\n",
    "\n",
    "# Fit to data to determine the parameters of the model\n",
    "gp_model.fit(X_train, y_noisy_train)\n",
    "\n",
    "# Make the predictions\n",
    "y_noisy_pred, sigma_noisy = gp_model.predict(X_test, return_std=True) # predictions including uncertainty (std)\n",
    "y_noisy_plot_pred, sigma_noisy_plot = gp_model.predict(X_plot, return_std=True) # for plotting\n",
    "\n",
    "# Plot the function, the prediction and the 95% confidence interval\n",
    "fig1, ax1 = plt.subplots() # This opens a new figure\n",
    "\n",
    "ax1.plot(x_plot, f(x_plot), 'r:', label=u'ground truth: $f(x) = x sin(x)$') # function to learn\n",
    "ax1.errorbar(x_train, y_noisy_train, np.abs(noise_std_train*2.0), fmt='ro', markersize=6, label=u'training points inc. uncertainty')\n",
    "ax1.errorbar(x_test, y_noisy_test, np.abs(noise_std_test*2.0), fmt='kX', markersize=6, label=u'testing points inc. uncertainty')\n",
    "\n",
    "ax1.plot(x_plot, y_noisy_plot_pred, 'b-', label=\"GPR prediction\")\n",
    "ax1.fill(np.concatenate([x_plot, x_plot[::-1]]),\n",
    "         np.concatenate([y_noisy_plot_pred - 1.9600 * sigma_noisy_plot,\n",
    "                        (y_noisy_plot_pred + 1.9600 * sigma_noisy_plot)[::-1]]),\n",
    "         alpha=.5, fc='b', ec='None', label='95% confidence interval')\n",
    "ax1.set_xlabel('$x$', fontsize=20)\n",
    "ax1.set_ylabel('$f(x)$', fontsize=20)\n",
    "ax1.set_title(\"Posterior kernel: %s\"\n",
    "              % gp_model.kernel_, fontsize=20) # Show in the title the value of the hyperparameters\n",
    "ax1.set_ylim(-10, 15) # just to provide more space for the legend\n",
    "ax1.legend(loc='upper left', fontsize=15)\n",
    "fig1.set_size_inches(8,8)\n",
    "plt.close(fig1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "fig1 # plot figure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Exercise 2\n",
    "\n",
    "Fit a polynomial of degree 4 (like we did last class) and compute the error metrics for that model as well as the above mentioned Gaussian process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Exercise 2.\n",
    "\n",
    "# until here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Finally, let me show you something super cool!\n",
    "\n",
    "There is a more general way to estimate aleatoric uncertainty when you **do not know the variance of your training points**.\n",
    "\n",
    "This is possible by adding a white Kernel to the previous kernel:\n",
    "\n",
    "$$\n",
    "k(x_i,x_j) = {\\color{red}s_1}^2\\exp{\\left(-\\frac{||x_i-x_j||^2}{2{\\color{red}l}^2}\\right)} + {\\color{red}s_2}^2\\delta_{ij}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In this case, however, you should not use the $\\alpha$ parameter (assume it zero or close to zero).\n",
    "\n",
    "The PPD for the heroscedastic Gaussian process where you want to **find the noise in your training points by setting it as a hyperparameter** instead of imposing the noise is then:\n",
    "\n",
    "$$\\require{color}\n",
    "{\\color{orange}p(y^*|\\mathbf{x}^*, \\mathcal{D})} = \\mathcal{N}\\left( y^* \\mid\n",
    "{\\mathbf{k}^*}^T \\mathbf{K}^{-1}\\mathbf{y} \\,,\\, k(\\mathbf{x}^*, \\mathbf{x}^*) - {\\mathbf{k}^*}^T \\mathbf{K}^{-1} \\mathbf{k}^*\\right)\n",
    "$$\n",
    "\n",
    "where you add a White kernel to the kernel function, for example use this kernel:\n",
    "\n",
    "$$\n",
    "k(x_i,x_j) = {\\color{red}s_1}^2\\exp{\\left(-\\frac{||x_i-x_j||^2}{2{\\color{red}l}^2}\\right)} + {\\color{red}s_2}^2\\delta_{ij}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* However, be careful! The hyperparameter optimization when you include the White kernel can be difficult if there is not enough training data and if you use unreasonable bounds for the hyperparameter of the white kernel... You can play with the cell below and understand what I mean..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Instanciate a Gaussian Process model with an RBF kernel + White kernel to learn the noise (not given)\n",
    "kernel = ConstantKernel(1.0, (1e-3, 1e3)) * RBF(\n",
    "    length_scale=1e-1, length_scale_bounds=(1e-2, 1e3)) + WhiteKernel(\n",
    "    noise_level=1e-2, noise_level_bounds=(1e-10, 1e1) )\n",
    "\n",
    "# Create the GP model but make sure that alpha=0.0 because now you have a White Kernel!\n",
    "gp_model = GaussianProcessRegressor(kernel=kernel, alpha=0.0,\n",
    "                                    optimizer='fmin_l_bfgs_b', n_restarts_optimizer=5)\n",
    "\n",
    "# Fit to data to determine the parameters of the model\n",
    "gp_model.fit(X_train, y_noisy_train)\n",
    "\n",
    "# Make the predictions\n",
    "y_noisy_pred, sigma_noisy = gp_model.predict(X_test, return_std=True) # predictions including uncertainty (std)\n",
    "y_noisy_plot_pred, sigma_noisy_plot = gp_model.predict(X_plot, return_std=True) # for plotting\n",
    "\n",
    "# Plot the function, the prediction and the 95% confidence interval\n",
    "fig1, ax1 = plt.subplots() # This opens a new figure\n",
    "\n",
    "ax1.plot(x_plot, f(x_plot), 'r:', label=u'ground truth: $f(x) = x sin(x)$') # function to learn\n",
    "ax1.errorbar(x_train, y_noisy_train, np.abs(noise_std_train)*1.9600, fmt='ro', markersize=6, label=u'training points inc. uncertainty')\n",
    "ax1.errorbar(x_test, y_noisy_test, np.abs(noise_std_test)*1.9600, fmt='kX', markersize=6, label=u'testing points inc. uncertainty')\n",
    "\n",
    "ax1.plot(x_plot, y_noisy_plot_pred, 'b-', label=\"GPR prediction\")\n",
    "ax1.fill(np.concatenate([x_plot, x_plot[::-1]]),\n",
    "         np.concatenate([y_noisy_plot_pred - 1.9600 * sigma_noisy_plot,\n",
    "                        (y_noisy_plot_pred + 1.9600 * sigma_noisy_plot)[::-1]]),\n",
    "         alpha=.5, fc='b', ec='None', label='95% confidence interval')\n",
    "ax1.set_xlabel('$x$', fontsize=20)\n",
    "ax1.set_ylabel('$f(x)$', fontsize=20)\n",
    "ax1.set_title(\"Posterior kernel: %s\"\n",
    "              % gp_model.kernel_, fontsize=20) # Show in the title the value of the hyperparameters\n",
    "ax1.set_ylim(-10, 15) # just to provide more space for the legend\n",
    "ax1.legend(loc='upper left', fontsize=15)\n",
    "fig1.set_size_inches(8,8)\n",
    "plt.close(fig1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "fig1 # plot figure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Note: the above figure should show that the model **did not learn the noise properly** (if you used few training points). Go back to the beginning of the notebook and **change the test_ratio to 0.7 (instead of 0.9) and run the entire notebook again. You will see that it learns the noise** (approximately)! Pretty cool!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### You will explore these and other things in Homework 5\n",
    "\n",
    "Have fun!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "### Solution for Exercise 1\n",
    "\n",
    "```python\n",
    "# We start by importing the polynomial predictor from scikit-learn\n",
    "from sklearn.preprocessing import PolynomialFeatures # For Polynomial fit\n",
    "from sklearn.linear_model import LinearRegression # For Least Squares\n",
    "from sklearn.pipeline import make_pipeline # to link different objects\n",
    "\n",
    "degree = 4 # degree of polynomial we want to fit\n",
    "poly_model = make_pipeline(PolynomialFeatures(degree),LinearRegression())\n",
    "poly_model.fit(X_train,y_train) # fit the polynomial to our 5 points in X_train which is a 2D array  \n",
    "y_poly_pred = poly_model.predict(X_test) # prediction of our polynomial\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, r2_score # Import error metrics\n",
    "# Compute MSE and R2 for the GP model\n",
    "gp_mse_value = mean_squared_error(y_test, y_pred)\n",
    "gp_r2_value = r2_score(y_test, y_pred)\n",
    "print('MSE for GPR = ', gp_mse_value)\n",
    "print('R2 score for GPR = ', gp_r2_value)\n",
    "\n",
    "# Compute MSE and R2 for the polynomial model\n",
    "poly_mse_value = mean_squared_error(y_test, y_poly_pred)\n",
    "poly_r2_value = r2_score(y_test, y_poly_pred)\n",
    "print('MSE for polynomial = ', poly_mse_value)\n",
    "print('R2 score for polynomial = ', poly_r2_value)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "### Solution to Exercise 2\n",
    "\n",
    "``` python\n",
    "degree = 4 # degree of polynomial we want to fit\n",
    "poly_model = make_pipeline(PolynomialFeatures(degree),LinearRegression())\n",
    "poly_model.fit(X_train,y_noisy_train) # fit the polynomial to our 5 points in X_train which is a 2D array\n",
    "y_poly_noisy_pred = poly_model.predict(X_test) # prediction of our polynomial\n",
    "\n",
    "# Compute MSE and R2 for the GP model\n",
    "# NOTE: here we will compare with the noiseless function (in practice we don't have this information!).\n",
    "gp_mse_value = mean_squared_error(y_test, y_noisy_pred)\n",
    "gp_r2_value = r2_score(y_test, y_noisy_pred)\n",
    "print('MSE for GPR = ', gp_mse_value)\n",
    "print('R2 score for GPR = ', gp_r2_value)\n",
    "\n",
    "# Compute MSE and R2 for the polynomial model\n",
    "poly_mse_value = mean_squared_error(y_test, y_poly_noisy_pred)\n",
    "poly_r2_value = r2_score(y_test, y_poly_noisy_pred)\n",
    "print('MSE for polynomial = ', poly_mse_value)\n",
    "print('R2 score for polynomial = ', poly_r2_value)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "hide_code_all_hidden": false,
  "kernelspec": {
   "display_name": "Python [conda env:3dasm] *",
   "language": "python",
   "name": "conda-env-3dasm-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
