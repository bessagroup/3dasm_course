{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=docs/tudelft_logo.jpg width=50%>\n",
    "\n",
    "## Data-driven Design and Analyses of Structures and Materials (3dasm)\n",
    "\n",
    "## Lecture 5\n",
    "\n",
    "### Miguel A. Bessa | <a href = \"mailto: M.A.Bessa@tudelft.nl\">M.A.Bessa@tudelft.nl</a>  | Associate Professor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "**What:** A lecture of the \"3dasm\" course\n",
    "\n",
    "**Where:** This notebook comes from this [repository](https://github.com/bessagroup/3dasm_course)\n",
    "\n",
    "**Reference for entire course:** Murphy, Kevin P. *Probabilistic machine learning: an introduction*. MIT press, 2022. Available online [here](https://probml.github.io/pml-book/book1.html)\n",
    "\n",
    "**How:** We try to follow Murphy's book closely, but the sequence of Chapters and Sections is different. The intention is to use notebooks as an introduction to the topic and Murphy's book as a resource.\n",
    "* If working offline: Go through this notebook and read the book.\n",
    "* If attending class in person: listen to me (!) but also go through the notebook in your laptop at the same time. Read the book.\n",
    "* If attending lectures remotely: listen to me (!) via Zoom and (ideally) use two screens where you have the notebook open in 1 screen and you see the lectures on the other. Read the book."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "**Optional reference (the \"bible\" by the \"bishop\"... pun intended ðŸ˜†) :** Bishop, Christopher M. *Pattern recognition and machine learning*. Springer Verlag, 2006.\n",
    "\n",
    "**References/resources to create this notebook:**\n",
    "* [Car figure](https://korkortonline.se/en/theory/reaction-braking-stopping/)\n",
    "\n",
    "Apologies in advance if I missed some reference used in this notebook. Please contact me if that is the case, and I will gladly include it here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## **OPTION 1**. Run this notebook **locally in your computer**:\n",
    "1. Confirm that you have the 3dasm conda environment (see Lecture 1).\n",
    "\n",
    "2. Go to the 3dasm_course folder in your computer and pull the last updates of the [repository](https://github.com/bessagroup/3dasm_course):\n",
    "```\n",
    "git pull\n",
    "```\n",
    "3. Open command window and load jupyter notebook (it will open in your internet browser):\n",
    "```\n",
    "conda activate 3dasm\n",
    "jupyter notebook\n",
    "```\n",
    "4. Open notebook of this Lecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## **OPTION 2**. Use **Google's Colab** (no installation required, but times out if idle):\n",
    "\n",
    "1. go to https://colab.research.google.com\n",
    "2. login\n",
    "3. File > Open notebook\n",
    "4. click on Github (no need to login or authorize anything)\n",
    "5. paste the git link: https://github.com/bessagroup/3dasm_course\n",
    "6. click search and then click on the notebook for this Lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": true,
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "# Basic plotting tools needed in Python.\n",
    "\n",
    "import matplotlib.pyplot as plt # import plotting tools to create figures\n",
    "import numpy as np # import numpy to handle a lot of things!\n",
    "from IPython.display import display, Math # to print with Latex math\n",
    "\n",
    "%config InlineBackend.figure_format = \"retina\" # render higher resolution images in the notebook\n",
    "plt.style.use(\"seaborn\") # style for plotting that comes from seaborn\n",
    "plt.rcParams[\"figure.figsize\"] = (8,4) # rescale figure size appropriately for slides"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Outline for today\n",
    "\n",
    "* Bayesian inference for one hidden rv\n",
    "    - Prior\n",
    "    - Likelihood\n",
    "    - Marginal likelihood\n",
    "    - Posterior\n",
    "    - Gaussian pdf's product\n",
    "\n",
    "**Reading material**: This notebook + Chapter 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cell_style": "center",
    "scrolled": true,
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "# This cell is hidden during presentation. It's just to define a function to plot the governing model of\n",
    "# the car stopping distance problem. Defining a function that creates a plot allows to repeatedly run\n",
    "# this function on cells used in this notebook.\n",
    "def car_fig(ax):\n",
    "    real_x = np.linspace(3, 83, 1000)\n",
    "    real_mu_y = 1.5*real_x + 0.1*(real_x**2) # Recall: E[z*x+0.1*x^2] = E[z]*E[x]+0.1*E[x^2]\n",
    "    real_sigma_y = np.sqrt( 0.5**2*real_x**2 ) # Recall: V[z*x+x^2] = V[z*x]+V[x^2]=...=sigma_z^2+mu_x^2\n",
    "    ax.set_xlabel(\"x (m/s)\", fontsize=20) # create x-axis label with font size 20\n",
    "    ax.set_ylabel(\"y (m)\", fontsize=20) # create y-axis label with font size 20\n",
    "    ax.set_title(\"Car stopping distance problem\", fontsize=20); # create title with font size 20\n",
    "    ax.plot(real_x, real_mu_y, 'k:', label=\"Governing model $\\mu_y$\")\n",
    "    ax.fill_between(real_x, real_mu_y - 1.9600 * real_sigma_y,\n",
    "                    real_mu_y + 1.9600 * real_sigma_y,\n",
    "                    color='k', alpha=0.2,\n",
    "                    label='95% confidence interval ($\\mu_y \\pm 1.96\\sigma_y$)') # plot 95% credence interval\n",
    "    ax.legend(fontsize=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### A slightly more complicated car stopping distance problem\n",
    "\n",
    "<img src=\"docs/reaction-braking-stopping.svg\" title=\"Car stopping distance\" width=\"25%\" align=\"right\">\n",
    "\n",
    "Today we will finally do some predictions!\n",
    "\n",
    "Recall the Homework of Lecture 4, and consider the car stopping distance problem for constant velocity $x=75$ m/s and for which **it is known** that $z_2 \\sim \\mathcal{N}(z_2|\\mu_{z_2}=0.1,\\sigma_{z_2}^2=0.01^2)$.\n",
    "\n",
    "The only information that we do not know is the driver's reaction time $z$ (here we are calling $z$ instead of $z_1$ as in Lecture 4 because this is the only hidden variable).\n",
    "\n",
    "* Can we predict $p(y)$ without knowing $p(z)$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Yes!! If we use Bayes' rule!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Recall the Homework of Lecture 4\n",
    "\n",
    "<img src=\"docs/reaction-braking-stopping.svg\" title=\"Car stopping distance\" width=\"25%\" align=\"right\">\n",
    "\n",
    "From last lecture's Homework, you demonstrated that the conditional pdf of the stopping distance given the reaction time $z$ (for convenience we write here $z$ instead of $z_1$) is\n",
    "\n",
    "$$\n",
    "p(y|z) = \\mathcal{N}\\left(y | \\mu_{y|z}=w z+b, \\sigma_{y|z}^2=s^2\\right)\n",
    "$$\n",
    "\n",
    "where $w$, $b$ and $s$ are all constants that you determined to be:\n",
    "\n",
    "$w=x=75$\n",
    "\n",
    "$b=x^2\\mu_{z_2}=75^2\\cdot0.1=562.5$\n",
    "\n",
    "$s^2=(x^2 \\sigma_{z_2})^2=(75^2\\cdot0.01)^2=56.25^2$\n",
    "\n",
    "because we are considering the car is going at constant velocity $x=75$ m/s and that we know $z_2= \\mathcal{N}(z_2|\\mu_{z_2}=0.1,\\sigma_{z_2}^2=0.01^2)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Understanding the Bayes' rule\n",
    "\n",
    "$\\require{color}$\n",
    "$$\n",
    "{\\color{green}p(z|y)} = \\frac{ {\\color{blue}p(y|z)}{\\color{red}p(z)} } {p(y)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* ${\\color{red}p(z)}$ is the **prior distribution**\n",
    "* ${\\color{blue}p(y)}$ is the **observation distribution** (conditional pdf)\n",
    "* $p(y)$ is the **marginal distribution**\n",
    "* ${\\color{green}p(z|y)}$ is the **posterior distribution**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "### A note about the term \"distribution\"\n",
    "\n",
    "The term distribution can mean two things:\n",
    "1. For **continuous** <a title=\"random variables\">rv's</a>, the term *distribution* means *probability density function* (<a title=\"probability density function\">pdf</a>).\n",
    "\n",
    "2. For **discrete** <a title=\"random variables\">rv's</a> the term *distribution* means *probability mass function* (<a title=\"probability mass function\">pmf</a>), as we will see later in the course.\n",
    "\n",
    "We won't talk about categorical distributions or <a title=\"probability mass functions\">pmf's</a> for a while. So, for now, when you see the term *distribution* it is the same as saying <a title=\"probability density function\">pdf</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Understanding the Bayes' rule\n",
    "\n",
    "Let's start by understanding the usefulness of Bayes' rule by calculating the posterior $p(z|y)$ for the car stopping distance problem (Homework of Lecture 4)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "As we mentioned, for our problem we know the **observation distribution**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$p(y|z) = \\mathcal{N}\\left(y | \\mu_{y|z}=w z+b, \\sigma_{y|z}^2\\right)$\n",
    "\n",
    "where $\\sigma_{y|z} = \\text{const}$, as well as $w$ and $b$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "but we **don't know** the prior $p(z)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Prior: our beliefs about the problem\n",
    "\n",
    "If we have absolutely no clue about what the distribution of the hidden rv $z$ is, then we can use a **Uniform distribution** (a.k.a. uninformative prior).\n",
    "\n",
    "This distribution assigns equal probability to any value of $z$ within an interval $z \\in (z_{min}, z_{max})$.\n",
    "\n",
    "$$\n",
    "p(z) = \\frac{1}{C_z}\n",
    "$$\n",
    "\n",
    "where $C_z = z_{max}-z_{min}$ is the **normalization constant** of the Uniform pdf, i.e. the value that guarantees that $p(z)$ integrates to one.\n",
    "\n",
    "As we will see, the actual values of $z_{max}$ and $z_{min}$ won't make any difference. But if you don't to use letters for constants, you can consider that $z_{min} = 0$ seconds is the limit of the fastest reaction time that is humanly possible, and $z_{max} = 3$ seconds is the slowest reaction time of a human being (you could get this additional information by asking a doctor)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Summary of our Model\n",
    "\n",
    "1. The **observation distribution**:\n",
    "\n",
    "$$\\begin{align}\n",
    "p(y|z) &= \\mathcal{N}\\left(y | \\mu_{y|z}=w z+b, \\sigma_{y|z}^2\\right) \\\\\n",
    "&= \\frac{1}{C_{y|z}} \\exp\\left[ -\\frac{1}{2\\sigma_{y|z}^2}(y-\\mu_{y|z})^2\\right]\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where $C_{y|z} = \\sqrt{2\\pi \\sigma_{y|z}^2}$ is the **normalization constant** of the Gaussian pdf, and where $\\mu_{y|z}=w z+b$, with $w$, $b$ and $\\sigma_{y|z}^2$ being constants, as previously mentioned.\n",
    "\n",
    "2. and the **prior distribution**: $p(z) = \\frac{1}{C_z}$\n",
    "\n",
    "where $C_z = z_{max}-z_{min}$ is the **normalization constant** of the Uniform pdf, i.e. the value that guarantees that $p(z)$ integrates to one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Posterior from Bayes' rule\n",
    "\n",
    "Since we have defined the **observation distribution** and the **prior distribution**, we can now compute the posterior distribution from Bayes' rule."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "But this requires a bit of algebra... Let's do it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "First, in order to apply Bayes' rule $p(z|y) = \\frac{ p(y|z)p(z)}{p(y)}$ we need to calculate $p(y)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$p(y)$ is obtained by marginalizing the joint distribution wrt $z$:\n",
    "\n",
    "$\n",
    "p(y) = \\int p(y|z)p(z) dz\n",
    "$\n",
    "\n",
    "which implies an integration over $z$. So, let's rewrite $p(y|z)$ so that the integration becomes easier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "$$\\begin{align}\n",
    "p(y|z) &= \\mathcal{N}\\left(y | \\mu_{y|z}=w z+b, \\sigma_{y|z}^2\\right) \\\\\n",
    "&= \\frac{1}{C_{y|z}} \\exp\\left[ -\\frac{1}{2\\sigma_{y|z}^2}(y-(w z+b))^2\\right] \\\\\n",
    "&= \\frac{1}{C_{y|z}} \\exp\\left\\{ -\\frac{1}{2\\left(\\frac{\\sigma_{y|z}}{w}\\right)^2}\\left[z-\\left(\\frac{y-b}{w}\\right)^2\\right]\\right\\} \\\\\n",
    "&= \\frac{1}{|w|}\\frac{1}{\\sqrt{2\\pi \\left(\\frac{\\sigma_{y|z}}{w}\\right)^2}} \\exp\\left\\{ -\\frac{1}{2\\left(\\frac{\\sigma_{y|z}}{w}\\right)^2}\\left[z-\\left(\\frac{y-b}{w}\\right)^2\\right]\\right\\}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "\n",
    "Note: This Gaussian pdf $\\mathcal{N}\\left(z | \\frac{y-b}{w}, \\left(\\frac{\\sigma_{y|z}}{w}\\right)^2\\right)$ is unnormalized when written wrt $z$ (due to $\\frac{1}{|w|}$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We can now calculate the marginal distribution $p(y)$:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "p(y) &= \\int p(y|z)p(z) dz \\\\\n",
    "&= \\int \\frac{1}{|w|}\\frac{1}{\\sqrt{2\\pi \\left(\\frac{\\sigma_{y|z}}{w}\\right)^2}} \\exp\\left\\{ -\\frac{1}{2\\left(\\frac{\\sigma_{y|z}}{w}\\right)^2}\\left[z-\\left(\\frac{y-b}{w}\\right)^2\\right]\\right\\} \\frac{1}{C_z} dz\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We can rewrite this expression as,\n",
    "\n",
    "$$\\require{color}\n",
    "\\begin{align}\n",
    "p(y) &= \\frac{1}{|w|\\cdot C_z} {\\color{blue}\\int \\frac{1}{\\sqrt{2\\pi \\left(\\frac{\\sigma_{y|z}}{w}\\right)^2}} \\exp\\left\\{ -\\frac{1}{2\\left(\\frac{\\sigma_{y|z}}{w}\\right)^2}\\left[z-\\left(\\frac{y-b}{w}\\right)^2\\right]\\right\\} dz} \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "What is the result for the <font color='blue'>blue term</font>?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "From where we conclude that the marginal distribution is:\n",
    "\n",
    "$$\\require{color}\n",
    "p(y) = \\frac{1}{|w| C_z }\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "So, now we can determine the posterior:\n",
    "\n",
    "$$\\require{color}\n",
    "\\begin{align}\n",
    "p(z|y) &= \\frac{ p(y|z)p(z)}{p(y)} \\\\\n",
    "&= |w| C_z \\cdot \\frac{1}{|w|}\\frac{1}{\\sqrt{2\\pi \\left(\\frac{\\sigma_{y|z}}{w}\\right)^2}} \\exp\\left\\{ -\\frac{1}{2\\left(\\frac{\\sigma_{y|z}}{w}\\right)^2}\\left[z-\\left(\\frac{y-b}{w}\\right)^2\\right]\\right\\} \\cdot \\frac{1}{C_z}\\\\\n",
    "&= \\frac{1}{\\sqrt{2\\pi \\left(\\frac{\\sigma_{y|z}}{w}\\right)^2}}\\exp\\left\\{ -\\frac{1}{2\\left(\\frac{\\sigma_{y|z}}{w}\\right)^2}\\left[z-\\left(\\frac{y-b}{w}\\right)^2\\right]\\right\\}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "which is a **normalized** Gaussian pdf in $z$: $\\mathcal{N}\\left(z | \\frac{y-b}{w}, \\left(\\frac{\\sigma_{y|z}}{w}\\right)^2\\right)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* **This is what the Bayes' rule does!** Computes the posterior $p(z|y)$ from $p(y|z)$ and $p(z)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Why should we care about the Bayes' rule?\n",
    "\n",
    "There are a few reasons:\n",
    "\n",
    "1. As we will see, models are usually (always?) wrong.\n",
    "\n",
    "\n",
    "2. But our beliefs may be a bit closer to reality! Bayes' rule enables us to get better models if our beliefs are reasonable!\n",
    "\n",
    "\n",
    "3. We don't observe distributions. We observe **DATA**. Bayes' rule is a very powerful way to predict the distribution of our quantity of interest (here: $y$) from data!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bayes' rule applied to observed data\n",
    "\n",
    "Previously, we already introduced Bayes' rule when applied to observed data $\\mathcal{D}_y$.\n",
    "\n",
    "$\\require{color}$\n",
    "$$\n",
    "{\\color{green}p(z|y=\\mathcal{D}_y)} = \\frac{ {\\color{blue}p(y=\\mathcal{D}_y|z)}{\\color{red}p(z)} } {p(y=\\mathcal{D}_y)} = \\frac{ {\\color{magenta}p(y=\\mathcal{D}_y, z)} } {p(y=\\mathcal{D}_y)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* ${\\color{red}p(z)}$ is the **prior** distribution\n",
    "* ${\\color{blue}p(y=\\mathcal{D}_y|z)}$ is the **likelihood** function\n",
    "* ${\\color{magenta}p(y=\\mathcal{D}_y, z)}$ is the **joint likelihood** (product of likelihood function with prior distribution)\n",
    "* $p(y=\\mathcal{D}_y)$ is the **marginal likelihood**\n",
    "* ${\\color{green}p(z|y=\\mathcal{D}_y)}$ is the **posterior**\n",
    "\n",
    "We can write Bayes' rule as <font color='green'>posterior</font> $\\propto$ <font color='blue'>likelihood</font> $\\times$ <font color='red'>prior </font>, where we are ignoring the denominator $p(y=\\mathcal{D}_y)$ because it is just a **constant** independent of the hidden variable $z$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bayes' rule applied to observed data\n",
    "\n",
    "But remember that Bayes' rule is just a way to calculate the posterior:\n",
    "\n",
    "$$\n",
    "p(z|y=\\mathcal{D}_y) = \\frac{ p(y=\\mathcal{D}_y|z)p(z) } {p(y=\\mathcal{D}_y)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Usually, what we really want is to be able to predict the distribution of the quantity of interest (here: $y$) after observing some data $\\mathcal{D}_y$:\n",
    "\n",
    "$$\\require{color}\n",
    "{\\color{orange}p(y|y=\\mathcal{D}_y)} = \\int p(y|z) p(z|y=\\mathcal{D}_y) dz\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "which is often written in simpler notation: $p(y|\\mathcal{D}_y) = \\int p(y|z) p(z|\\mathcal{D}_y) dz$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Bayesian inference for car stopping distance problem\n",
    "\n",
    "Now we will solve the first Bayesian ML problem from some given data $y=\\mathcal{D}_y$:\n",
    "\n",
    "| $y_i$ (m) |\n",
    "| ---- |\n",
    "| 601.5 |\n",
    "| 705.9 |\n",
    "| 693.8 |\n",
    "| ...   |\n",
    "| 711.3 |\n",
    "\n",
    "where the data $\\mathcal{D}_y$ could be a Pandas dataframe with $N$ data points ($N$ rows)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* **Very Important Question (VIQ)**: Can we calculate the **likelihood** function from this data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Likelihood for car stopping distance problem\n",
    "\n",
    "Of course! As we saw a few cells ago, the **likelihood** is obtained by evaluating the **observation distribution** at the data $\\mathcal{D}_y$.\n",
    "\n",
    "Noting that each observation in $\\mathcal{D}_y$ is independent of each other, then:\n",
    "\n",
    "$$\n",
    "p(y=\\mathcal{D}_y | z) = \\prod_{i=1}^{N} p(y=y_i|z) = p(y=y_1|z)p(y=y_2|z) \\cdots p(y=y_N|z)\n",
    "$$\n",
    "\n",
    "which gives the **probability density** of observing that data if using our observation distribution (part of our model!)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Calculating the likelihood\n",
    "\n",
    "Let's calculate it:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "p(y=\\mathcal{D}_y | z) &= \\prod_{i=1}^{N} p(y=y_i|z) \\\\\n",
    "&= \\prod_{i=1}^{N} \\frac{1}{C_{y|z}} \\exp\\left\\{ -\\frac{1}{2\\left(\\frac{\\sigma_{y|z}}{w}\\right)^2}\\left[z-\\left(\\frac{y_i-b}{w}\\right)^2\\right]\\right\\}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "This seems a bit daunting... I know. Do not dispair yet!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##### Product of Gaussian pdf's of the same rv $z$\n",
    "\n",
    "It can be shown that the product of $N$ univariate Gaussian pdf's of the same rv $z$ is:\n",
    "\n",
    "$$\n",
    "\\prod_{i=1}^{N} \\mathcal{N}(z|\\mu_i, \\sigma_i^2) = C \\cdot \\mathcal{N}(z|\\mu, \\sigma^2)\n",
    "$$\n",
    "\n",
    "with mean: $\\mu = \\sigma^2 \\left( \\sum_{i=1}^{N} \\frac{\\mu_i}{\\sigma_i^2}\\right)$\n",
    "\n",
    "variance: $\\sigma^2= \\frac{1}{\\sum_{i=1}^{N} \\frac{1}{\\sigma_i^2}}$\n",
    "\n",
    "and normalization constant: $C = \\frac{1}{2\\pi^{(N-1)/2}}\\sqrt{\\frac{\\sigma^2}{\\prod_{i=1}^n \\sigma_i^2}} \\exp\\left[-\\frac{1}{2}\\left(\\sum_{i=1}^{N} \\frac{\\mu_i^2}{\\sigma_i^2} - \\frac{\\mu^2}{\\sigma^2}\\right)\\right]$\n",
    "\n",
    "Curiosity: the normalization constant $C$ is itself a Gaussian! You can see it more clearly if you consider $N=2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Note that the normalization constant shown in the previous cell can also be written as:\n",
    "\n",
    "$$\n",
    "C = \\frac{1}{2\\pi^{(N-1)/2}}\\sqrt{\\frac{\\sigma^2}{\\prod_{i=1}^n \\sigma_i^2}} \\exp\\left[-\\frac{1}{2}\\left(\\sum_{i=1}^{N-1}\\sum_{j=i+1}^{N} \\frac{(\\mu_i-\\mu_j)^2}{\\sigma_i^2 \\sigma_j^2}\\sigma^2\\right)\\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <font color='red'>HOMEWORK</font>\n",
    "\n",
    "Show that the product of two Gaussian <a title=\"probability density functions\">pdf's</a> for the same <a title=\"random variable\">rv</a> $z$ is:\n",
    "\n",
    "$\\mathcal{N}(z|\\mu_1, \\sigma_1^2)\\cdot \\mathcal{N}(z|\\mu_2, \\sigma_2^2)= C \\cdot \\mathcal{N}(z | \\mu, \\sigma^2)$\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\sigma^2&=\\frac{1}{\\sigma_1^2+\\sigma_2^2}\\\\\n",
    "\\mu&=\\frac{1}{\\sigma^2}(\\frac{\\mu_1}{\\sigma_1^2} +  \\frac{\\mu_2}{\\sigma_2^2})\\\\\n",
    "C &= \\frac{1}{\\sqrt{2\\pi(\\sigma_1^2+\\sigma_2^2)}} \\exp\\left[-\\frac{1}{2(\\sigma_1^2+\\sigma_2^2)}(\\mu_1-\\mu_2)^2\\right]\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Back to calculating the likelihood\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "p(y=\\mathcal{D}_y | z) &= \\prod_{i=1}^{N} p(y=y_i|z) \\\\\n",
    "&= \\prod_{i=1}^{N} \\frac{1}{|w|} \\frac{1}{\\sqrt{2\\pi \\left(\\frac{\\sigma_{y|z}}{w}\\right)^2}} \\exp\\left\\{ -\\frac{1}{2\\left(\\frac{\\sigma_{y|z}}{w}\\right)^2}\\left[z-\\left(\\frac{y_i-b}{w}\\right)^2\\right]\\right\\} \\\\\n",
    "&= \\frac{1}{|w|^N} \\prod_{i=1}^{N} \\frac{1}{\\sqrt{2\\pi \\left(\\frac{\\sigma_{y|z}}{w}\\right)^2}} \\exp\\left\\{ -\\frac{1}{2\\left(\\frac{\\sigma_{y|z}}{w}\\right)^2}\\left[z-\\left(\\frac{y_i-b}{w}\\right)^2\\right]\\right\\}\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "So, using the result of a product of $N$ Gaussian pdf's to calculate the likelihood, and noting that $\\sigma_i = \\frac{\\sigma_{y|z}}{w}$ and $\\mu_i = \\frac{y_i - b}{w}$ we get:\n",
    "\n",
    "$$\n",
    "p(y=\\mathcal{D}_y | z) = \\frac{1}{|w|^N} \\cdot C \\cdot \\frac{1}{2\\pi \\sigma^2} \\exp\\left[ -\\frac{1}{2\\sigma^2}(z-\\mu)^2\\right]\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$\\mu = \\frac{\\sigma^2}{\\sigma_i^2} \\sum_{i=1}^N \\mu_i = \\frac{w^2\\sigma^2}{\\sigma_{y|z}^2} \\sum_{i=1}^N \\mu_i$\n",
    "\n",
    "$\\sigma^2 = \\frac{1}{ \\sum_{i=1}^N \\frac{1}{\\sigma_i^2} } = \\frac{1}{ \\sum_{i=1}^N \\frac{w^2 N}{\\sigma_{y|z}^2} } = \\frac{\\sigma_{y|z}^2}{\\sum_{i=1}^N w^2 N}$\n",
    "\n",
    "$\n",
    "C = \\frac{1}{2\\pi^{(N-1)/2}} \\sqrt{\\frac{\\sigma^2}{\\left( \\frac{\\sigma_{y|z}^2}{w^2}\\right)^N}} \\exp\\left[-\\frac{1}{2}\\left(\\frac{w^2}{\\sigma_{y|z}^2}\\sum_{i=1}^N \\mu_i - \\frac{\\mu^2}{\\sigma^2}\\right) \\right]\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Calculating the marginal likelihood\n",
    "\n",
    "$$\\begin{align}\n",
    "p(y=\\mathcal{D}_y) &= \\int p(y=\\mathcal{D}_y | z) p(z) dz \\\\\n",
    "&= \\int \\frac{1}{|w|^N} C \\cdot \\mathcal{N}(z|\\mu, \\sigma^2)\\cdot \\frac{1}{C_z} dz\\\\\n",
    "&= \\frac{C}{|w|^N C_z} \\int \\cdot \\mathcal{N}(z|\\mu, \\sigma^2)\\cdot \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "We can now calculate the posterior:\n",
    "\n",
    "$$\\begin{align}\n",
    "p(z|y=\\mathcal{D}_y) &= \\frac{ p(y=\\mathcal{D}_y|z)p(z) } {p(y=\\mathcal{D}_y)} \\\\\n",
    "&= \\frac{1}{p(y=\\mathcal{D}_y)} \\cdot \\frac{1}{|w|^N} C \\cdot \\mathcal{N}(z|\\mu,\\sigma^2) \\cdot \\frac{1}{C_z} \\\\\n",
    "&= \\mathcal{N}(z|\\mu, \\sigma^2)\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Calculating the Predictive Posterior Distribution (PPD)\n",
    "\n",
    "Having found the posterior, we can determine the PPD:\n",
    "\n",
    "$$\n",
    "p(y|\\mathcal{D}_y) = \\int p(y| z) p(z|\\mathcal{D}_y) dz\n",
    "$$\n",
    "\n",
    "To calculate this, we will have to use the identity for a product of two Gaussians.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "p(y|\\mathcal{D}_y) &= \\int \\frac{1}{|w|} \\mathcal{N}\\left(z|\\frac{y-b}{w}, \\left(\\frac{\\sigma_{y|z}}{w}\\right)^2\\right) \\mathcal{N}(z|\\mu, \\sigma^2) dz \\\\\n",
    "&= \\int \\frac{1}{|w|} C^* \\mathcal{N}\\left(z|\\mu^*, \\left(\\sigma^*\\right)^2\\right) dz \\\\\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "#### Calculating the Predictive Posterior Distribution (PPD)\n",
    "\n",
    "We can find these parameters from the identity for a product of two Gaussians.\n",
    "\n",
    "$$\n",
    "p(y|\\mathcal{D}_y) = \\int \\frac{1}{|w|} C^* \\mathcal{N}\\left(z|\\mu^*, \\left(\\sigma^*\\right)^2\\right) dz\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$\\mu^* = \\left(\\sigma^* \\right)^2 \\left( \\frac{\\mu}{\\sigma^2} + \\frac{(y-b)/w}{\\left(\\frac{\\sigma_{y|z}}{w}\\right)^2} \\right) = \\left(\\sigma^* \\right)^2 \\left( \\frac{\\mu}{\\sigma^2} + \\frac{(y-b)\\cdot w}{\\sigma_{y|z}^2} \\right)$\n",
    "\n",
    "$\\left( \\sigma^* \\right)^2 = \\frac{1}{\\frac{1}{\\sigma^2}+\\frac{1}{\\left( \\frac{\\sigma_{y|z}}{w}\\right)^2}}= \\frac{1}{\\frac{1}{\\sigma^2}+\\frac{w^2}{\\sigma_{y|z}^2}}$\n",
    "\n",
    "$C^* = \\frac{1}{\\sqrt{2\\pi \\left( \\sigma^2 + \\frac{\\sigma_{y|z}^2}{w^2} \\right)}}\\exp\\left[ - \\frac{\\left(\\mu - \\frac{y-b}{w}\\right)^2}{2\\left( \\sigma^2+\\frac{\\sigma_{y|z}^2}{w^2}\\right)}\\right]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## In-class exercise\n",
    "\n",
    "Integrate the PPD $p(y|\\mathcal{D}_y)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### See you next class\n",
    "\n",
    "Have fun!"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "hide_code_all_hidden": false,
  "kernelspec": {
   "display_name": "Python [conda env:3dasm] *",
   "language": "python",
   "name": "conda-env-3dasm-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
