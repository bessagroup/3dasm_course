{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=../figures/Brown_logo.svg width=50%>\n",
    "\n",
    "# Data-Driven Design & Analyses of Structures & Materials (3dasm)\n",
    "\n",
    "## Lecture 26: A brief introduction to optimization (for ML)\n",
    "\n",
    "### Suryanarayanan M. S. | <a href = \"mailto: s.manojsanu@tudelft.nl\">s.manojsanu@tudelft.nl</a>  | PhD Candidate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Outline for today's lecture\n",
    "\n",
    "<img align=right src=./figures/grr.png width=60%>\n",
    "\n",
    "* Optimization problem formulation\n",
    "    * Objective, variables and constraints\n",
    "* Characteristics of optimization problems\n",
    "    * Modality\n",
    "    * Convexity\n",
    "* Solving optimization problems\n",
    "    * Classify optimizers\n",
    "    * Taylor series expansion\n",
    "    \n",
    "\n",
    "\n",
    "**References:**\n",
    "* *J. R. R. Martins* & Andrew Ning, Engineering Design Optimization, 2021 - Chapters 1 & 4\n",
    "* For practical details of algorithms (Extra):\n",
    "    * Nocedal, Jorge, and Stephen J. Wright. Numerical optimization. Springer Science & Business Media, 2006."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Why optimization matters?\n",
    "\n",
    "*In machine learning:*\n",
    "1. **Parameter estimation**\n",
    "    * Optimization is the tool to achieve our true goal: \"generalization\". \n",
    "    \n",
    "    *  $\\theta^* \\in \\text{arg}\\,\\min\\limits_{\\theta}\\, \\mathcal{L}(\\mathbf{\\theta})$, where $\\mathcal{L}$ is the loss function and $\\theta$ are the model parameters.\n",
    "    \n",
    "\n",
    "2. **Hyper-parameter tuning**\n",
    "    * Anything can be a hyper-parameter!\n",
    "\n",
    "\n",
    "*In the data-driven process:*\n",
    "\n",
    "<img src=./figures/blocks.png width=100%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Setting up an optimization problem\n",
    "\n",
    "**General formulation**\n",
    "\n",
    "<img src=./figures/general_form.png width=50%>\n",
    "\n",
    "* $\\vec{x}$ are decision variables.\n",
    "    * $\\underline{x}$ and $\\overline{x}$ are the lower and upper bounds.\n",
    "* $f(\\vec{x})$ is the objective function.\n",
    "* $g$ and $h$ are constraint functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Note:\n",
    "1. The lecture assumes this formulation\n",
    "2. Different textbooks follow different conventions\n",
    "    * Optimization packages (and sometimes functions within a package) might follow different formulaions\n",
    "    * Always check whether the constraints are defined as given above (LHS form)\n",
    "    * E.g. [Scipy](https://docs.scipy.org/doc/scipy/tutorial/optimize.html#constrained-minimization-of-multivariate-scalar-functions-minimize) has two algorithms `SLSQP` and `trust-constr`. One uses a combined LHS and RHS form while the latter uses RHS form."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### A. Decision variables or Design variables\n",
    "\n",
    "<img style=\"float: right;\" src=\"./figures/dvs.png\" width=30%>\n",
    "\n",
    "* Represent the space of possibilities (aka Design space)\n",
    "    * To be searched for the best solution\n",
    "    \n",
    "* Continuous or discrete\n",
    "\n",
    "* Bounded or unbounded\n",
    "\n",
    "* No: of design variables = dimensionaity\n",
    "\n",
    "* Physically meaningful or abstract\n",
    "    * E.g. thickness of a beam or a neural network's weight parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### B. Objective function\n",
    "\n",
    "<img style=\"float: right;\" src=\"./figures/objective.png\" width=30%>\n",
    "\n",
    "* Performance metric to be optimized\n",
    "\n",
    "* Single or multiple objectives are possible\n",
    "\n",
    "    * E.g. Cost vs quality, Weight vs stiffness etc.\n",
    "\n",
    "* Can be linear or non-linear (in the design variables)\n",
    "\n",
    "* E.g. weight of a beam or accuracy of a neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "**Note**\n",
    "- Optimization is often shown as a minimization problem. Maximization problems can be converted to minimization problems by multiplying the objective function by -1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### C. Constraints\n",
    "\n",
    "<img style=\"float: right;\" src=\"./figures/constraint.png\" width=40%>\n",
    "\n",
    "\n",
    "* Functions that limit the design space.\n",
    "    * Linear or non-linear\n",
    "    * E.g. Stress in a beam should <= yield stress of the material.\n",
    "\n",
    "* Makes life (and optimization) much more difficult.\n",
    "    * Solution has to be **feasible** (not violate any constraints)\n",
    "    * Solution has to be **optimal** (best possible solution)\n",
    "\n",
    "* Equality or inequality\n",
    "    * Inequality is more difficult to handle!\n",
    "    * They can be on or off (active or inactive)\n",
    "    \n",
    "* Bounds on the design variables are called **box constraints**.\n",
    "    * However, they are much easier to handle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Narrowing our focus\n",
    "\n",
    "- Optimization is extremely vast\n",
    "- Let's simplify things for ML\n",
    "    - Discrete or continuous decision variables\n",
    "    - Single objective\n",
    "    - No constraints -> Unconstrained\n",
    "\n",
    "<img align=center src=\"./figures/narrow_focus.png\" width=50%>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Characteristics of an optimization problem\n",
    "\n",
    "* Attributes of the problem \n",
    "    * Helps classify the problem\n",
    "    * May help in getting better solutions with less resources!\n",
    "* Some important attributes are:\n",
    "    * Smoothness of the functions involved\n",
    "    * Linearity\n",
    "    * Stochasticity\n",
    "    * **Modality**\n",
    "    * **Convexity**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Modality\n",
    "\n",
    "* Modes = Minima (Remember our convention!)\n",
    "    * Minima are points that are better than their neighbours\n",
    "    * $x^*$ is a minima if $f(x^*) \\leq f(x)$ for every $x \\in \\{||x - x^*|| < \\epsilon\\ , \\epsilon > 0 \\}$\n",
    "    * Global minima vs local minima\n",
    "    \n",
    "<img align=right src=\"./figures/modality.png\" width=40%>\n",
    "\n",
    "* Multi-modal functions cause problems (like multi-modal distributions)\n",
    "    * Getting stuck in local minima\n",
    "    * Every global minimum is also a local minimum\n",
    "    \n",
    "**How do we know we have found the best solution possible ?**\n",
    "* In general, we don't!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Convexity\n",
    "\n",
    "* Purely mathematical\n",
    "* **Golden rule: If an optimization problem is convex, all local minima are global minima**\n",
    "\n",
    "\n",
    "* What is a convex optimization problem ?\n",
    "    * If all functions involved are convex\n",
    "    * ...\n",
    "\n",
    "* Convex functions should meet 2 criteria"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**A** : *The line segment joining any two points of the graph of the function has to be above the function itself.*\n",
    "* For any two points $\\vec{x}, \\vec{y}$ in the domain of $f$ and variable $0 \\leq \\alpha \\leq 1$ :\n",
    "    \n",
    "$$f(\\alpha \\vec{x} + (1 - \\alpha) \\vec{y}) \\leq \\alpha f(\\vec{x}) + (1 - \\alpha) f(\\vec{y})$$\n",
    "<img align=centre src=\"./figures/convexity.png\" width=60%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**B** : *The domain of the function should not have holes*\n",
    "* The line segment joining two points of a set should also lie in the set\n",
    "* For continuous design variables in unconstrained settings, this is always valid\n",
    "\n",
    "<img align=centre src=\"./figures/convexity.png\" width=60%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Solving optimization problems\n",
    "\n",
    "* Searching the design space = optimizers\n",
    "* Choice of optimizer is dependent on problem characteristics\n",
    "    * No-free lunch theorem!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Classification\n",
    "\n",
    "<img align=centre src=\"./figures/optimizers.png\" width=60%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "1. **Search strategy**\n",
    "    * Local: Search in the viscinity of their initialization\n",
    "        * Exploitative nature\n",
    "        * Finds the nearest minima\n",
    "        \n",
    "    * Global: Search the entire design space (or try)\n",
    "        * Exploratory in nature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "2. **Algorithm design**\n",
    "    * Heuristics\n",
    "        * Nature inspired or based on rules-of-thumb\n",
    "        * Robust (fewer assumptions) & general\n",
    "    * Mathematically designed\n",
    "        * Strictly converge to an optimal point*.\n",
    "        * Works well when the assumptions match the problem\n",
    "        * Very efficient\n",
    "        * Difficult to design and implement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "3. **Order of information used**\n",
    "\n",
    "* Practically, the most important classification\n",
    "* Lets dive a bit deeper!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Taylor series and order of information\n",
    "\n",
    "\n",
    "* Approximating a function $f$ at a given point $x_0$\n",
    "    \n",
    "$$f(x + \\alpha)|_{x=x_0} \\approx f(x_0) + \\frac{df}{dx}\\Big\\lvert_{x=x0} \\alpha + \\frac{1}{2}\\frac{d^2 f}{d x^2}\\Big\\lvert_{x=x_0}\\alpha^2 + \\mathcal{O}(\\alpha^3)$$ \n",
    " \n",
    " \n",
    "\n",
    "        \n",
    "\n",
    "* $\\alpha$ is the perturbation\n",
    "    * Since there is one direction, the perturbation is along $\\hat{x}$\n",
    "* More terms implies a better approximation\n",
    "    * The error in the approximation scales as $\\alpha^{n+1}$, if we include only n terms\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**In n-dimensions**\n",
    "\n",
    "\n",
    "$$ f(\\vec{x} + \\alpha \\vec{p})|_{x=\\vec{x}_0} \\approx f(\\vec{x}_0) + \\alpha \\nabla f(\\vec{x}_0)^T \\vec{p} + \\frac{1} {2}\\alpha^2 \\vec{p}^T   \\mathbf{H}(\\vec{x}_0)   \\vec{p}$$\n",
    "\n",
    "* The main difference is that we perturb along a given direction!\n",
    "* This is the form used for many optimization algorithms "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Three terms in the series\n",
    "    * The value of the function at the x (our point of interest) - **Zeroth term**\n",
    "    \n",
    "    * The gradient - vector of partial derivatives - **First order term**\n",
    "        - Generalization of the first-derivative to n dimensions\n",
    "        - Magnitude\n",
    "         $$ ||\\nabla f(\\vec{x})|| = \\Big[\\frac{\\partial f}{ \\partial x_1}, \\frac{\\partial f}{ \\partial x_2} ..., \\frac{\\partial f}{ \\partial x_n} \\Big]$$\n",
    "        - Direction - steepest ascent of the function. i.e. the direction in which the function increases the fastest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Hessian -  matrix of second-order derivatives - **Second order term**\n",
    "    - Generalization of the second-derivative to n dimensions - A symmetric matrix with size n x n\n",
    "$$ H(\\vec{x}) = \\begin{bmatrix} \\frac{\\partial^2 f}{\\partial x_1^2} & \\cdots & \\frac{\\partial^2 f}{\\partial x_1 \\partial x_n} \\\\ \\vdots &  \\ddots & \\vdots \\\\ \\frac{\\partial^2 f}{\\partial x_n \\partial x_1} & \\cdots & \\frac{\\partial^2 f}{\\partial x_n^2} \\end{bmatrix} $$\n",
    "\n",
    "    - This matrix represents the curvature of the function (since it is a change in the gradients!)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Zeroth-order optimizers\n",
    "    * Uses only function evaluations\n",
    "    * E.g. hyper-parameter tuning\n",
    "* First-order optimizers\n",
    "    * Gradient-based optimizers\n",
    "    * Most common in ML\n",
    "* Second-order optimizers\n",
    "    * The fastest and most expensive optimizers\n",
    "    * Not too popular in ML\n",
    "    * E.g. K-FAC, Shampoo etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summary\n",
    "\n",
    "* General form of an optimization problem\n",
    "* Constraints make optimization difficult\n",
    "* Convex optimization problems are super nice!\n",
    "* Optimizers - Algorithms for searching the design space for solutions\n",
    "\n",
    "**Next lecture**\n",
    "* More about optimizers\n",
    "* Optimality criteria - To identify minima\n",
    "* Automatic differentiation"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
