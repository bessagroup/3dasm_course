{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=../figures/Brown_logo.svg width=50%>\n",
    "\n",
    "## Data-Driven Design & Analyses of Structures & Materials (3dasm)\n",
    "\n",
    "## Lecture 28\n",
    "\n",
    "### Miguel A. Bessa | <a href = \"mailto: miguel_bessa@brown.edu\">miguel_bessa@brown.edu</a>  | Associate Professor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "**What:** A lecture of the \"3dasm\" course\n",
    "\n",
    "**Where:** This notebook comes from this [repository](https://github.com/bessagroup/3dasm_course)\n",
    "\n",
    "**Reference for entire course:** Murphy, Kevin P. *Probabilistic machine learning: an introduction*. MIT press, 2022. Available online [here](https://probml.github.io/pml-book/book1.html)\n",
    "\n",
    "**How:** We try to follow Murphy's book closely, but the sequence of Chapters and Sections is different. The intention is to use notebooks as an introduction to the topic and Murphy's book as a resource.\n",
    "* If working offline: Go through this notebook and read the book.\n",
    "* If attending class in person: listen to me (!) but also go through the notebook in your laptop at the same time. Read the book.\n",
    "* If attending lectures remotely: listen to me (!) via Zoom and (ideally) use two screens where you have the notebook open in 1 screen and you see the lectures on the other. Read the book."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "**Optional reference (the \"bible\" by the \"bishop\"... pun intended ðŸ˜†) :** Bishop, Christopher M. *Pattern recognition and machine learning*. Springer Verlag, 2006.\n",
    "\n",
    "**References/resources to create this notebook:**\n",
    "* This simple tutorial is still based on a script I created for this article: https://imechanica.org/node/23957\n",
    "* It follows from some examples provided by the scikit-learn user guide, which seem to have originated from Mathieu Blondel, Jake Vanderplas, Vincent Dubourg, and Jan Hendrik Metzen.\n",
    "\n",
    "Apologies in advance if I missed some reference used in this notebook. Please contact me if that is the case, and I will gladly include it here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "## **OPTION 1**. Run this notebook **locally in your computer**:\n",
    "1. Confirm that you have the '3dasm' mamba (or conda) environment (see Lecture 1).\n",
    "2. Go to the 3dasm_course folder in your computer and pull the last updates of the [repository](https://github.com/bessagroup/3dasm_course):\n",
    "```\n",
    "git pull\n",
    "```\n",
    "    - Note: if you can't pull the repo due to conflicts (and you can't handle these conflicts), use this command (with **caution**!) and your repo becomes the same as the one online:\n",
    "```\n",
    "git reset --hard origin/main\n",
    "```\n",
    "3. Open command window and load jupyter notebook (it will open in your internet browser):\n",
    "```\n",
    "jupyter notebook\n",
    "```\n",
    "5. Open notebook of this Lecture and choose the '3dasm' kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "## **OPTION 2**. Use **Google's Colab** (no installation required, but times out if idle):\n",
    "\n",
    "1. go to https://colab.research.google.com\n",
    "2. login\n",
    "3. File > Open notebook\n",
    "4. click on Github (no need to login or authorize anything)\n",
    "5. paste the git link: https://github.com/bessagroup/3dasm_course\n",
    "6. click search and then click on the notebook for this Lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": true,
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "# Basic plotting tools needed in Python.\n",
    "\n",
    "import matplotlib.pyplot as plt # import plotting tools to create figures\n",
    "import numpy as np # import numpy to handle a lot of things!\n",
    "from IPython.display import display, Math # to print with Latex math\n",
    "\n",
    "%config InlineBackend.figure_format = \"retina\" # render higher resolution images in the notebook\n",
    "plt.rcParams[\"figure.figsize\"] = (8,4) # rescale figure size appropriately for slides"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Outline for today\n",
    "\n",
    "* Gaining some intuition about Artificial Neural Networks\n",
    "\n",
    "**Reading material**: This notebook + (ANNs in Chapter 13)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Today focuses on explaining how we train artificial neural networks.\n",
    "\n",
    "However, before we start with that, let's see what happens when we consider a feedforward neural network with **linear activation functions**!\n",
    "\n",
    "* Draw on the board a simple feedforward ANN with 2 hidden layers for multidimensional inputs (2) and outputs (2), considering the first hidden layer with 3 neurons and the second hidden layer with 2 neurons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We can write the neurons of a neural network with $L$ hidden layers in the following way:\n",
    "\n",
    "$x_i \\equiv n_{i,0} \\qquad n_{k,l} = f_{k,l}(b_{k,l-1}+W_{jk,l-1}n_{j,l-1}) \\qquad n_{q,L+1}\\equiv y_q$\n",
    "\n",
    "* Layer $0$ is the **input layer**:\n",
    "    * $n_{i,0}$ are the input neurons, which are equal to the inputs (features) $x_i$, where $i = 1, \\ldots, D_0$ with $D_0 \\equiv D_x$ being the input dimension (dimension of $\\mathbf{x}$);\n",
    "* **Hidden layer** $l$:\n",
    "    * $n_{k,l}$ are the hidden neurons, where $k = 1, \\ldots, D_l$ with $D_l$ being the number of neurons in layer $l$, and where $f_{k,l}$ is the **activation function** for neuron $k$ of layer $l$ (**Note:** Usually the same activation function is used for all neurons in the same layer, so $f_{1,l}=f_{2,l}=\\ldots=f_{d_l,l}$).\n",
    "* Layer $L+1$ is the **output layer**:\n",
    "    * $n_{q,L+1}$ are the output neurons, which are equal to the outputs (targets) $y_q$, where $q = 1, \\ldots, D_{L+1}$ with $D_{L+1}\\equiv D_y$ being the output dimension (dimension of $\\mathbf{y}$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## A note about Linear activation functions\n",
    "\n",
    "$x_i \\equiv n_{i,0} \\qquad n_{k,l} = f_{k,l}(b_{k,l-1}+W_{jk,l-1}n_{j,l-1}) \\qquad n_{q,L+1}\\equiv y_q$\n",
    "\n",
    "So, if we use **linear activation functions**, $f(u) = u$, each neuron is simply given by:\n",
    "\n",
    "$n_{k,l} = b_{k,l-1}+W_{jk,l-1}n_{j,l-1} \\qquad \\text{or in tensor notation:} \\qquad \\mathbf{n}_l = \\mathbf{b}_{l-1} + \\mathbf{W}_{l-1} \\mathbf{n}_{l-1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example of ANN with linear activation functions (why it is not used!)\n",
    "\n",
    "For example, an ANN with $L=2$ hidden layers and linear activation functions has the following neurons:\n",
    "\n",
    "* Layer 0 (input layer): $\\mathbf{n}_0 = \\mathbf{x}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Layer 1 (first hidden layer): $\\require{color}{\\color{blue}\\mathbf{n}_1} = {\\color{blue}\\mathbf{b}_0} + {\\color{blue}\\mathbf{W}_0}\\mathbf{n}_0 = {\\color{blue}\\mathbf{b}_0} + {\\color{blue}\\mathbf{W}_0}\\mathbf{x}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Layer 2 (second hidden layer): $\\require{color}{\\color{red}\\mathbf{n}_2} = {\\color{red}\\mathbf{b}_1 + \\mathbf{W}_1}{\\color{blue}\\mathbf{n}_1} \\Rightarrow$\n",
    "\n",
    "    $\\Rightarrow {\\color{red}\\mathbf{n}_2} = {\\color{red}\\mathbf{b}_1} + {\\color{red}\\mathbf{W}_1}({\\color{blue}\\mathbf{b}_0} + {\\color{blue}\\mathbf{W}_0} \\mathbf{x}) = ({\\color{red}\\mathbf{b}_1}+{\\color{red}\\mathbf{W}_1}{\\color{blue}\\mathbf{b}_0})+ {\\color{red}\\mathbf{W}_1}{\\color{blue}\\mathbf{W}_0}\\mathbf{x}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Layer $L+1=3$ (output layer): $\\require{color}\\mathbf{y} = {\\color{orange}\\mathbf{n}_3} = {\\color{orange}\\mathbf{b}_2} + {\\color{orange}\\mathbf{W}_2}{\\color{red}\\mathbf{n}_2} \\Rightarrow$\n",
    "\n",
    "    $\\Rightarrow \\mathbf{y} = {\\color{orange}\\mathbf{b}_2} + {\\color{orange}\\mathbf{W}_2}({\\color{red}\\mathbf{b}_1}+{\\color{red}\\mathbf{W}_1}{\\color{blue}\\mathbf{b}_0}+ {\\color{red}\\mathbf{W}_1}{\\color{blue}\\mathbf{W}_0}\\mathbf{x}) = \\underbrace{({\\color{orange}\\mathbf{b}_2}+{\\color{orange}\\mathbf{W}_2}{\\color{red}\\mathbf{b}_1}+{\\color{orange}\\mathbf{W}_2}{\\color{red}\\mathbf{W}_1}{\\color{blue}\\mathbf{b}_0})}_{\\tilde{\\mathbf{b}}}+ \\underbrace{{\\color{orange}\\mathbf{W}_2}{\\color{red}\\mathbf{W}_1}{\\color{blue}\\mathbf{W}_0}}_{\\tilde{\\mathbf{W}}}\\mathbf{x}$\n",
    "    \n",
    "Conclusion: only using **linear activation functions** leads to a linear prediction!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "For this reason, only having linear activation functions is **not commonly considered**!\n",
    "\n",
    "However, please note that as soon as you consider ANY nonlinear activation function, the output prediction becomes nonlinear:\n",
    "\n",
    "$\\require{color}\\mathbf{y} = {\\color{orange}\\mathbf{f}_3} \\left\\{ {\\color{orange}\\mathbf{W}_2} {\\color{red}\\mathbf{f}_2}\\left[{\\color{red}\\mathbf{W}_1}{\\color{blue}\\mathbf{f}_1}\\left( {\\color{blue}\\mathbf{W}_0} \\mathbf{x} + {\\color{blue}\\mathbf{b}_0}\\right)+{\\color{red}\\mathbf{b}_1}\\right] + {\\color{orange}\\mathbf{b}_2}\\right\\}$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Simple rules of thumb when training neural networks:\n",
    "\n",
    "* For regression problems: linear activation function for the **output layer** (why??)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* For classification problems: sigmoid (or softmax) activation function for the **output layer** (why??)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* For predicting a smooth map: use smooth (differentiable) activation functions in the hidden layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Do not start with a huge neural network with many hidden layers and neurons! In fact, it's good to start training simple models and gradually increase complexity, as you need it.\n",
    "    * For example, start by training a linear model, then a kernel machine, and only then a neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's explore ANNs with a popular interactive demo: https://playground.tensorflow.org."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Consider the simple classification problem of the \"**Circle**\" dataset in that demo:\n",
    "\n",
    "    1. Considering **ReLu** activation function and the same **2 hidden layers** with **4 neurons each** (all other hyperparameters are visible at the top of the page): [click here](https://playground.tensorflow.org/#activation=relu&batchSize=10&dataset=circle&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=0&networkShape=4,2&seed=0.74415&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false)\n",
    "    \n",
    "    2. Considering **Tanh** activation function and the same 2 hidden layers with 4 neurons each (all other hyperparameters are visible at the top of the page): [click here](https://playground.tensorflow.org/#activation=tanh&batchSize=10&dataset=circle&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=0&networkShape=4,2&seed=0.38062&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false)\n",
    "\n",
    "    3. Considering **Linear** activation function and the same 2 hidden layers with 4 neurons each (all other hyperparameters are visible at the top of the page): [click here](https://playground.tensorflow.org/#activation=linear&batchSize=10&dataset=circle&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=0&networkShape=4,2&seed=0.38062&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false)\n",
    "    \n",
    "    4. Still considering **ReLu** activation function but now with **6 hidden layers** with **8 neurons** each (all other hyperparameters are visible at the top of the page): [click here](https://playground.tensorflow.org/#activation=relu&batchSize=10&dataset=circle&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=0&networkShape=8,8,8,8,8,8&seed=0.74136&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false)\n",
    "\n",
    "    5. Considering **Tanh** activation function and the same 6 hidden layers with 8 neurons each (all other hyperparameters are visible at the top of the page): [click here](https://playground.tensorflow.org/#activation=tanh&batchSize=10&dataset=circle&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=0&networkShape=8,8,8,8,8,8&seed=0.74136&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "There are many characteristics of the training of ANNs that you can understand empirically by exploring the above-mentioned [interactive demo](https://playground.tensorflow.org) (for both regression and classification):\n",
    "\n",
    "* Even the ReLu activation function is capable of nonlinear predictions! It's also faster to train than other activation functions.\n",
    "\n",
    "* Tanh activation function leads to smooth predictions because it is infinitely differentiable.\n",
    "\n",
    "* More complicated neural networks are not necessarily better. You do not know a priori how many layers and how many neurons you need to train a good model. It depends on the data characteristics and quantity!\n",
    "\n",
    "* Neural networks with more layers and neurons (more parameters) are slower to train! Each epoch takes longer and you can see that from playing the above demos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Defining ANNs from a Bayesian perspective\n",
    "\n",
    "Although ANNs are usually viewed in the literature from a deterministic perspective, this does not need to be the case.\n",
    "\n",
    "* For classification, they are a very trivial generalization from Logistic regression\n",
    "\n",
    "* For regression, they are a very trivial generalization from Linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "ANN for **classification** problems is defined similarly to what we did for Logistic regression, but using a nonlinear function to model the mean:\n",
    "\n",
    "1. Categorical observation distribution: $p(\\mathbf{y}|\\mathbf{x}, \\mathbf{z}) = \\text{Cat}\\left(\\mathbf{y}| \\boldsymbol{\\rho} = \\mathbf{f}(\\mathbf{x};\\, \\mathbf{z}) \\right)$\n",
    "\n",
    "    where $\\mathbf{z} \\equiv \\{\\mathbf{W}, \\mathbf{b}\\}$ are all the unknown model parameters (weights and biases of every layer) and where $\\mathbf{f}(\\mathbf{x};\\, \\mathbf{z})$ is nonlinear in the weights due to the choice of activation functions (the activation function in the **output layer** is the **softmax activation**, just like in logistic regression).\n",
    "\n",
    "2. Uniform prior distribution for each hidden rv $\\mathbf{z}$: $p(\\mathbf{z}) \\propto 1$\n",
    "    \n",
    "    * Note: we can also use other priors! For example a Gaussian prior ($\\ell_2$ regularization) or a Laplace prior ($\\ell_1$ regularization).\n",
    "\n",
    "3. MLE point estimate for posterior: $\\hat{\\mathbf{z}}_{\\text{mle}} = \\underset{z}{\\mathrm{argmin}}\\left[-\\sum_{i=1}^{N}\\log{ p(\\mathbf{y}=\\mathbf{y}_i|\\mathbf{x}=\\mathbf{x}_i, \\mathbf{z})}\\right]$\n",
    "    * Note: if we use a different prior, then it becomes the MAP estimate.\n",
    "\n",
    "Final prediction is given by the <font color='orange'>PPD</font>: $\\require{color}\n",
    "{\\color{orange}p(\\mathbf{y}^*|\\mathbf{x}^*, \\mathcal{D})} = \\int p(\\mathbf{y}^*|\\mathbf{x}^*,\\mathbf{z}) \\delta(\\mathbf{z}-\\hat{\\mathbf{z}}) d\\mathbf{z} = p(\\mathbf{y}^*|\\mathbf{x}^*, \\mathbf{z}=\\hat{\\mathbf{z}})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "ANN for **regression** problems is defined similarly to what we did for Linear regression, but using a nonlinear function to model the mean:\n",
    "\n",
    "1. Gaussian observation distribution (**ignore variance**): $p(\\mathbf{y}|\\mathbf{x}, \\mathbf{z}) = \\mathcal{N}(\\mathbf{y}| \\boldsymbol{\\mu}_{y|z} = \\mathbf{f}(\\mathbf{x};\\, \\mathbf{z}), \\sigma_{y|z}^2 = \\sigma^2)$\n",
    "\n",
    "    where $\\mathbf{z} \\equiv \\{\\mathbf{W}, \\mathbf{b}\\}$ are all the unknown model parameters (weights and biases of every layer) and where $\\mathbf{f}(\\mathbf{x};\\, \\mathbf{z})$ is nonlinear in the weights due to the choice of activation functions (the activation function in the **output layer** is the **linear activation**, just like in linear regression).\n",
    "\n",
    "2. Uniform prior distribution for each hidden rv $\\mathbf{z}$: $p(\\mathbf{z}) \\propto 1$\n",
    "    \n",
    "    * Note: we can also use other priors! For example a Gaussian prior ($\\ell_2$ regularization) or a Laplace prior ($\\ell_1$ regularization).\n",
    "\n",
    "3. MLE point estimate for posterior: $\\hat{\\mathbf{z}}_{\\text{mle}} = \\underset{z}{\\mathrm{argmin}}\\left[-\\sum_{i=1}^{N}\\log{ p(\\mathbf{y}=\\mathbf{y}_i|\\mathbf{x}=\\mathbf{x}_i, \\mathbf{z})}\\right]$\n",
    "    * Note: if we use a different prior, then it becomes the MAP estimate.\n",
    "\n",
    "Final prediction is given by the <font color='orange'>PPD</font>: $\\require{color}\n",
    "{\\color{orange}p(\\mathbf{y}^*|\\mathbf{x}^*, \\mathcal{D})} = \\int p(\\mathbf{y}^*|\\mathbf{x}^*,\\mathbf{z}) \\delta(\\mathbf{z}-\\hat{\\mathbf{z}}) d\\mathbf{z} = p(\\mathbf{y}^*|\\mathbf{x}^*, \\mathbf{z}=\\hat{\\mathbf{z}})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In both cases (classification and regression), we need to calculate the point estimate of the unknowns $\\mathbf{z}$.\n",
    "\n",
    "Without loss of generality, let's start by focusing on the **regression** case and assuming uniform prior, i.e. no \"regularization\" (as we did in Lecture 11 for Linear Least Squares):\n",
    "\n",
    "$$\\begin{align}\n",
    "\\hat{\\mathbf{z}}_{\\text{mle}} &= \\underset{z}{\\mathrm{argmin}}\\left[\\text{NLL}(\\mathbf{z})\\right]\n",
    "\\\\\n",
    "&= \\underset{z}{\\mathrm{argmin}}\\left[-\\sum_{n=1}^{N}\\log{ p(\\mathbf{y}=\\mathbf{y}_n|\\mathbf{x}=\\mathbf{x}_n, \\mathbf{z})}\\right]\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "If we used a prior then the point estimate would "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "$$\n",
    "\\begin{align}\n",
    "\\hat{\\mathbf{z}}_{\\text{mle}} &= \\underset{z}{\\mathrm{argmin}}\\left[-\\sum_{n=1}^{N}\\log{ p(\\mathbf{y}=\\mathbf{y}_n|\\mathbf{x}=\\mathbf{x}_n, \\mathbf{z})}\\right] \\\\\n",
    "&= \\underset{z}{\\mathrm{argmin}}\\left[-\\sum_{n=1}^{N}\\log{\\left( \\frac{1}{\\sqrt{2\\pi \\sigma^2}} \\exp\\left\\{ -\\frac{1}{2\\sigma^2}\\left[\\mathbf{y}_n-\\mathbf{f}(\\mathbf{x}_n;\\, \\mathbf{z})\\right]^T\\left[\\mathbf{y}_n-\\mathbf{f}(\\mathbf{x}_n;\\, \\mathbf{z})\\right]\\right\\}\\right)}\\right]\\\\\n",
    "&= \\underset{z}{\\mathrm{argmin}}\\left[\\frac{N}{2}\\log{\\left(2\\pi \\sigma^2\\right)}+\\frac{1}{2 \\sigma^2}\\sum_{n=1}^{N}\\left[\\mathbf{y}_n-\\mathbf{f}(\\mathbf{x}_n;\\, \\mathbf{z})\\right]^T\\left[\\mathbf{y}_n-\\mathbf{f}(\\mathbf{x}_n;\\, \\mathbf{z})\\right] \\right]\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where we recall that the unknowns are $\\mathbf{z}$ (the weights and biases of the network), and where we can ignore the variance term (because we assume it to be constant, i.e. we will not solve for it)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Therefore, the point estimate of the ANN for regression becomes:\n",
    "\n",
    "$$\n",
    "\\hat{\\mathbf{z}}_{\\text{mle}} = \\underset{z}{\\mathrm{argmin}}\\left[\\frac{1}{2}\\sum_{n=1}^{N}\\left[\\mathbf{y}_n-\\mathbf{f}(\\mathbf{x}_n;\\, \\mathbf{z})\\right]^T\\left[\\mathbf{y}_n-\\mathbf{f}(\\mathbf{x}_n;\\, \\mathbf{z})\\right] \\right]\n",
    "$$\n",
    "\n",
    "where the argument is called the $\\ell_2$ loss function, i.e. the square of the $\\ell_2$ norm (equivalently, we could also use mean squared error loss function if we divide the $\\ell_2$ loss by the number of samples $N$).\n",
    "\n",
    "As we have seen previously, we can rewrite the $\\ell_2$ loss as follows:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}\\left[(\\mathbf{x},\\mathbf{y}), \\mathbf{z} \\right] = \\frac{1}{2} ||\\mathbf{Y}-\\mathbf{f}(\\mathbf{X},\\mathbf{z}) ||_2^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Note 1: Similarly, we can  find the loss function if we consider a nonuniform prior (see notes below for the case of a Gaussian prior)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Note 2: The loss function that you obtain for the **classification** case is different! It becomes the **cross-entropy** loss function, as we derived in Lecture 21, but where the prediction now depends on a nonlinear function: $\\mathbf{f}(\\mathbf{x}_n;\\, \\mathbf{z})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "### Explanation of Note 1: ANN for regression with Gaussian prior\n",
    "\n",
    "As we showed in Lecture 12 (Ridge regression), if we assume a nonuniform prior and the MAP point estimate (Lecture 8):\n",
    "\n",
    "$\\require{color}\n",
    "\\hat{\\mathbf{z}}_{\\text{map}} = \\underset{z}{\\mathrm{argmin}}\\left[-\\log{p(\\mathbf{y}=\\mathbf{y}_n|\\mathbf{x}=\\mathbf{x}_n, \\mathbf{z})} - \\log{{\\color{red}p(z)}}\\right]$\n",
    "\n",
    "For example, if the prior $p(z)$ is a Gaussian distribution on the weights $\\mathbf{w}$ (we can also use a Gaussian distribution for the biases):\n",
    "\n",
    "$p(\\mathbf{w}) = \\mathcal{N}(\\mathbf{w}| \\mathbf{0}, \\overset{\\scriptscriptstyle <}{\\sigma}_w^2 \\mathbf{I})$\n",
    "\n",
    "where $\\overset{\\scriptscriptstyle <}{\\sigma}_w$ is specified by us (hyperparameter), and where we are writing the weights of the ANN as a vector (we unraveled the weight matrix into a vector).\n",
    "\n",
    "* If we wrote the weights as a matrix, then we can use double contraction of the weight matrix with itself (leading to the same result as the inner product of the weight vector, of course)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Performing the calculation, the point estimate when assuming a Gaussian prior becomes:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\hat{\\mathbf{z}}_{\\text{map}} &= \\underset{z}{\\mathrm{argmin}}\\left[\\frac{1}{2\\sigma^2}\\sum_{n=1}^{N}\\left[\\mathbf{y}_n-\\mathbf{f}(\\mathbf{x}_n;\\, \\mathbf{z})\\right]^T\\left[\\mathbf{y}_n-\\mathbf{f}(\\mathbf{x}_n;\\, \\mathbf{z})\\right] + \\frac{1}{2\\overset{\\scriptscriptstyle <}{\\sigma}_w^2}\\mathbf{w}^T\\mathbf{w} \\right] \\\\\n",
    "&= \\underset{z}{\\mathrm{argmin}}\\left[\\frac{1}{2}\\sum_{n=1}^{N}\\left[\\mathbf{y}_n-\\mathbf{f}(\\mathbf{x}_n;\\, \\mathbf{z})\\right]^T\\left[\\mathbf{y}_n-\\mathbf{f}(\\mathbf{x}_n;\\, \\mathbf{z})\\right] + \\alpha\\mathbf{w}^T\\mathbf{w} \\right] \\\\\n",
    "&= \\underset{z}{\\mathrm{argmin}}\\left[\\frac{1}{2} ||\\mathbf{Y}-\\mathbf{f}(\\mathbf{X},\\mathbf{z}) ||_2^2 + \\alpha ||\\mathbf{w}||_2^2\\right]\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where $\\alpha = \\frac{\\sigma^2}{\\overset{\\scriptscriptstyle <}{2\\sigma}_w^2}$ is a hyperparameter (proportional to the strength of the prior and the value assumed for the variance of the observation distribution), and\n",
    "\n",
    "$||\\mathbf{w}||_2 = \\sqrt{\\sum |w_m|^2} = \\sqrt{\\mathbf{w}^T\\mathbf{w}} \\quad \\text{where the sum is over all weights (excluding biases) of the ANN.}$\n",
    "\n",
    "This is called the $l_2$ norm of the vector $\\mathbf{w}$. As we saw in Lecture 12 for Ridge regression, this causes a penalty on the weights that become too large in magnitude. In ML literature this is usually called $l_2$ **regularization** or **weight decay**, and is very widely used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Conclusion: as in the other ML models, finding the point estimate means that we need to minimize the loss function (finding minimum location of the NLL or of the negative log joint likelihood if the prior is not uniform)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* However, there is a **major** problem now: this function is no longer convex because of the mean $\\mathbf{f}(\\mathbf{x}_n;\\, \\mathbf{z})$ is a composition of nonlinear activation functions..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "So, how do we solve this optimization problem??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* We do the same thing as before! We use optimization algorithms that find the minimum of the loss function!\n",
    "\n",
    "* As we discussed for previous ML models, using first-order optimizers (gradient-based optimizers) is more efficient.\n",
    "    * So, **we need to calculate the gradient of the loss function** and then provide both the loss function and its gradient to an optimizer that will find an optimum (next lecture!)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### What is the consequence of the loss function not being convex?\n",
    "\n",
    "When finding the minimum of a non-convex function, we basically cross our fingers and hope that we land in a \"decent\" optimum (which almost never is the \"global\" optimum, contrary to what happened in linear models)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The conundrum we face in machine learning is to:\n",
    "\n",
    "* **\"Make everything as simple as possible, but not simpler\"** (paraphrasing Albert Einstein)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Linear models are nice because you can find the global optimum and you usually reach it very quickly (convex optimization problems are easy to solve). **BUT** linear models usually are **not sufficiently expressive** to predict complicated maps..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Nonlinear models with many parameters (and hyperparameters...) are **more expressive** (after all, a nonlinear model includes a linear model!), but the optimization problem to find a point estimate is more difficult to solve and we are not guaranteed to reach the global optimum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Still, there are countless examples where very expressive nonlinear models (different types of ANNs) outperform simpler models for complex tasks:\n",
    "\n",
    "* For example, classification problems on large datasets (e.g. ImageNet dataset), learning how to play chess or GO, ChatGPT, etc.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### See you next class\n",
    "\n",
    "Have fun!"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "hide_code_all_hidden": false,
  "kernelspec": {
   "display_name": "Python [conda env:3dasm]",
   "language": "python",
   "name": "conda-env-3dasm-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
