{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=../figures/Brown_logo.svg width=50%>\n",
    "\n",
    "## Data-Driven Design & Analyses of Structures & Materials (3dasm)\n",
    "\n",
    "## Lecture 12\n",
    "\n",
    "### Miguel A. Bessa | <a href = \"mailto: miguel_bessa@brown.edu\">miguel_bessa@brown.edu</a>  | Associate Professor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "**What:** A lecture of the \"3dasm\" course\n",
    "\n",
    "**Where:** This notebook comes from this [repository](https://github.com/bessagroup/3dasm_course)\n",
    "\n",
    "**Reference for entire course:** Murphy, Kevin P. *Probabilistic machine learning: an introduction*. MIT press, 2022. Available online [here](https://probml.github.io/pml-book/book1.html)\n",
    "\n",
    "**How:** We try to follow Murphy's book closely, but the sequence of Chapters and Sections is different. The intention is to use notebooks as an introduction to the topic and Murphy's book as a resource.\n",
    "* If working offline: Go through this notebook and read the book.\n",
    "* If attending class in person: listen to me (!) but also go through the notebook in your laptop at the same time. Read the book.\n",
    "* If attending lectures remotely: listen to me (!) via Zoom and (ideally) use two screens where you have the notebook open in 1 screen and you see the lectures on the other. Read the book."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "**Optional reference (the \"bible\" by the \"bishop\"... pun intended ðŸ˜†) :** Bishop, Christopher M. *Pattern recognition and machine learning*. Springer Verlag, 2006.\n",
    "\n",
    "**References/resources to create this notebook:**\n",
    "* Chapter 11 of Murphy's book.\n",
    "\n",
    "Apologies in advance if I missed some reference used in this notebook. Please contact me if that is the case, and I will gladly include it here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "## **OPTION 1**. Run this notebook **locally in your computer**:\n",
    "1. Confirm that you have the '3dasm' mamba (or conda) environment (see Lecture 1).\n",
    "2. Go to the 3dasm_course folder in your computer and pull the last updates of the [repository](https://github.com/bessagroup/3dasm_course):\n",
    "```\n",
    "git pull\n",
    "```\n",
    "    - Note: if you can't pull the repo due to conflicts (and you can't handle these conflicts), use this command (with **caution**!) and your repo becomes the same as the one online:\n",
    "        ```\n",
    "        git reset --hard origin/main\n",
    "        ```\n",
    "3. Open command window and load jupyter notebook (it will open in your internet browser):\n",
    "```\n",
    "jupyter notebook\n",
    "```\n",
    "5. Open notebook of this Lecture and choose the '3dasm' kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "## **OPTION 2**. Use **Google's Colab** (no installation required, but times out if idle):\n",
    "\n",
    "1. go to https://colab.research.google.com\n",
    "2. login\n",
    "3. File > Open notebook\n",
    "4. click on Github (no need to login or authorize anything)\n",
    "5. paste the git link: https://github.com/bessagroup/3dasm_course\n",
    "6. click search and then click on the notebook for this Lecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Outline for today\n",
    "\n",
    "* Derivation of a few more Linear Regression models:\n",
    "    - Ridge regression\n",
    "    - Lasso regression\n",
    "    - Bayesian linear regression\n",
    "\n",
    "**Reading material**: This notebook + Chapter 11 of the book."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summary of important Linear Regression Models\n",
    "\n",
    "Recall our view of Linear regression models from a Bayesian perspective: it's all about the choice of **likelihood** and **prior**!\n",
    "\n",
    "| Likelihood | Prior (on the weights)    | Posterior      | Name of the model | Book section  |\n",
    "|---        |---         |---             |---              |---            |\n",
    "| Gaussian  | Uniform    | Point estimate | Least Squares regression  | 11.2.2  |\n",
    "| Gaussian  | Gaussian    | Point estimate | Ridge regression   | 11.3  |\n",
    "| Gaussian  | Laplace    | Point estimate | Lasso regression  | 11.4  |\n",
    "| Student-$t$  | Uniform    | Point estimate | Robust regression   | 11.6.1  |\n",
    "| Laplace  | Uniform    | Point estimate | Robust regression   | 11.6.2  |\n",
    "| Gaussian  | Gaussian    | Gaussian | Bayesian linear regression   | 11.7 |\n",
    "\n",
    "Last Lecture we covered Least Squares regression. Today we will cover Ridge regression, Lasso regression and Bayesian linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Ridge regression: Linear regression with Gaussian likelihood, Gaussian prior and posterior via Point estimate\n",
    "\n",
    "| Likelihood | Prior (on the weights)    | Posterior      | Name of the model | Book section  |\n",
    "|---        |---         |---             |---              |---            |\n",
    "| Gaussian  | Gaussian    | Point estimate | Ridge regression   | 11.3  |\n",
    "\n",
    "1. Gaussian observation distribution: $p(y|\\mathbf{x}, \\mathbf{z}) = \\mathcal{N}(y| \\mu_{y|z} = \\mathbf{w}^T \\boldsymbol{\\phi}(\\mathbf{x}), \\sigma_{y|z}^2 = \\sigma^2)$\n",
    "\n",
    "    where $\\mathbf{z} = \\mathbf{w}$ are all the unknown model parameters (hidden rv's), but where $\\sigma$ is a **specified** value (i.e. $\\sigma$ is chosen by us, unlike in the last Lecture where we had considered it an unknown)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "2. But using a Gaussian prior for the weights $\\mathbf{w}$: $p(\\mathbf{w}) = \\mathcal{N}(\\mathbf{w}| \\mathbf{0}, \\overset{\\scriptscriptstyle <}{\\sigma}_w^2 \\mathbf{I})$ where $\\overset{\\scriptscriptstyle <}{\\sigma}_w$ is **specified** by us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "3. MAP point estimate for posterior: $\\hat{\\mathbf{z}}_{\\text{map}} = \\underset{z}{\\mathrm{argmin}}\\left[-\\sum_{i=1}^{N}\\log{ p(y=y_i|\\mathbf{z})} - \\log{p(\\mathbf{w})}\\right]$,\n",
    "\n",
    "Final prediction is given by the <font color='orange'>PPD</font>: $\\require{color}{\\color{orange}p(y^*|\\mathbf{x}^*, \\mathcal{D})} = \\int p(y^*|\\mathbf{x}^*,\\mathbf{z}) \\delta(\\mathbf{z}-\\hat{\\mathbf{z}}_{\\text{map}}) dz = p(y^*|\\mathbf{x}^*, \\mathbf{z}=\\hat{\\mathbf{z}}_{\\text{map}}, \\mathcal{D})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Important concepts for all ML models: parameters vs hyperparameters\n",
    "\n",
    "At this point we have to highlight something very important that we have referred to multiple times, but never used the formal nomenclature:\n",
    "\n",
    "* The difference between **parameters** and **hyperparameters**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Parameters** of an ML model: these are the unknown variables $\\mathbf{z}$, i.e. the rv's that are hidden (not explicitly visible in our data).Our goal when making a prediction from the PPD is to:\n",
    "*  Marginalize these unknown parameters $\\mathbf{z}$, if using Bayesian inference (i.e. integrating them out)  \n",
    "* **OR** estimate the value of these parameters $\\mathbf{z}$ using a Point estimate $\\hat{\\mathbf{z}}$ (if doing deterministic inference, i.e. the common machine learning models without uncertainty estimation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Hyperparameters** of an ML model: these are the variables that we are specifying or assuming for our model and that are not going to be updated after training, i.e. they do not change when we determine the PPD.\n",
    "\n",
    "* For example, for the above-mentioned Ridge regression model we defined **3 hyperparameters**:\n",
    "    * The prior parameters $\\overset{\\scriptscriptstyle <}{\\boldsymbol{\\mu}}_w$ (that we set to $\\mathbf{0}$) and $\\overset{\\scriptscriptstyle <}{\\sigma}_w$ (that we can set to any constant value $\\mathrm{cte}$)\n",
    "    * **AND** the variance of the observation distribution $\\sigma$ (that you set to whatever particular value you want)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### A note about the prior\n",
    "\n",
    "Ususally the Gaussian prior is imposed only on the weights (not on the bias term, i.e. the $w_0$ term):\n",
    "\n",
    "* This is actually very common. The bias term in the observation distribution usually has a Uniform prior because it does not contribute to overfitting.\n",
    "\n",
    "You can also see this in the previous Lecture from the expressions for $\\hat{w}_{0, \\text{mle}}$ and $\\sigma^2_{\\text{mle}}$, as they act on the global mean and MSE (mean squared error) of the residuals, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Coming back to our Ridge regression model, computing the MAP estimate is very similar to what we did in the last Lecture, leading to:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\hat{\\mathbf{w}}_{\\text{map}} &= \\underset{w}{\\mathrm{argmin}}\\left[\\frac{1}{2\\sigma^2}\\left(\\boldsymbol{\\Phi}\\mathbf{w} - \\mathbf{y}\\right)^T \\left(\\boldsymbol{\\Phi}\\mathbf{w} - \\mathbf{y}\\right) + \\frac{1}{2\\overset{\\scriptscriptstyle <}{\\sigma}_w^2}\\mathbf{w}^T\\mathbf{w}\\right] \\\\\n",
    "&= \\underset{w}{\\mathrm{argmin}}\\left[\\text{RSS}(\\mathbf{w}) + \\alpha ||\\mathbf{w}||_2^2\\right]\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where $\\alpha = \\frac{\\sigma^2}{\\overset{\\scriptscriptstyle <}{\\sigma}_w^2}$ is proportional to the strength of the prior (depends on two hyperparameters), and\n",
    "\n",
    "$||\\mathbf{w}||_2 = \\sqrt{\\sum_{m=1}^{M-1} |w_m|^2} = \\sqrt{\\mathbf{w}^T\\mathbf{w}}$\n",
    "\n",
    "is called the $l_2$ norm of the vector $\\mathbf{w}$. Thus, we are penalizing weights that become too large in magnitude. In ML literature this is usually called $l_2$ **regularization** or **weight decay**, and is very widely used.\n",
    "\n",
    "\n",
    "In Homework 4, you explore the difference between Linear Least Squares and Ridge regression, reporting on the influence of the prior strength for the latter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Then, solving the MAP first for the weights $\\mathbf{w}$, as we did for the MLE:\n",
    "\n",
    "$$\\begin{align}\n",
    "\\nabla_{\\mathbf{w}} \\left[\\frac{1}{2\\sigma^2}\\left(\\boldsymbol{\\Phi}\\mathbf{w} - \\mathbf{y}\\right)^T \\left(\\boldsymbol{\\Phi}\\mathbf{w} - \\mathbf{y}\\right) + \\frac{1}{2\\overset{\\scriptscriptstyle <}{\\sigma}_w^2}\\mathbf{w}^T\\mathbf{w}\\right] &= \\mathbf{0} \\\\\n",
    "\\nabla_{\\mathbf{w}} \\left[\\left(\\boldsymbol{\\Phi}\\mathbf{w} - \\mathbf{y}\\right)^T \\left(\\boldsymbol{\\Phi}\\mathbf{w} - \\mathbf{y}\\right) + \\alpha\\mathbf{w}^T\\mathbf{w}\\right] = \\mathbf{0}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "from which we determine the MAP estimate for the the weights $\\mathbf{w}$ as:\n",
    "\n",
    "$$\n",
    "\\hat{\\mathbf{w}}_{\\text{map}} = \\left( \\boldsymbol{\\Phi}^T\\boldsymbol{\\Phi} + \\alpha \\mathbf{I}_M \\right)^{-1} \\boldsymbol{\\Phi}^T \\mathbf{y} = \\left( \\sum_{n=1}^{N} \\boldsymbol{\\phi}(\\mathbf{x}_n)\\boldsymbol{\\phi}(\\mathbf{x}_n)^T + \\alpha \\mathbf{I}_M \\right)^{-1} \\left( \\sum_{n=1}^{N} \\boldsymbol{\\phi}(\\mathbf{x}_n) y_n \\right)\n",
    "$$\n",
    "\n",
    "Once again, this can be solved using SVD or other methods to ensure that the Moore-Penrose pseudo-inverse is calculated properly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Lasso regression: Linear regression with Gaussian likelihood, Laplace prior and posterior via Point estimate\n",
    "\n",
    "| Likelihood | Prior (on the weights)    | Posterior      | Name of the model | Book section  |\n",
    "|---        |---         |---             |---              |---            |\n",
    "| Gaussian  | Laplace    | Point estimate | Lasso regression  | 11.4  |\n",
    "\n",
    "1. Gaussian observation distribution: $p(y|\\mathbf{x}, \\mathbf{z}) = \\mathcal{N}(y| \\mu_{y|z} = \\mathbf{w}^T \\boldsymbol{\\phi}(\\mathbf{x}), \\sigma_{y|z}^2 = \\sigma^2)$\n",
    "\n",
    "where $\\mathbf{z} = \\mathbf{w}$ are all the unknown model parameters (hidden rv's), and $\\sigma$ is a hyperparameter.\n",
    "\n",
    "2. But using a **Laplace** prior for the weights $\\mathbf{w}$: $p(\\mathbf{w}) = \\prod_{m=1}^{M-1}\\text{Lap}\\left(w_m| 0, 1/\\overset{\\scriptscriptstyle <}{\\lambda}_w\\right) \\propto \\prod_{m=1}^{M-1} \\exp{\\left[ -\\overset{\\scriptscriptstyle <}{\\lambda}_w |w_m|\\right]}$\n",
    "\n",
    "3. MAP point estimate for posterior: $\\hat{\\mathbf{z}}_{\\text{map}} = \\underset{z}{\\mathrm{argmin}}\\left[-\\sum_{i=1}^{N}\\log{ p(y=y_i|\\mathbf{z})} - \\log{p(\\mathbf{w})}\\right]$\n",
    "\n",
    "Final prediction is given by the <font color='orange'>PPD</font>: ${\\color{orange}p(y^*|\\mathbf{x}^*, \\mathcal{D})} = \\int p(y^*|\\mathbf{x}^*,\\mathbf{z}) \\delta(\\mathbf{z}-\\hat{\\mathbf{z}}_{\\text{map}}) dz = p(y^*|\\mathbf{x}^*, \\mathbf{z}=\\hat{\\mathbf{z}}_{\\text{map}}, \\mathcal{D})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "#### Note about the sparsity parameter $\\overset{\\scriptscriptstyle <}{\\lambda}_w$\n",
    "\n",
    "Please note that the $\\overset{\\scriptscriptstyle <}{\\lambda}_w$ hyperparameter defining the strength of the Laplace prior is different from the $\\alpha$ hyperparameter defined in Ridge regression.\n",
    "\n",
    "#### Note about number of parameters $M$ and number of input dimensions $D$\n",
    "\n",
    "If $M=D$ the method is called Lasso, but if there are more parameters than input variables $M>D$ then it is called Group Lasso (Section 11.4.7).\n",
    "\n",
    "* The next cells describe Lasso, which introduces sparsity by making a weight associated to a particular variable tending to zero.\n",
    "\n",
    "* Group Lasso leads to a sparsity of more than one parameter that is associated to a given variable (which is an interesting way to induce sparsity in overparameterized models such as Artificial Neural Networks). It is derived in a very similar manner."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Computing the MAP estimate:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\hat{\\mathbf{w}}_{\\text{map}} &= \\underset{w}{\\mathrm{argmin}}\\left[\\left(\\boldsymbol{\\Phi}\\mathbf{w} - \\mathbf{y}\\right)^T \\left(\\boldsymbol{\\Phi}\\mathbf{w} - \\mathbf{y}\\right) + \\overset{\\scriptscriptstyle <}{\\lambda}_w||\\mathbf{w}||_1\\right]\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where $||\\mathbf{w}||_1 = \\sum_{m=1}^{M-1}|w_m|$ is called the $l_1$ norm of $\\mathbf{w}$. In ML literature this is called $l_1$ regularization. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Calculating the MAP for Lasso is not done the same way as for Ridge because the term $||\\mathbf{w}||_1$ is not differentiable whenever $w_m = 0$. In this case, the solution is found using hard- or soft-thresholding (See Section 11.4.3 in the book) because the gradient becomes a branch function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* A more important point is to see that **different types of prior distributions** introduce **different regularizations** on the weights, aleviating overfitting in a different manner.\n",
    "    - For example, in the case of Lasso, since the Laplace prior puts more density around the mean (which is zero here) than the Gaussian prior, then it has a tendency to lead the weights to zero, i.e. it introduces **sparsity** when estimating the weights via MAP. The book has a beautiful discussion about this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bayesian linear regression: Linear regression with Gaussian likelihood, Gaussian prior and Gaussian posterior (Bayesian solution)\n",
    "\n",
    "As we saw in the beginning of the Lecture, there are many more models we can define! The book covers quite a few!\n",
    "\n",
    "| Likelihood | Prior (on the weights)    | Posterior      | Name of the model | Book section  |\n",
    "|---        |---         |---             |---              |---            |\n",
    "| Gaussian  | Gaussian    | Gaussian | Bayesian linear regression   | 11.7 |\n",
    "\n",
    "1. Gaussian observation distribution (with **known** variance): $p(y|\\mathbf{x}, \\mathbf{z}) = \\mathcal{N}(y| \\mu_{y|z} = \\mathbf{w}^T \\boldsymbol{\\phi}(\\mathbf{x}), \\sigma_{y|z}^2 = \\sigma^2)$\n",
    "    \n",
    "    where $\\mathbf{z} = \\mathbf{w}$ are all the unknown model parameters (hidden rv's), and where $\\sigma$ is **not** treated as an unknown, i.e. it is a **hyperparameter**.\n",
    "\n",
    "2. Gaussian prior for the weights $\\mathbf{w}$: $p(\\mathbf{w}) = \\mathcal{N}(\\mathbf{w}| \\overset{\\scriptscriptstyle <}{\\boldsymbol{\\mu}}_w, \\overset{\\scriptscriptstyle <}{\\boldsymbol{\\Sigma}}_w)$\n",
    "\n",
    "3. Gaussian posterior (obtained from Bayes rule)\n",
    "\n",
    "Final prediction is given by the <font color='orange'>PPD</font>: $\\require{color}\n",
    "{\\color{orange}p(y^*|\\mathbf{x}^*, \\mathcal{D})} = \\int p(y^*|\\mathbf{x}^*,\\mathbf{z}) p(\\mathbf{z}|\\mathcal{D}) dz$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "At this point you may notice that we have already derived this model in Lecture 7.\n",
    "\n",
    "The only differences are that now we have multiple weights $\\mathbf{z}$, multidimensional inputs $\\mathbf{x}$ and that we allow them to have different values.\n",
    "\n",
    "Yet, the derivation is the same! We just need to bold the letters. Let's do it:\n",
    "\n",
    "The likelihood is a (multivariate) Gaussian distribution (product of MVN evaluated at each data point):\n",
    "\n",
    "$$\n",
    "p(\\mathcal{D}|\\mathbf{w}, \\sigma^2) = \\prod_{n=1}^N p(y_n | \\mathbf{w}^T\\boldsymbol{\\phi}(\\mathbf{x}_n), \\sigma^2) = \\mathcal{N}(\\mathbf{y} | \\boldsymbol{\\Phi}\\mathbf{w}, \\sigma^2 \\mathbf{I}_N)\n",
    "$$\n",
    "\n",
    "where $\\mathbf{I}_N$ is the $N\\times N$ identity matrix, as defined previously."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "To calculate the posterior, we also use the product of Gaussians rule (in Lecture 5 we also defined this rule for multivariate Gaussians!):\n",
    "\n",
    "$$\n",
    "p(\\mathbf{w}| \\boldsymbol{\\Phi}, \\mathbf{y}, \\sigma^2) \\propto \\mathcal{N}(\\mathbf{y} | \\boldsymbol{\\Phi}\\mathbf{w}, \\sigma^2 \\mathbf{I}_N)    \\mathcal{N}(\\mathbf{w}| \\overset{\\scriptscriptstyle <}{\\boldsymbol{\\mu}}_w, \\overset{\\scriptscriptstyle <}{\\boldsymbol{\\Sigma}}_w) = \\mathcal{N}(\\mathbf{w}| \\overset{\\scriptscriptstyle >}{\\boldsymbol{\\mu}}_w, \\overset{\\scriptscriptstyle >}{\\boldsymbol{\\Sigma}}_w)\n",
    "$$\n",
    "\n",
    "where the mean and covariance of the posterior are given by:\n",
    "\n",
    "$$\n",
    "\\overset{\\scriptscriptstyle >}{\\boldsymbol{\\mu}}_w = \\overset{\\scriptscriptstyle >}{\\boldsymbol{\\Sigma}}_w \\left( \\overset{\\scriptscriptstyle <}{\\boldsymbol{\\Sigma}}_w^{-1} \\overset{\\scriptscriptstyle <}{\\boldsymbol{\\mu}}_w + \\frac{1}{\\sigma^2}\\boldsymbol{\\Phi}^T\\mathbf{y}\\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\overset{\\scriptscriptstyle >}{\\boldsymbol{\\Sigma}}_w = \\left( \\overset{\\scriptscriptstyle <}{\\boldsymbol{\\Sigma}}_w^{-1} + \\frac{1}{\\sigma^2}\\boldsymbol{\\Phi}^T\\boldsymbol{\\Phi}\\right)^{-1}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Two additional notes** about this result (it's recommended to go through these notes):\n",
    "1. The above result for the posterior is similar to what we did before in Lecture 5, but it involves a little more algebra. If you are curious, I derived it for you (see **NOTE 1** below).\n",
    "2. It's easy to reduce the posterior mean for Bayesian linear regression to the MAP estimate of Ridge regression (see **NOTE 2** below)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "#### NOTE 1: Calculation of the posterior in Bayesian linear regression \n",
    "\n",
    "$$\n",
    "p(\\mathbf{w}| \\boldsymbol{\\Phi}, \\mathbf{y}, \\sigma^2) \\propto \\mathcal{N}(\\mathbf{y} | \\boldsymbol{\\Phi}\\mathbf{w}, \\sigma^2 \\mathbf{I}_N)    \\mathcal{N}(\\mathbf{w}| \\overset{\\scriptscriptstyle <}{\\boldsymbol{\\mu}}_w, \\overset{\\scriptscriptstyle <}{\\boldsymbol{\\Sigma}}_w)\n",
    "$$\n",
    "\n",
    "where we are ignoring the marginal likelihood (denominator in Bayes' rule) because we already showed many times that it is only a constant that scales the numerator to normalize it.\n",
    "\n",
    "Now, as we did several times before, calculating the posterior involves a product of Gaussian distributions in $\\mathbf{w}$. Therefore, before we calculate the posterior, we need to:\n",
    "\n",
    "- Express the likelihood in $\\mathbf{w}$, instead of $\\mathbf{y}$.\n",
    "\n",
    "We have done this before for a simpler case, and found that the likelihood written in $\\mathbf{w}$ is a (non-normalized) Gaussian:\n",
    "\n",
    "$$\n",
    "\\mathcal{N}(\\mathbf{y} | \\boldsymbol{\\Phi}\\mathbf{w}, \\sigma^2 \\mathbf{I}_N) \\propto \\mathcal{N}(\\mathbf{w} | \\boldsymbol{\\mu}_1, \\boldsymbol{\\Sigma}_1 )\n",
    "$$\n",
    "\n",
    "We can determine this mean of the likelihood $\\boldsymbol{\\mu}_1$ and its covariance matrix $\\boldsymbol{\\Sigma}_1$ by rewriting the likelihood accordingly. We do this by first considering the Gaussian likelihood in terms of $\\mathbf{y}$:\n",
    "\n",
    "$$\\mathcal{N}(\\mathbf{y} | \\boldsymbol{\\Phi}\\mathbf{w}, \\sigma^2 \\mathbf{I}_N) = \\frac{1}{\\sqrt{2\\pi \\det\\left(\\sigma^2\\mathbf{I}_N\\right)}} \\exp\\left\\{ -\\frac{1}{2}\\left(\\mathbf{y}-\\boldsymbol{\\Phi}\\mathbf{w}\\right)^T\\left(\\frac{1}{\\sigma^2}\\mathbf{I}_N\\right)\\left(\\mathbf{y}-\\boldsymbol{\\Phi}\\mathbf{w}\\right)\\right\\}$$\n",
    "\n",
    "and then by focusing on the **exponent** term only, which we can rewrite as:\n",
    "\n",
    "$$\n",
    "-\\frac{1}{2}\\left(\\mathbf{y}-\\boldsymbol{\\Phi}\\mathbf{w}\\right)^T\\left(\\frac{1}{\\sigma^2}\\mathbf{I}_N\\right)\\left(\\mathbf{y}-\\boldsymbol{\\Phi}\\mathbf{w}\\right) = -\\frac{1}{2\\sigma^2} \\left( \\mathbf{y}^T\\mathbf{y} - \\mathbf{w}^T\\boldsymbol{\\Phi}^T\\mathbf{y} - \\mathbf{y}^T\\boldsymbol{\\Phi}\\mathbf{w} + \\mathbf{w}^T\\boldsymbol{\\Phi}^T\\boldsymbol{\\Phi}\\mathbf{w} \\right)\n",
    "$$\n",
    "\n",
    "However, if we want to get a Gaussian as a function of $\\mathbf{w}$ whose mean is $\\boldsymbol{\\mu}_1$, then we need to do a \"magic\" trick by considering:\n",
    "\n",
    "$$\\mathbf{y} = \\boldsymbol{\\Phi}\\boldsymbol{\\mu}_1$$\n",
    "\n",
    "from which we can rewrite the previous exponent as:\n",
    "\n",
    "$$\\begin{align}\n",
    "-\\frac{1}{2\\sigma^2} \\left( \\mathbf{y}^T\\mathbf{y} - \\mathbf{w}^T\\boldsymbol{\\Phi}^T\\mathbf{y} - \\mathbf{y}^T\\boldsymbol{\\Phi}\\mathbf{w} + \\mathbf{w}^T\\boldsymbol{\\Phi}^T\\boldsymbol{\\Phi}\\mathbf{w} \\right) =&\n",
    "-\\frac{1}{2\\sigma^2} \\left( \\boldsymbol{\\mu}_1^T\\boldsymbol{\\Phi}^T\\boldsymbol{\\Phi}\\boldsymbol{\\mu}_1 - \\mathbf{w}^T\\boldsymbol{\\Phi}^T\\boldsymbol{\\Phi}\\boldsymbol{\\mu}_1 - \\boldsymbol{\\mu}_1^T\\boldsymbol{\\Phi}^T\\boldsymbol{\\Phi}\\mathbf{w} + \\mathbf{w}^T\\boldsymbol{\\Phi}^T\\boldsymbol{\\Phi}\\mathbf{w} \\right)\\\\\n",
    "=& -\\frac{1}{2}\\left(\\mathbf{w}-\\boldsymbol{\\mu}_1\\right)^T\\left(\\frac{1}{\\sigma^2}\\boldsymbol{\\Phi}^T\\boldsymbol{\\Phi}\\right)\\left(\\mathbf{w}-\\boldsymbol{\\mu}_1\\right)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Reaching the interesting conclusion concerning the likelihood that we were already expecting:\n",
    "\n",
    "$$\\mathcal{N}(\\mathbf{y} | \\boldsymbol{\\Phi}\\mathbf{w}, \\sigma^2 \\mathbf{I}_N) \\propto \\mathcal{N}(\\mathbf{w} | \\boldsymbol{\\mu}_1, \\boldsymbol{\\Sigma}_1 )$$\n",
    "\n",
    "where the mean and covariance are given respectively by:\n",
    "\n",
    "$\\boldsymbol{\\mu}_1 = \\left(\\boldsymbol{\\Phi}^T\\boldsymbol{\\Phi}\\right)^{-1}\\boldsymbol{\\Phi}^T\\mathbf{y} = \\boldsymbol{\\mu}_1$\n",
    "\n",
    "$\\boldsymbol{\\Sigma}_1 = \\left(\\frac{1}{\\sigma^2}\\boldsymbol{\\Phi}^T\\boldsymbol{\\Phi}\\right)^{-1}$\n",
    "\n",
    "As we said, the scaling factor does not matter because the marginal likelihood will ensure that after we multiply this likelihood by the prior it leads to a normalized pdf...\n",
    "\n",
    "- In other words, we don't even need to explicitly calculate the scaling factor of the likelihood, neither do we need to calculate the marginal likelihood. But if this bothers you, you can do it! It's basically the same calculation that we did in Lecture 5 (and other lectures) ðŸ˜‰\n",
    "\n",
    "Anyway, now that we finally rewrote the likelihood as a function of $\\mathbf{w}$ we can finally calculate the product of MVN pdf's that defines the posterior:\n",
    "\n",
    "$$\n",
    "p(\\mathbf{w}| \\boldsymbol{\\Phi}, \\mathbf{y}, \\sigma^2) \\propto \\mathcal{N}(\\mathbf{w} | \\boldsymbol{\\mu}_1, \\boldsymbol{\\Sigma}_1 )   \\mathcal{N}(\\mathbf{w}| \\overset{\\scriptscriptstyle <}{\\boldsymbol{\\mu}}_w, \\overset{\\scriptscriptstyle <}{\\boldsymbol{\\Sigma}}_w)\n",
    "$$\n",
    "\n",
    "that is obtained from the rule of products of MVNs (introduced in Lecture 5), leading to:\n",
    "\n",
    "$$\n",
    "p(\\mathbf{w}| \\boldsymbol{\\Phi}, \\mathbf{y}, \\sigma^2) \\propto \\mathcal{N}(\\mathbf{w}| \\overset{\\scriptscriptstyle >}{\\boldsymbol{\\mu}}_w, \\overset{\\scriptscriptstyle >}{\\boldsymbol{\\Sigma}}_w)\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$\\overset{\\scriptscriptstyle >}{\\boldsymbol{\\mu}}_w = \\overset{\\scriptscriptstyle >}{\\boldsymbol{\\Sigma}}_w\\left(\\boldsymbol{\\Sigma}_1^{-1}\\boldsymbol{\\mu}_1 + \\overset{\\scriptscriptstyle <}{\\boldsymbol{\\Sigma}}_w^{-1}\\overset{\\scriptscriptstyle <}{\\boldsymbol{\\mu}}_w \\right) = \\overset{\\scriptscriptstyle >}{\\boldsymbol{\\Sigma}}_w \\left( \\frac{1}{\\sigma^2}\\boldsymbol{\\Phi}^T\\boldsymbol{\\Phi} \\boldsymbol{\\mu}_1 + \\overset{\\scriptscriptstyle <}{\\boldsymbol{\\Sigma}}_w^{-1}\\overset{\\scriptscriptstyle <}{\\boldsymbol{\\mu}}_w\\right) = \\overset{\\scriptscriptstyle >}{\\boldsymbol{\\Sigma}}_w \\left( \\frac{1}{\\sigma^2}\\boldsymbol{\\Phi}^T\\mathbf{y} + \\overset{\\scriptscriptstyle <}{\\boldsymbol{\\Sigma}}_w^{-1}\\overset{\\scriptscriptstyle <}{\\boldsymbol{\\mu}}_w\\right)$\n",
    "\n",
    "$\\overset{\\scriptscriptstyle >}{\\boldsymbol{\\Sigma}}_w = \\left( \\boldsymbol{\\Sigma}_1^{-1}+\\overset{\\scriptscriptstyle <}{\\boldsymbol{\\Sigma}}_w^{-1}\\right)^{-1} = \\left( \\frac{1}{\\sigma^2}\\boldsymbol{\\Phi}^T\\boldsymbol{\\Phi} + \\overset{\\scriptscriptstyle <}{\\boldsymbol{\\Sigma}}_w^{-1} \\right)^{-1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "#### NOTE 2: On the reduction of the posterior mean for Bayesian linear regression to the MAP estimate of Ridge regression \n",
    "\n",
    "If we use a prior with zero mean $\\overset{\\scriptscriptstyle <}{\\boldsymbol{\\mu}}_w =\\mathbf{0}$ and diagonal covariance $\\overset{\\scriptscriptstyle <}{\\boldsymbol{\\Sigma}}_w = \\overset{\\scriptscriptstyle <}{\\sigma}_w^2 \\mathbf{I}_M$ then the posterior mean becomes\n",
    "\n",
    "$$\\overset{\\scriptscriptstyle >}{\\boldsymbol{\\mu}}_w = \\frac{1}{\\overset{\\scriptscriptstyle <}{\\sigma}_w^2} \\overset{\\scriptscriptstyle >}{\\boldsymbol{\\Sigma}}_w \\boldsymbol{\\Phi}^T\\mathbf{y}\n",
    "$$\n",
    "\n",
    "which is the same as the Ridge regression estimate when we define $\\alpha = \\frac{\\sigma^2}{\\overset{\\scriptscriptstyle <}{\\sigma}_w^2}$,\n",
    "\n",
    "$\\overset{\\scriptscriptstyle >}{\\boldsymbol{\\mu}}_w =\\left( \\alpha \\mathbf{I}_M  + \\boldsymbol{\\Phi}^T\\boldsymbol{\\Phi}\\right)^{-1} \\boldsymbol{\\Phi}^T \\mathbf{y}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Having determined the posterior, we can determine what we really want: the PPD.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "{\\color{orange}p(y^*|\\mathbf{x}^*, \\mathcal{D})} &= \\int p(y^*|\\mathbf{x}^*,\\mathbf{z}) p(\\mathbf{z}|\\mathcal{D}) d\\mathbf{z} \\\\\n",
    "p(y^*|\\mathbf{x}^*, \\mathcal{D}, \\sigma^2) &= \\int p(y^*|\\mathbf{x}^*,\\mathbf{w}, \\sigma^2) p(\\mathbf{w}|\\mathcal{D}) d\\mathbf{w} \\\\\n",
    "&= \\int \\mathcal{N}(y^* | \\boldsymbol{\\phi}(\\mathbf{x}^*)^T\\mathbf{w}, \\sigma^2)    \\mathcal{N}(\\mathbf{w}| \\overset{\\scriptscriptstyle >}{\\boldsymbol{\\mu}}_w, \\overset{\\scriptscriptstyle >}{\\boldsymbol{\\Sigma}}_w) d\\mathbf{w}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "We calculate this integral like we did in previous lectures, i.e. again by making explicit the observation distribution as a function of $\\mathbf{w}$ and then do the product of the two MVNs. This time, I really don't think we need to show these steps again... So, I will just skip the same procedure and give the final result:\n",
    "\n",
    "$$\n",
    "{\\color{orange}p(y^*|\\mathbf{x}^*, \\mathcal{D})} = \\mathcal{N}\\left(y^* \\mid \\boldsymbol{\\phi}(\\mathbf{x}^*)^T \\overset{\\scriptscriptstyle >}{\\boldsymbol{\\mu}}_w  \\,,\\, \\sigma^2 + \\boldsymbol{\\phi}(\\mathbf{x})^T \\overset{\\scriptscriptstyle >}{\\boldsymbol{\\Sigma}}_w \\boldsymbol{\\phi}(\\mathbf{x})\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Review the notation about integrating the PPD for multiple unknowns\n",
    "\n",
    "The book uses the following notation:\n",
    "\n",
    "$$\\begin{align}\n",
    "p(y^*|x^*, \\mathcal{D}, \\sigma^2) &= \\int p(y^*|x^*,\\mathbf{w}, \\sigma^2) p(\\mathbf{w}|\\mathcal{D}) d\\mathbf{w}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "We could be more explicit and use the following notation:\n",
    "\n",
    "$$\\begin{align}\n",
    "p(y^*|x^*, \\mathcal{D}, \\sigma^2) &= \\int p(y^*|x^*,\\mathbf{w}, \\sigma^2) p(\\mathbf{w}|\\mathcal{D}) d^{M}\\mathbf{w} \\\\\n",
    "&= \\int\\int\\cdots \\int p(y^*|x^*,\\mathbf{w}, \\sigma^2) p(\\mathbf{w}|\\mathcal{D}) dw_0 dw_1 \\cdots dw_{M-1}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where it is clear that we are integrating for all variables (not integrating to get a vector).\n",
    "\n",
    "However, despite this notation being more precise, it is also less appealing. Just make sure you realize the type of integral that we are calculating when we are finding the PPD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We conclude that the PPD of the Bayesian linear regression model is:\n",
    "\n",
    "$$\n",
    "{\\color{orange}p(y^*|\\mathbf{x}^*, \\mathcal{D})} = \\mathcal{N}\\left(y^* \\mid \\boldsymbol{\\phi}(\\mathbf{x}^*)^T \\overset{\\scriptscriptstyle >}{\\boldsymbol{\\mu}}_w  \\,,\\, \\sigma^2 + \\boldsymbol{\\phi}(\\mathbf{x}^*)^T \\overset{\\scriptscriptstyle >}{\\boldsymbol{\\Sigma}}_w \\boldsymbol{\\phi}(\\mathbf{x}^*)\\right)\n",
    "$$\n",
    "\n",
    "where the mean and covariance of the posterior are given by:\n",
    "\n",
    "$$\n",
    "\\overset{\\scriptscriptstyle >}{\\boldsymbol{\\mu}}_w = \\overset{\\scriptscriptstyle >}{\\boldsymbol{\\Sigma}}_w \\left( \\overset{\\scriptscriptstyle <}{\\boldsymbol{\\Sigma}}_w^{-1} \\overset{\\scriptscriptstyle <}{\\boldsymbol{\\mu}}_w + \\frac{1}{\\sigma^2}\\boldsymbol{\\Phi}^T\\mathbf{y}\\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\overset{\\scriptscriptstyle >}{\\boldsymbol{\\Sigma}}_w = \\left( \\overset{\\scriptscriptstyle <}{\\boldsymbol{\\Sigma}}_w^{-1} + \\frac{1}{\\sigma^2}\\boldsymbol{\\Phi}^T\\boldsymbol{\\Phi}\\right)^{-1}\n",
    "$$\n",
    "\n",
    "And where we recall the meaning of each term:\n",
    "\n",
    "* ($\\mathbf{x}$, $y$) is the point were we want to make a prediction\n",
    "* $\\boldsymbol{\\phi}(\\mathbf{x})$ is an $M\\times 1$ vector of basis functions\n",
    "* $\\overset{\\scriptscriptstyle >}{\\boldsymbol{\\mu}}_w$ is the $M\\times 1$ vector with the mean of the posterior for the weights\n",
    "* and $\\overset{\\scriptscriptstyle >}{\\boldsymbol{\\Sigma}}_w$ is the $M \\times M$ covariance matrix of the posterior for the weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "$$\n",
    "\\begin{align}\n",
    "p(y^*|\\mathbf{x}^*, \\mathcal{D}, \\sigma^2) &= \\mathcal{N}\\left(y^* \\mid \\overset{\\scriptscriptstyle >}{\\boldsymbol{\\mu}}^T_w \\boldsymbol{\\phi}(\\mathbf{x}^*) \\,,\\, \\sigma^2 + \\boldsymbol{\\phi}(\\mathbf{x}^*)^T \\overset{\\scriptscriptstyle >}{\\boldsymbol{\\Sigma}}_w \\boldsymbol{\\phi}(\\mathbf{x}^*)\\right)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "The above PPD for Bayesian linear regression is quite interesting because:\n",
    "\n",
    "1. The PPD does not depend on the weights $\\mathbf{w}$. This is in contrast with the other models we saw for linear regression that depended on point estimates, instead of following the full Bayesian treatment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "2. The variance of the PPD at a point $\\mathbf{x}^*$ after seeing $N$ data points depends on two terms: (1) the variance of the observation noise, $\\sigma^2$ that we defined to be constant; and (2) the variance in the parameters obtained by the posterior $\\overset{\\scriptscriptstyle >}{\\boldsymbol{\\Sigma}}_w$.\n",
    "\n",
    "This means that the predicted uncertainty increases when $\\mathbf{x}^*$ is located far from the training data $\\mathcal{D}$, just like we want it to be! We are less certain about points **away** from our observations (training data)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "You will see that more advanced methods such as Gaussian processes are not too different from Bayesian linear regression..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Finally, for convenience, let's write the PPD of Bayesian linear regression including all the terms to get the complete (and rather long!) expression:\n",
    "\n",
    "$$\n",
    "{\\color{orange}p(y^*|\\mathbf{x}^*, \\mathcal{D})} = \\mathcal{N}\\left(y^* \\mid \\boldsymbol{\\phi}(\\mathbf{x}^*)^T \\overset{\\scriptscriptstyle >}{\\boldsymbol{\\Sigma}}_w \\left( \\overset{\\scriptscriptstyle <}{\\boldsymbol{\\Sigma}}_w^{-1} \\overset{\\scriptscriptstyle <}{\\boldsymbol{\\mu}}_w + \\frac{1}{\\sigma^2}\\boldsymbol{\\Phi}^T\\mathbf{y}\\right) \\,,\\, \\sigma^2 + \\boldsymbol{\\phi}(\\mathbf{x}^*)^T \\overset{\\scriptscriptstyle >}{\\boldsymbol{\\Sigma}}_w \\boldsymbol{\\phi}(\\mathbf{x}^*)\\right)\n",
    "$$\n",
    "\n",
    "with the previously determined covariance: $\\overset{\\scriptscriptstyle >}{\\boldsymbol{\\Sigma}}_w = \\left( \\overset{\\scriptscriptstyle <}{\\boldsymbol{\\Sigma}}_w^{-1} + \\frac{1}{\\sigma^2}\\boldsymbol{\\Phi}^T\\boldsymbol{\\Phi}\\right)^{-1}$\n",
    "\n",
    "This long expression of the PPD is (very!) **often simplified** by considering a prior with zero mean, i.e. $\\overset{\\scriptscriptstyle <}{\\boldsymbol{\\mu}}_w = \\mathbf{0}$:\n",
    "\n",
    "$$\n",
    "{\\color{orange}p(y^*|\\mathbf{x}^*, \\mathcal{D})} = \\mathcal{N}\\left(y^* \\mid \\boldsymbol{\\phi}(\\mathbf{x}^*)^T \\overset{\\scriptscriptstyle >}{\\boldsymbol{\\Sigma}}_w \\frac{1}{\\sigma^2}\\boldsymbol{\\Phi}^T\\mathbf{y} \\,,\\, \\sigma^2 + \\boldsymbol{\\phi}(\\mathbf{x}^*)^T \\overset{\\scriptscriptstyle >}{\\boldsymbol{\\Sigma}}_w \\boldsymbol{\\phi}(\\mathbf{x}^*)\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Other linear regression models\n",
    "\n",
    "As we saw in the beginning of the Lecture, there are many more linear regression models we can define! The book covers quite a few!\n",
    "\n",
    "| Likelihood | Prior (on the weights)    | Posterior      | Name of the model | Book section  |\n",
    "|---        |---         |---             |---              |---            |\n",
    "| Gaussian  | Uniform    | Point estimate | Least Squares regression  | 11.2.2  |\n",
    "| Gaussian  | Gaussian    | Point estimate | Ridge regression   | 11.3  |\n",
    "| Gaussian  | Laplace    | Point estimate | Lasso regression  | 11.4  |\n",
    "| Gaussian  | Gaussian$\\times$Laplace    | Point estimate | Elastic net  | 11.4.8  |\n",
    "| Student-$t$  | Uniform    | Point estimate | Robust regression   | 11.6.1  |\n",
    "| Laplace  | Uniform    | Point estimate | Robust regression   | 11.6.2  |\n",
    "| Gaussian  | Gaussian    | Gaussian | Bayesian linear regression   | 11.7 |\n",
    "\n",
    "For example, the \"Elastic net\" is literally the combination of Ridge regression and Lasso by defining a prior that depends on both a Gaussian and a Laplace distribution. This model was proposed in 2005. Five years later the \"Bayesian Elastic Net\" was also proposed, where the posterior is calculated in a Bayesian way (just like what we did for Bayesian linear regression)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summary\n",
    "\n",
    "Although we only considered Linear Regression Models until now, we are starting to realize something very important: \n",
    "\n",
    "**ML models can be derived from the Bayes' rule by following the same 4 steps**!\n",
    "\n",
    "1. Define the Observation distribution and compute the likelihood.\n",
    "\n",
    "2. Define the prior and its parameters.\n",
    "\n",
    "3. Compute the posterior to estimate the unknown parameters (whether via a Bayesian approach or via a Point estimate).\n",
    "\n",
    "4. Compute the PPD.\n",
    "\n",
    "Done.\n",
    "\n",
    "If you only care about the mean of the PPD (the mean of your prediction), then you can compute it directly without even thinking about uncertainty... But then, make sure you characterize the quality of your predictions using an appropriate **error metric** and use strategies like **cross-validation**, as you will explore in Homework 4!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### See you next class\n",
    "\n",
    "Have fun!"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "hide_code_all_hidden": false,
  "kernelspec": {
   "display_name": "Python [conda env:3dasm]",
   "language": "python",
   "name": "conda-env-3dasm-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
