{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=../figures/Brown_logo.svg width=50%>\n",
    "\n",
    "# Data-Driven Design & Analyses of Structures & Materials (3dasm)\n",
    "\n",
    "## Lecture 30\n",
    "\n",
    "### Miguel A. Bessa | <a href = \"mailto: miguel_bessa@brown.edu\">miguel_bessa@brown.edu</a>  | Associate Professor\n",
    "\n",
    "### Suryanarayanan M. S. | <a href = \"mailto: s.manojsanu@tudelft.nl\">s.manojsanu@tudelft.nl</a>  | PhD Candidate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Outline for today's lecture\n",
    "\n",
    "* Recap\n",
    "* Finite differencing\n",
    "* Symbolic differentiation\n",
    "* Automatic differentiation\n",
    "\n",
    "**Reading material**: This notebook + (Section 13.3 of the book)\n",
    "\n",
    "**Other references:**\n",
    "* J. R. R. Martins & Andrew Ning, Engineering Design Optimization, 2021 - Chapter-6\n",
    "* Extras:\n",
    "    * Automatic Differentiation in Machine Learning: a Survey, https://arxiv.org/abs/1502.05767\n",
    "    * [JAX](https://jax.readthedocs.io/en/latest/notebooks/autodiff_cookbook.html) Documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Recap\n",
    "\n",
    "* Optimizers\n",
    "    * Zeroth-order [Uses only function evaluations]\n",
    "    * Gradient-based optimizers\n",
    "        * First-order & Second-order\n",
    "        * For a scalar function with vector inputs\n",
    "        \n",
    "$$ \\text{Gradient = } ||\\nabla f(\\vec{x})|| = \\Big[\\frac{\\partial f}{ \\partial x_1}, \\frac{\\partial f}{ \\partial x_2} ..., \\frac{\\partial f}{ \\partial x_n} \\Big]$$\n",
    "\n",
    "\n",
    "$$ \\text{Hessian = }H(\\vec{x}) = \\begin{bmatrix} \\frac{\\partial^2 f}{\\partial x_1^2} & \\cdots & \\frac{\\partial^2 f}{\\partial x_1 \\partial x_n} \\\\ \\vdots &  \\ddots & \\vdots \\\\ \\frac{\\partial^2 f}{\\partial x_n \\partial x_1} & \\cdots & \\frac{\\partial^2 f}{\\partial x_n^2} \\end{bmatrix} $$\n",
    "\n",
    "* Optimality criteria to identfy minima\n",
    "    * Differentiation is important for optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Numerical differentiation\n",
    "\n",
    "- Hand calculation (Analytical)\n",
    "- Finite differences \n",
    "- Symbolic differentiation \n",
    "- Automatic differentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# A. Hand calculations\n",
    "\n",
    "* Hard-code derived formula\n",
    "\n",
    "* Trusting your high-school calculus  \n",
    "\n",
    "<img align=right src=./figures/hand_rule_graph.png width=40%>\n",
    "\n",
    "* Chain-rule for derivatives\n",
    "    * ### $y = f(x)$\n",
    "    * ### $z = g(y) = g(f(x))$\n",
    "    \n",
    "$$\\frac{dz}{dx} = \\frac{dz}{dy} \\times \\frac{dy}{dx}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "**Derivatives = Sensitivities**\n",
    "\n",
    "\n",
    "<img align=center src=./figures/hand_rule_sliders.png width=40%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Slighly more complicated**\n",
    "<img align=right src=./figures/hand_with_branches.png width=50%>\n",
    "\n",
    "* ## $f: \\mathcal{R} \\rightarrow \\mathcal{R}^2$\n",
    "    * A vector valued function\n",
    "    * $\\vec{y} = [y_1, y_2]^T = f(x)$\n",
    "    \n",
    "    \n",
    "* ## $g: \\mathcal{R}^2 \\rightarrow \\mathcal{R}$\n",
    "    * A scalar valued function (we see a lot of these!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "$$\\frac{dz}{dx} = \\Big(\\frac{\\partial z}{\\partial y_1} \\times \\frac{d y_1}{dx} \\Big) + \\Big(\\frac{\\partial z}{\\partial y_2} \\times \\frac{d y_2}{dx}\\Big)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## For a one layer neural network\n",
    "\n",
    "<img align=right src=./figures/nn_hand.png width=60%>\n",
    "\n",
    "* Input = $x$\n",
    "* Linear layer ($l$)\n",
    "    * $z = Wx + b$\n",
    "    * ### $z = l(W, b, x)$\n",
    "* Non-linearity\n",
    "    * ### $y = \\sigma(z)$\n",
    "* MSE loss\n",
    "    * $L = 0.5 * (y - \\bar{y})^2$\n",
    "    * ### $L = f(y, \\bar{y})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Exercise**\n",
    "\n",
    "* Find $\\frac{\\partial L}{\\partial W}$ and $\\frac{\\partial L}{\\partial b}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Derivation:**\n",
    "\n",
    "<img align=right src=./figures/nn_hand.png width=50%>\n",
    "\n",
    "* $\\frac{\\partial L}{\\partial y} = (y-\\bar{y})$\n",
    "\n",
    "* $\\frac{dy}{dz} = \\sigma^{'}$\n",
    "\n",
    "* $\\frac{\\partial z}{\\partial W} = x$\n",
    "\n",
    "* $\\frac{\\partial z}{\\partial b} = 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img align=right src=./figures/nn_hand.png width=40%>\n",
    "\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial W} = \\frac{\\partial L}{\\partial y} \\times \\frac{dy}{dz} \\times  \\frac{\\partial z}{\\partial W}$$\n",
    "$$= (y-\\bar{y}).\\sigma^{'}.x$$\n",
    "\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial b} = \\frac{\\partial L}{\\partial y} \\times \\frac{dy}{dz} \\times  \\frac{\\partial z}{\\partial b}$$\n",
    "$$= (y-\\bar{y}).\\sigma^{'}.1$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Analytical derivatives for ML\n",
    "\n",
    "* Pros:\n",
    "    * Fast\n",
    "    * Exact\n",
    "    * No special software needed\n",
    "* Cons:\n",
    "    * *Trust* your skills\n",
    "        * error-prone\n",
    "    * Time-consuming\n",
    "    * Redundancies!\n",
    "        * $(y-\\bar{y}).\\sigma^{'}.\\{\\}$ - Common"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# B. Finite differencing\n",
    "\n",
    "* Approximate the derivative by a small change in the input\n",
    "    \n",
    "**Taylor series makes a comeback**\n",
    "* From 1D Taylor series\n",
    "\n",
    "$$f(x + h) = f(x) + h\\frac{df}{dx} + \\mathcal{O}(h^2)$$\n",
    "\n",
    "Rearranging the terms, we get:\n",
    "\n",
    "$$\\frac{df}{dx} = \\frac{f(x + h) - f(x)}{h} + \\mathcal{O}(h)$$\n",
    "\n",
    "* Comparing with the definition of derivative\n",
    "\n",
    "\n",
    "$$\\frac{df}{dx} = \\lim_{h\\to0} \\frac{f(x + h) - f(x)}{h}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**In n-dimensions**\n",
    "\n",
    "We saw that Taylor series is expanded along a direction!\n",
    "$$ f(\\vec{x} + \\alpha \\vec{p})|_{x=\\vec{x}_0} \\approx f(\\vec{x}_0) + \\alpha \\nabla f(\\vec{x}_0)^T \\vec{p}$$\n",
    "\n",
    "For finite differencing, the directions are the unit-vectors along a coordinate axis.\n",
    "\n",
    "$$\\frac{\\partial f}{ \\partial x_j} = \\frac{f(\\vec{x} + h \\hat{e}_j) - f(\\vec{x})}{h} + \\mathcal{O}(h)$$\n",
    "$$ j= 1, 2, 3, ..., n$$\n",
    "where $n$ is the dimensionality of $\\vec{x}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In 2D, it is easier to see:\n",
    "\n",
    "* If $\\vec{x}  = \\begin{bmatrix} x_1  \\\\ x_2 \\end{bmatrix} \\in \\mathcal{R}^2$\n",
    "* Then we need two derivatives to form the gradient of f i.e.\n",
    "\n",
    "    $$\\nabla f = \\begin{bmatrix} \\frac{\\partial f}{\\partial x_1} \\\\ \\frac{\\partial f}{\\partial x_2}\\end{bmatrix}$$\n",
    "    \n",
    "* We have two coordinate directions as well\n",
    "\n",
    "$$\\hat{e} = \\begin{bmatrix} 1  \\\\ 0 \\end{bmatrix} \\quad \\text{or} \\quad \\begin{bmatrix} 0  \\\\ 1 \\end{bmatrix}$$\n",
    "\n",
    "* For getting each component\n",
    "    * One finite difference is needed!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Strictly,\n",
    "* This is the forward finite difference methods\n",
    "* This are reverse and central finite difference methods as well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Finite differencing for ML\n",
    "\n",
    "**Pros:**\n",
    "* Simple to implement\n",
    "* Works for any function (black-box)\n",
    "* Used to check other methods\n",
    "\n",
    "**Cons:**\n",
    "- Not exact (depends on the choice of $h$)\n",
    "    * $h$ needs to be small\n",
    "        * for accurate gradients - Truncation error\n",
    "    * If $h$ is too small\n",
    "        * finite precision errors creep in - Roundoff error\n",
    "- Slow\n",
    "    - Requires multiple function evaluations for gradients\n",
    "        - $n$ inputs need $n$ function evaluations\n",
    "    - E.g. For a neural network\n",
    "        - We need to evaluate the function for each parameter\n",
    "        - What if we have 1 million parameters?\n",
    "        - or if the function is a simulation!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# C. Symbolic differentiation\n",
    "- Use symbolic data types to represent mathematical expressions\n",
    "- Use equations of calculus\n",
    "    - Basically what you did with hand but now automatic!\n",
    "\n",
    "<img align=right src=./figures/expression_swell.png width=40%>\n",
    "\n",
    "**Pros:**\n",
    "- Exact\n",
    "- The formula identifies problem structure!\n",
    "\n",
    "**Cons:**\n",
    "- Slow due to redundancies\n",
    "- Not scalable - expression swell\n",
    "- Incomprehensible formulae\n",
    "- Wasteful\n",
    "    - We need the gradient's value at a point and not a formula!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# D. Automatic differentiation\n",
    "\n",
    "* aka Algorithmic differentiation (or autodiff)\n",
    "* Combines the best aspects of symbolic and numerical differentiation\n",
    "\n",
    "⭐ Use on any function (like Finite Differences)\n",
    "\n",
    "⭐ Exact (like Symbolic differentiation)\n",
    "\n",
    "⭐ Fast and scalable (unlike both)\n",
    "\n",
    "⭐ Modular"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Core idea**\n",
    "* Every function is made from elementary operations (in a computer!).\n",
    "* We know the derivative of each elementary operation.\n",
    "* So, we can compute the derivative of the entire function by applying the chain rule repeatedly!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## For implementation:\n",
    "\n",
    "- A program as an **acyclic computational graph**!\n",
    "    - Variables connected by operations (as nodes)\n",
    "    - Any function can be an operation\n",
    "    - Obtained usually by tracing variables\n",
    "\n",
    "<img align=right src=./figures/comp_graph.png width=40%>\n",
    "\n",
    "- Look up the rules for each operation somewhere\n",
    "    - E.g. `def sin_derivative(x): return cos(x)`\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Two modes of automatic differentiation\n",
    "\n",
    "- Direction of traversing the graph\n",
    "\n",
    "<img align=right src=./figures/forward_vs_backward.png width=35% height=75%>\n",
    "\n",
    "- **Forward accumulation**\n",
    "    - Derivatives \"flow\" along with program execution\n",
    "    - Inputs to outputs\n",
    "- **Reverse accumulation**\n",
    "    - Derivative computation is done once the execution is over\n",
    "    - Similar to how we did by hand [Working backwards]\n",
    "    - Outputs to inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Analogy: Thinking of autodiff as a piping-system\n",
    "\n",
    "<img align=right src=./figures/pipe_ad.png width=30%>\n",
    "\n",
    "- Input channels\n",
    "- Output channels\n",
    "- Many many intermediate operations\n",
    "    - Connects the inputs to the outputs\n",
    "- Unconnected parts have no influence on one another\n",
    "    - Derivatives = 0\n",
    "- Cyclic dependencies\n",
    "    - Causes flow stagnation\n",
    "    - A big NO!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Forward-mode \n",
    "\n",
    "<img align=right src=./figures/forward_1.png width=30%>\n",
    "\n",
    "* To know the partial derivative w.r.t one of inputs i.e. $\\frac{\\partial}{\\partial x_1}$\n",
    "    * You start from that variable\n",
    "        * Pour water there\n",
    "        * aka Seeding\n",
    "    * Flow through the graph\n",
    "    * Get \"accumulated\" at the outputs\n",
    "* It does not matter how many outputs you have\n",
    "    * You get the effect of the input on all the outputs!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* If $x_1$ is seeded, we will get a vector like\n",
    "$$\\begin{bmatrix}\\frac{\\partial y_1}{\\partial x_1} \\\\ \\frac{\\partial y_2}{\\partial x_1} \\\\ \\frac{\\partial y_3}{\\partial x_1} \\\\ ... \\end{bmatrix}$$\n",
    "\n",
    "**We get the influence of $x_1$ on all outputs in one go!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Reverse-mode\n",
    "\n",
    "<img align=right src=./figures/reverse_1.png width=15%>\n",
    "\n",
    "* If you want *all* partial derivatives of *one* of the outputs!\n",
    "    * Flip the graph\n",
    "    * Seed the output you are interested in\n",
    "    * Flow through the graph\n",
    "    * This is essentially the gradient\n",
    "\n",
    "$$\\begin{bmatrix}\\frac{\\partial y_1}{\\partial x_1} & \\frac{\\partial y_1}{\\partial x_2} & \\frac{\\partial y_1}{\\partial x_3} &... \\end{bmatrix}$$\n",
    "\n",
    "* It does not matter how many inputs there are\n",
    "    * You will get the effect of all of them on a single output    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Mathematics behind autodiff\n",
    "\n",
    "* Consider a general function $f: \\mathcal{R}^n \\rightarrow \\mathcal{R}^m$\n",
    "    * Domain = $\\mathcal{R}^n$; $n$ inputs $\\begin{bmatrix}x_1 \\\\ x_2 \\\\... \\\\x_n \\end{bmatrix}$\n",
    "    * Range = $\\mathcal{R}^m$; $m$ outputs $\\begin{bmatrix}y_1 \\\\ y_2 \\\\... \\\\y_m \\end{bmatrix}$\n",
    "    * $\\vec{y} = f(\\vec{x})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Jacobians rule the ML world**\n",
    "\n",
    "* We can define the jacobian matrix ($\\mathcal{J}$) as:\n",
    "\n",
    "$$\\mathcal{J} = \\begin{bmatrix} \\frac{\\partial y_1}{\\partial x_1} & \\frac{\\partial y_1}{\\partial x_2} & ... & \\frac{\\partial y_1}{\\partial x_n} \\\\\n",
    "... & ... & ... & ...\\\\\n",
    "... & ... & ... & ...\\\\\n",
    "\\frac{\\partial y_m}{\\partial x_1} & \\frac{\\partial y_m}{\\partial x_2} & ... & \\frac{\\partial y_m}{\\partial x_n}\n",
    "\\end{bmatrix}_{\\quad m\\times n}$$\n",
    "\n",
    "* Extension of gradient!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Why jacobians matter?\n",
    "\n",
    "<img align=right src=./figures/jacfwd.png width=50%>\n",
    "\n",
    "* Forward-mode\n",
    "    * Gave us one column of the jacobian\n",
    "    * To construct the entire jacobian\n",
    "        * $n$ forward accumulation steps are needed\n",
    "        * One per input\n",
    "    * Useful when the function has more outputs than inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img align=right src=./figures/jacobian.png width=50%>\n",
    "\n",
    "* Reverse-mode\n",
    "    * Gave us one row of the jacobian\n",
    "    * To construct the entire jacobian\n",
    "        * $m$ reverse accumulation steps are needed\n",
    "        * One per output\n",
    "    * Useful when the function has more inputs than outputs\n",
    "    \n",
    "   \n",
    "**IN ML, most of the times, number of inputs (model parameters) >>>>> Number of outputs (loss value). So, Reverse-mode is better and is known as back propagation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "### Autodiff's modularity\n",
    "\n",
    "<img align=right src=./figures/modularity.png width=30%>\n",
    "\n",
    "* Each operation (or function) works independently\n",
    "* Each operation needs to propagate information\n",
    "    * Either from inputs to outputs (Upstream to downstream)\n",
    "    * Or from outputs to inputs (Downstream to upstream)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "**For forward-mode**\n",
    "\n",
    "<img align=right src=./figures/tangents_primals.png width=30%>\n",
    "\n",
    "* Propagates both values and derivatives simulataneously\n",
    "* Propagation of primals and tangents through a function\n",
    "    * Primals (Primary values)\n",
    "        * Inputs & outputs\n",
    "        * $\\vec{x}$ & $\\vec{y}$\n",
    "    * Tangents (Derivatives)\n",
    "        * $\\dot{\\vec{x}}$ (Upstream gradient) & $\\dot{\\vec{y}}$ (Downstream gradient)\n",
    "        * Geometrically, tangent to a curve = derivative!\n",
    "        * $\\mathcal{J}_f$ is the function's jacobian at the given input and output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "* The jacobian times a vector is propagated forward\n",
    "    * This is called `jacobian-vector product` or `jvp`\n",
    "    * Jacobian times an input vector = output vector\n",
    "* This means we dont have to store anything!\n",
    "    * Forward-mode is independent of the depth of the graph\n",
    "\n",
    "**For forward-mode autodiff to work, `jvp` rules have to written for all operations!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "# In pseudo-code\n",
    "# Some frameworks may not support forward-mode autodiff\n",
    "\n",
    "\n",
    "def unknown_function(x, y):\n",
    "    \"\"\" A black-box function.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x, y\n",
    "        Scalars, Arrays, or matrices of VALUES.\n",
    "        Unlike symbolic differentiation, remember that we work with numerical values!\n",
    "    \"\"\"\n",
    "    # Do something blackboxy!\n",
    "    ...\n",
    "    return z\n",
    "\n",
    "def unknown_function_jvp_rule(up_primals, up_tangents):\n",
    "    \"\"\" Tells the autodiff program how to differentiate the above function.\n",
    "\n",
    "    Briefly tell the software how to propagate the derivatives!\n",
    "    See how modular this is. User doesnot need to know the computational graph.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    up_primals\n",
    "        Upstream primals = Inputs to the original function\n",
    "        i.e. x & y\n",
    "    up_tangents\n",
    "        Upstream tangents = Derivative information accumulated in the primals\n",
    "        i.e. x_dot & y_dot\n",
    "        Imagine the flow of water!\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    down_primals\n",
    "        Primals to pass on downstream = Outputs of the function\n",
    "        z = unknown_function(x, y)\n",
    "    down_tangents\n",
    "        Downstream tangents = Derivative information incorporting the up_tangents\n",
    "        and the function's jacobian\n",
    "        z_dot = f(x_dot, y_dot, Jf)\n",
    "    \"\"\"\n",
    "    x, y = up_primals\n",
    "    x_dot, y_dot = up_tangents\n",
    "    down_primals = z = unknown_function(x, y)\n",
    "    # Compute the jacobian of the unknown_function =  Jf\n",
    "    down_tangents =  XXX # Compute the jvp [Jacobian times x_dot & y_dot]\n",
    "    return down_primals, down_tangents\n",
    "\n",
    "# Register the jvp rule\n",
    "\n",
    "# YOUR_FRAMEWORK.register_jvp(func=unknown_function,\n",
    "#                             jvp_rule=unknown_function_jvp_rule)\n",
    "\n",
    "# Now this function can be differentiated by the system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "#### Notes:\n",
    "* Upstream derivatives are denoted with \"dots\" notation i.e. $\\dot{\\vec{x}}$\n",
    "* Forward-mode also has a nice interpretation using *Dual numbers*\n",
    "    * Dual numbers are of the form $a + \\epsilon b$, where $a$ and $b$ $\\in \\mathcal{R}^n$\n",
    "    * $\\epsilon$ is a hypothetical number having the property $\\epsilon^2 = 0$ and   $\\epsilon \\neq 0$ [I know its weird]\n",
    "    * They are represented as $(a, b)$\n",
    "    * If you take the Taylor series expansion of any function at $a$ along $b$\n",
    "    $$f\\Big((a, b)\\Big) = f(a + \\epsilon b) = f(a) + \\epsilon b * f^{'}(a) + 0 + 0 + 0 + ... \\\\\n",
    "    = c + \\epsilon d = (c, d)\\\\\n",
    "    \\text{where,} \\; c = f(a) \\\\\n",
    "    d = b* f^{'}(a)$$\n",
    "* If $(a, b) = (\\vec{x}, \\dot{\\vec{x}})$\n",
    "    * For any function, $\\vec{y} = f(\\vec{x})$\n",
    "    * $f\\Big((\\vec{x}, \\dot{\\vec{x}})\\Big) = (\\vec{y}, \\dot{\\vec{y}})$\n",
    "    * Where, $\\dot{\\vec{y}} = \\mathcal{J}_f*\\dot{\\vec{x}}$ = JVP!\n",
    "    * i.e. Any function, evaluated on dual numbers, propagates outputs and derivatives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "**For Reverse-mode**\n",
    "* First, we have to go from the start to end of the graph\n",
    "    * Propagate the primals only\n",
    "* Next, we start traversing backwards\n",
    "    * Propagate the downstream derivatives [from outputs to inputs]\n",
    "    * Forward pass stores required values to help with this\n",
    "* What is needed are not `JVP` rules\n",
    "    * We need `VJP`s or `vector-jacobian` products\n",
    "    * We are \"pulling\" tangents backwards\n",
    "    * Ouput vector times the Jacobian = input vector\n",
    "* Memory scales with graph depth!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "**Notes**\n",
    "* JVP = $\\mathcal{J} \\times \\vec{v}$, where $\\vec{v}$ is from the input of the function \n",
    "* VJP = $\\vec{w} \\times \\mathcal{J}$, where $\\vec{w}$ is from the output of the function\n",
    "    *  = $\\mathcal{J}^T \\times \\vec{w}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "# In pseudo-code\n",
    "\n",
    "def unknown_function(x, y):\n",
    "    \"\"\" A black-box function.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x, y\n",
    "        Scalars, Arrays, or matrices of VALUES.\n",
    "        Unlike symbolic differentiation, remember that we work with numerical values!\n",
    "    \"\"\"\n",
    "    # Do something blackboxy!\n",
    "    ...\n",
    "    return z\n",
    "\n",
    "# VJP rule - Needs two functions unlike forward-mode!\n",
    "\n",
    "def unknown_function_forward(up_primals):\n",
    "    \"\"\"Forward pass during reverse-mode.\n",
    "\n",
    "    This is very similar to the original function except that you can store (cache)\n",
    "    values for the backward pass.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    up_primals\n",
    "        The inputs of the original function\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    down_primals\n",
    "        Output of the original function\n",
    "    stuff_to_store\n",
    "        Residual values needed for backward pass\n",
    "    \"\"\"\n",
    "\n",
    "    down_primals = z = unknown_function(x, y)\n",
    "\n",
    "    stuff_to_store = (x, y)  # People call this as residual [Leftovers from evaluating the function]\n",
    "\n",
    "    return down_primals, stuff_to_store\n",
    "\n",
    "def unknown_function_backward(stored_stuff, down_tangents):\n",
    "    \"\"\"This is executed only after the entire graph has been traversed.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    stored_stuff\n",
    "        The stuff you stored during the forward pass (A long time ago)\n",
    "    down_tangents\n",
    "        Downstream tangents (or derivatives) accumulated so far\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    up_tangents\n",
    "        The derivatives propagated through the function to its inputs\n",
    "    \"\"\"\n",
    "    x, y = stored_stuff\n",
    "    z_bar = down_tangents\n",
    "\n",
    "    # We need to propagate z_bar to get x_bar and y_bar\n",
    "    x_bar = z_bar * XXX # The vector-jacobian product [w.r.t x]\n",
    "    y_bar = z_bar * YYY # The vector-jacobian product [w.r.t y]\n",
    "\n",
    "    up_tangents = (x_bar, y_bar)\n",
    "    return up_tangents\n",
    "\n",
    "\n",
    "# Register the vjp rule\n",
    "\n",
    "# YOUR_FRAMEWORK.register_vjp(func=unknown_function,\n",
    "#                             func_forward=unknown_function_forward,\n",
    "#                             func_backward=unknown_func_backward)\n",
    "\n",
    "# Now this function can be differentiated by the system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Summary\n",
    "\n",
    "* Different methods for calculating derivatives\n",
    "    * Finite differences - Slow and Error-prone\n",
    "    * Autodiff\n",
    "* Two modes of autodiff\n",
    "    * Forward\n",
    "        * Functions with many outputs\n",
    "    * Reverse\n",
    "        * Functions with many inputs\n",
    "        * Backprop in ML\n",
    "* Thinking of autodiff\n",
    "    * Computational graphs\n",
    "    * Analogy: Pipeline, fluids flowing through them"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [conda env:3dasm]",
   "language": "python",
   "name": "conda-env-3dasm-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
