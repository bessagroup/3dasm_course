{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70972ef6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=../figures/Brown_logo.svg width=30%>\n",
    "\n",
    "# Project 1: Learning to Choose Optimizers\n",
    "\n",
    "\n",
    "### Martin van der Schelling | <a href = \"mailto: martin_van_der_schelling@brown.edu\">martin_van_der_schelling@brown.edu</a>  | PhD candidate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507e04ce",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Outline of today\n",
    "\n",
    "At the end of this lecture you have learned:\n",
    "\n",
    "* Why we need optimization algorithms\n",
    "* Why we need to make an informed decision on our optimizer choice\n",
    "* How we might get free lunch anyway!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7cd91c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Optimization problems are everywhere\n",
    "\n",
    "Optimization is the process of finding the **best solution** or outcome from a set of possible options based on a certain **criteria**.\n",
    "\n",
    "<!-- * learning goal: understand that optimization is used in machine learning but also in other disciplines: -->\n",
    "\n",
    "<img src=./img/examples_opt.png width=60%>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b371e5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Different kind of optimization problems\n",
    "\n",
    "### Time complexity\n",
    "* *\"How much does the time to solve increase when we make the problem bigger?\"*\n",
    "* Time complexity is a measure of the **amount of time an algorithm takes to run as a function of the size of the input ($n$)**\n",
    "\n",
    "* This is often expressed using **\"Big O\" notation**: $\\mathcal{O}(n)$, $\\mathcal{O}(n \\; \\mathrm{log}(n))$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44aabfb5",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Why is this important?\n",
    "* Gives us a sense if the current algorithm can solve this problem\n",
    "* It let us compare the **efficiency** of algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7438589",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=./img/time-complexity-examples.png width=40%>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1bcfdeb",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "What happens when we **scale the problem** and try to solve a optimization problem that has $\\mathcal{O}(2^n)$ ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6424750",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**We run out of memory/time to solve the problem exactly**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae51dc6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Well how do we solve those problems?\n",
    "\n",
    "ü´±üèΩ‚Äçü´≤üèª Making a compromise: Get a good enough solution within polynomial time complexity\n",
    "* Trade-off between **complexity** and **solution quality**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ec2684",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Iterative procedure\n",
    "\n",
    "<img src=./img/schematic_optimization.png width=80%>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc66818",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Step 1) Choose an **initial guess** ($\\mathbf{x}_0$)\n",
    "\n",
    "Step 2) Update your current solution with an **optimization algorithm**\n",
    "\n",
    " $\\mathbf{x}_{t+1} = \\mathbf{x}_t + \\omega$\n",
    " \n",
    "Step 3) Repeat step 2 until some **stopping criteria**\n",
    "\n",
    "Step 4) Take your optimized value and hope for the best :)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "822ce683",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## For now, you can treat the optimization as a black box\n",
    "\n",
    "*More in-depth information about 'opening the black-box' of optimization in a later lecture!*\n",
    "\n",
    "<img src=./img/opt_blackbox.png width=40%>\n",
    "\n",
    "Your next iteration $\\mathbf{x}_{t+1}$ depends on:\n",
    "* The **current solution** ($\\mathbf{x}_{t}$) and **response** ($y_{t}$) that you have\n",
    "* (Optionally) other information like **gradients** or **history of evaluations** ($\\mathbf{X}_{0 .. t}$ and $\\mathbf{y}_{0..t}$)\n",
    "* The **choice** of optimization algorithm\n",
    "* Any **hyperparameters** of the optimizer (e.g. the learning rate $\\alpha$)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "709e9165",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## There are many, many different optimization algorithms ..\n",
    "\n",
    "Each field has its own collection of optimizers: \n",
    "\n",
    "| Engineering Application | Optimization Algorithms |\n",
    "|-------------------------|--------------------------|\n",
    "| Structural design      | Genetic algorithms, simulated annealing, gradient descent |\n",
    "| Topology optimization  | Method of moving Asymptotes (MMA), Interior-point line-search (IPOPT), Optimality Criteria (OC) |\n",
    "| Process control        | Linear programming, quadratic programming, nonlinear programming |\n",
    "| Supply chain management| Linear programming, mixed integer programming |\n",
    "| Machine learning       | Stochastic gradient descent (SGD), Adam, Conjugate gradient (CG) |\n",
    "| Computer vision        | Gradient descent, stochastic gradient descent (SGD), coordinate descent |\n",
    "| Robotics               | Trajectory optimization, model predictive control |\n",
    "| Protein docking        | Monte Carlo with minimization (MCM), conformational space annealing (CSA), particle swarm optimization (PSO) |\n",
    " \n",
    "Basically, every optimizer has an $\\omega$ operation of calculating the next iterate $\\mathbf{x}_{t+1}$\n",
    "\n",
    "For example: Gradient descent: \n",
    "\n",
    "$\\mathbf{x}_{t+1} = \\mathbf{x}_{t} - \\alpha \\cdot \\frac{dy}{dx}$\n",
    "\n",
    "$\\omega = - \\alpha \\cdot \\frac{dy}{dx}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd9b511",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "But why so many? Why don't we have **one optimizer to rule them all**?\n",
    "\n",
    "<img src=./img/one_optimizer.png width=40%, aling='center'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf65d85",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## ‚ùåü•™ No Free Lunch Theorem [1]\n",
    "\n",
    "> \"Any elevated performance over one class of problems is offset by performance over another class.\"\n",
    "\n",
    "\n",
    "*[1] Wolpert, D. H., & Macready, W. G. (1997). No free lunch theorems for optimization. IEEE Transactions on Evolutionary Computation, 1(1), 67‚Äì82. https://doi.org/10.1109/4235.585893*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88586969",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In other words; some optimization algorithms work on specific problems, but might not work for others!\n",
    "\n",
    "In the limit: **no optimization algorithm will perform better than random search over the entire space of optimization problems**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f616e5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example: gradient based optimizer\n",
    "<img src=./img/Adam.gif width=40%, align='right'>\n",
    "\n",
    "\n",
    "\n",
    "<img src=./img/Sphere.png width=20%, align='left'><img src=./img/Schwefel.png width=20%, align='left'>\n",
    "\n",
    "\n",
    "* Gradient based optimizers work well in convex landscapes\n",
    "* They fail when multiple local minima are involved!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c9e6b1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Optimizers are designed to **exploit differen problem characteristics** in order to gain an advantage over random search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef08552",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How do we choose an optimizer?\n",
    "\n",
    "* Choose one based on the **knowledge you have** about your problem\n",
    "* Or you can try a bunch of them out (architecture search)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5365d9c9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "But what if we **learn our optimization choice with data?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b13c7ddd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Learning to Optimize (L2O)\n",
    "\n",
    "Adjust optimizer baseed on the response of the problem\n",
    "\n",
    "<img src=./img/l2o.png width=50%>\n",
    "\n",
    "*[2] Li, K., & Malik, J. (2017). Learning to optimize. 5th International Conference on Learning Representations, ICLR 2017 - Conference Track Proceedings.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef47296",
   "metadata": {},
   "source": [
    "From **constant** update step to **trainable model**:\n",
    "\n",
    "* 'Classic' optimizer: $\\mathbf{x}_{t+1} = \\mathbf{x}_t + \\omega$\n",
    "* Learning to otpimize: $\\mathbf{x}_{t+1} = \\mathbf{x}_t + m(\\omega ; \\phi)$"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [conda env:presentation] *",
   "language": "python",
   "name": "conda-env-presentation-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
