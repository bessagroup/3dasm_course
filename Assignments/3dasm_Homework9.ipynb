{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a03168e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1a005b0c576ca69814e142351112a584",
     "grade": false,
     "grade_id": "cell-bce01a551d8dabe6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# ENGN2350 Data-Driven Design and Analysis of Structures and Materials\n",
    "\n",
    "Coding exercises to explore the [`f3dasm`](https://f3dasm.readthedocs.io/en/latest/) package.\n",
    "\n",
    "**General instructions**:\n",
    "\n",
    "- Read the questions and answer in the cells under the \"PUT YOUR CODE IN THE CELL BELOW\" message.\n",
    "- Work through the notebook and make sure you fill in any place that says `YOUR CODE HERE` or `YOUR ANSWER HERE`. You can remove the `'raise NotImplementedError()'` code.\n",
    "- After \"END OF YOUR CODE\" , there is a cell that contains simple tests (with `assert` statements) to see if you did the exercises correctly. Not all exercises have tests! If you run the cell containing the tests and no error is given, you have succesfully solved the exercise!\n",
    "- Make sure you have the right version of `f3dasm` (1.5.4). The cell below checks this:\n",
    "\n",
    "- **ONLY WORK ON THE EXERCISE IN A JUPYTER NOTEBOOK ENVIRONMENT**\n",
    "\n",
    "> The homework assignments are generated and automatically graded by the `nbgrader` extension. If you open and save the notebook in Google Colab, metadata from Colab will be added, and the `nbgrader` metadata will be altered. As a result, `nbgrader` will be unable to automatically grade your homework. Therefore, we kindly ask students to only work on the notebook in Jupyter Notebook.\n",
    "\n",
    "- **DO NOT ADD OR REMOVE CELLS IN THE NOTEBOOK**\n",
    "\n",
    "> Most cells containing tests are set to read-only, but VS Code can bypass this restriction. Modifying or removing cells in the notebook may disrupt the `nbgrader` system, preventing automatic grading of your homework.\n",
    "\n",
    "**Instructions for handing in the homework**\n",
    "\n",
    "- Upload the Jupyter Notebook (`.ipynb file`) to canvas\n",
    "\n",
    "If there are any questions about the homework, send Martin an email (m.p.vanderschelling@tudelft.nl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61f6f0c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "842be57f6f7a2576703ef499c8ecd970",
     "grade": false,
     "grade_id": "cell-4a9d48a9aa0178c3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "We make sure you have the right version of `f3dasm` (1.5.4). The cell below checks this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf448a2",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "eb77065df8e10baff6decc95a6c1bcd3",
     "grade": false,
     "grade_id": "cell-2885211d7254cac3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import f3dasm\n",
    "import IPython\n",
    "\n",
    "# Check if the f3dasm version is correct\n",
    "assert f3dasm.__version__ == '1.5.4', \"Your version of f3dasm is incorrect, please update it.\"\n",
    "\n",
    "# Check if the IPython version is at least 3\n",
    "assert IPython.version_info[0] >= 3, \"Your version of IPython is too old, please update it.\"\n",
    "\n",
    "# If no assert statements are triggered, print a succes message\n",
    "print(\"Your environment is set up correctly :)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b15ad4f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "40ada260916a8fcb3dcb0a68252395fa",
     "grade": false,
     "grade_id": "cell-29cde56aff653320",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "You can put your name in the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b751f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e92a0ea8",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "45c2bf59ec09d762fd2d607eb147c1a4",
     "grade": false,
     "grade_id": "cell-925719e94f410246",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb91ac5-facd-4639-912d-e04c691659e9",
   "metadata": {},
   "source": [
    "## Homework 9\n",
    "\n",
    "In this homework, you will delve into optimization using the `f3dasm` package.\n",
    "\n",
    "By the end of this homework, you will:\n",
    "- Gain a deeper understanding of the strengths and weaknesses of various optimization algorithms.\n",
    "- Learn how initial conditions and hyperparameters affect optimization performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f017b625-df69-4b46-97f2-f00984fe852f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import f3dasm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91add61b-ccfe-40ca-b1da-fe9f3eb642a1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Exercise 1\n",
    "\n",
    "`f3dasm` does not have a lot of optimization algorithms built in. In order to access more optimization algorithms, you are going to install the `f3dasm_optimize` extension library:\n",
    "\n",
    "1.1 Install the `f3dasm_optimize` library. You can find the instructions on the [documentation page](https://bessagroup.github.io/f3dasm_optimize/)\n",
    "\n",
    "```\n",
    "pip install f3dasm_optimize\n",
    "```\n",
    "\n",
    "By installing this library, additional built-in default optimizers are integrated into the `f3dasm` framework. These optimizers, adapted from other libraries, are made compatible with the `ExperimentData` object. To use them, you'll also need to install the corresponding source libraries.\n",
    "\n",
    "1.2 Install the `optax` and `evosax` libraries in your environment in order to get access to gradient-based and gradient-free optimizers:\n",
    "\n",
    "```\n",
    "pip install optax evosax\n",
    "```\n",
    "\n",
    "In order to check if the optimizers are succesfully loaded into `f3dasm`, you can inspect the `f3dasm.optimization.OPTIMIZERS` list:\n",
    "\n",
    "If everything was done correctly, more optimizers like `'EvoSaxCMAES'` and`'Adam'` are added to the `OPTIMIZERS` list. [More optimizers](https://bessagroup.github.io/f3dasm_optimize/rst_doc_files/optimizers.html#) can be installed by following the instructions in the documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db513125-fe5c-4617-bc0a-80b65c0b8617",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell is to check if you installed f3dasm_optimize correctly\n",
    "try:\n",
    "    import f3dasm_optimize\n",
    "except ModuleNotFoundError:\n",
    "    assert False, \"f3dasm_optimize is not installed!\"\n",
    "\n",
    "from f3dasm.optimization import OPTIMIZERS\n",
    "\n",
    "assert 'EvoSaxCMAES' in OPTIMIZERS, \"EvoSax optimizers are not correctly installed!\"\n",
    "assert 'Adam' in OPTIMIZERS, \"Optax optimizers are not correctly installed!\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "499a17f9-c629-464e-9e84-14b8a3997d10",
   "metadata": {},
   "source": [
    "> If you didn't manage to install the `f3dasm_optimize` library, you can do Exercise 2 with the `'Nelder-Mead'` optimizer instead of the `'Adam'` optimizer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67baad43-16af-487d-b2b5-2466f48cbb85",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Exercise 2\n",
    "\n",
    "We are going to evaluate the performance of various optimizers on various benchmark functions. In order to do this, a function is given below that constructs a benchmark function with a given dimensionality and optimizes it with the requested optimizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8559a2-af3f-4362-b7c1-5d16ebc12e8f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c39ce44cc029fa0577d991d5377beb42",
     "grade": false,
     "grade_id": "cell-0928e48c04692141",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from f3dasm.design import make_nd_continuous_domain\n",
    "from f3dasm import ExperimentData\n",
    "from typing import Optional\n",
    "\n",
    "def optimize_benchmark_fn(benchmark_fn: str, optimizer: str, dimensionality: int, noise: float, seed: int, iterations: int,\n",
    "                         hyperparameters: Optional[dict] = None) -> ExperimentData:\n",
    "\n",
    "    # If no overwriting hyperparameters are given, create an empty dictionary\n",
    "    if hyperparameters is None:\n",
    "        hyperparameters = {}\n",
    "\n",
    "    # Add at least the seed as an 'hyperparameter' (this is required to enable reproducibility)\n",
    "    hyperparameters['seed'] = seed\n",
    "\n",
    "    # Create a single-objective, continuous domain with bounds [0., 1.]\n",
    "    domain = make_nd_continuous_domain([[0.,1.]]*dimensionality)\n",
    "\n",
    "    # Create an empty ExperimentData object from the domain\n",
    "    data = ExperimentData(domain=domain)\n",
    "\n",
    "    # Set the kwargs for the data generator\n",
    "    kwargs= {'scale_bounds': domain.get_bounds(), 'offset': False, 'seed': seed, 'noise': noise}\n",
    "\n",
    "    # Optimize the benchmark function with the given optimizer\n",
    "    # The x0 selection strategy is set to 'new', hence first sampling new candidate(s) as a starting point\n",
    "    data.optimize(optimizer=optimizer, data_generator=benchmark_fn, kwargs=kwargs, \n",
    "                  iterations=iterations, x0_selection='new', hyperparameters=hyperparameters)\n",
    "\n",
    "    # Return the data\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f791bd7-138b-41fc-a2dd-57d6c29c0a83",
   "metadata": {},
   "source": [
    "The function takes the following inputs:\n",
    "\n",
    "- **`benchmark_fn`**: The name of a [built-in benchmark function](https://f3dasm.readthedocs.io/en/latest/rst_doc_files/defaults.html#implemented-benchmark-functions) from `f3dasm`.\n",
    "- **`optimizer`**: The name of a [built-in optimizer](https://bessagroup.github.io/f3dasm_optimize/rst_doc_files/optimizers.html#).\n",
    "- **`dimensionality`**: The number of continuous input dimensions.\n",
    "- **`noise`**: Magnitude of Gaussian noise added to the objective value. Value denotes the fraction of the objective value that will be used as the standard deviation in the Gaussian distribution.\n",
    "- **`seed`**: The seed for generating initial candidate solutions (this is done with random uniform sampling).\n",
    "- **`iterations`**: The total number of function evaluations allowed for optimizing the function.\n",
    "- **`hyperparameters`**: _(optional)_ A dictionary to set non-default hyperparameters for the optimizer.\n",
    "\n",
    "The function tries to minimize the benchmark function $f(\\vec{x})$ with a finite budget while changing the input $\\vec{x} = (x^0, x^1, x^2 \\dots x^D)$ with $D$ the dimensionality of the search space. We constrain $\\vec{x}$ to be in a unit domain, $[0., 1.]^D$, \n",
    "\n",
    "The output of the `optimize_benchmark_fn` is an `ExperimentData` object containing the entire history of function evaluations during optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b03d02-41ff-4d8b-bae3-d361a9e58949",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "2.1 _(3 points)_ Optimize the noiseless 2D `'Sphere'` function with the `'Adam'` optimizer for $500$ iterations by using the `optimize_benchmark_fn`. Use $123$ as the random seed. \n",
    "\n",
    "Make a plot where you show the progression of the objective values (y-axis) w.r.t the number of function evaluations (x-axis).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de9fc97-b6d9-4378-96ba-0cfacb568653",
   "metadata": {},
   "source": [
    "YOUR CODE IN THE CELL BELOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb04b7ea-fb9a-41d3-932b-2b6d0005d097",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3f13cb414f4a3fae8343fdf632af4dd2",
     "grade": true,
     "grade_id": "cell-e065cf5645652534",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f9b0ff-96de-4334-93f8-cdd1fa9ba1a8",
   "metadata": {},
   "source": [
    "END OF YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f3dfce-96fb-4bc7-9b41-57b2e2003715",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "In addition to tracking how the objective function value evolves with each update step, visualizing the input parameters throughout the iterations can provide valuable insights. This visualization, referred to as the optimizer's trajectory, can be achieved by plotting it over a contour plot of the benchmark function.\n",
    "\n",
    "A **contour plot** is a two-dimensional visualization of a three-dimensional surface. In this plot, we represent constant values of the function on a 2D plane using contour lines, or \"level curves.\" Each line on the plot connects points where the function value is the same. This allows us to easily see how the function changes over a domain, highlighting areas of increase and decrease.\n",
    "\n",
    "In optimization, contour plots are especially useful for understanding the shape of the function landscape, which can reveal minima, maxima, and saddle points.\n",
    "\n",
    "We can generate a contour plot of benchmark functions using the following function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0338ad-f8f3-443a-bfec-1de33d6c417a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9a9f78ee85230d4c9791589cc2e28802",
     "grade": false,
     "grade_id": "cell-b67a712ca5f25d69",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "def plot_function(data_generator: str, resolution: int = 31):\n",
    "    # Step 1: Create the input data by creating a grid of points, equally spaced\n",
    "    x0 = np.linspace(0., 1., resolution) # Create equally spaced points between 0 and 1, x0\n",
    "    x1 = np.linspace(0., 1., resolution) # Create equally spaced points between 0 and 1, x1\n",
    "    input_data = np.array(list(product(x0, x1))) # Create a grid of points\n",
    "\n",
    "    # Step 2: Create the experiment data\n",
    "    domain = make_nd_continuous_domain([[0.,1.]]*2) # Create a 2D domain\n",
    "    experiment_data = ExperimentData(input_data=input_data, domain=domain)\n",
    "\n",
    "    # Step 3: Evaluate the grid of points on the data generator\n",
    "    experiment_data.evaluate(data_generator=data_generator, kwargs={\n",
    "        'scale_bounds': domain.get_bounds(), 'offset': False})\n",
    "\n",
    "    # Step 3.1: Retrieve the best found point in the grid, estimating the 'global minimum'\n",
    "    x_min, _ = experiment_data.get_n_best_output(1).to_numpy()  \n",
    "    \n",
    "    # Step 4: Create the contour plot\n",
    "    array_in, array_out = experiment_data.to_numpy()\n",
    "    X = array_in[:, 0].reshape(resolution, resolution) # Reshape in order to make the contour plot\n",
    "    Y = array_in[:, 1].reshape(resolution, resolution) # Reshape in order to make the contour plot\n",
    "    Z = array_out.reshape(resolution, resolution)\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    contour = ax.contour(X, Y, Z, levels=50, cmap='viridis', zorder=0)\n",
    "    ax.contourf(X, Y, Z, levels=50, cmap='viridis', alpha=0.7, zorder=0)\n",
    "    ax.scatter(x_min[:, 0], x_min[:, 1], color='k', marker='x', label='global minimum') # Display the global minimum estimate with a black cross\n",
    "    fig.colorbar(contour)\n",
    "    ax.set_xlabel(\"$x_0$\")\n",
    "    ax.set_ylabel(\"$x_1$\")\n",
    "    return fig, ax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36203bd5-e1ac-43cc-8206-fccbd5705d90",
   "metadata": {},
   "source": [
    "The function takes the following inputs:\n",
    "\n",
    "- **`benchmark_fn`**: The name of a [built-in benchmark function](https://f3dasm.readthedocs.io/en/latest/rst_doc_files/defaults.html#implemented-benchmark-functions) from `f3dasm`.\n",
    "- **`resolution`**: _(optional)_ The number of equally spaced points per dimension used for constructing the contour plot, by default $31$.\n",
    "\n",
    "The function returns a Matplotlib figure (`plt.Figure`) and axis (`plt.Axes`) object, allowing you to use the axis to overlay the optimization trajectory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f62104-fe2d-4e13-87fc-20a73ca3af72",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "2.2 _(2 points)_ Create a contour plot of the 'Sphere' function using the `plot_function` function and plot the trajectory (= 2D input dimension) of the Adam optimizer created in the previous exercise on top of this landscape.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "205bba01-bc6c-4c55-ac9e-1258814349d5",
   "metadata": {},
   "source": [
    "YOUR CODE IN THE CELL BELOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a93558-b813-41a5-a5f1-c677fa16b681",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "58498f6494b4b68d637afe5a88ca2343",
     "grade": true,
     "grade_id": "cell-c3a81ec05858fcba",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63bff1e0-cc35-4732-8bee-f972e0186d20",
   "metadata": {},
   "source": [
    "END OF YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d18c0723-3b1f-4884-8c65-d12cce1af58f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "2.3 _(3 points)_ Optimize the same benchmark function in exercise 2.1, but now use the Adam optimizer with **4 different values of the learning rate hyperparameter**: $10^{-3}$, $10^{-2}$, $10^{-1}$ and $10^{0}$. Use the same seed $123$ and run for a maximum of $500$ function evaluations.\n",
    "- Create a contour plot with the trajectories of the four Adam optimizers\n",
    "- Make one figure where you plot the objective value w.r.t. the number of function evaluations for the four runs.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d73e1dfe-d579-4ee6-a089-190681f40412",
   "metadata": {},
   "source": [
    "YOUR CODE IN THE CELL BELOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85cfbf47-e5cc-4f5c-b634-ac75e26648be",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5f302ba219617290d767b4e81684cae6",
     "grade": true,
     "grade_id": "cell-4919b68e0ab870a5",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9261adcd-efb6-4104-bee7-38b7663c0ea7",
   "metadata": {},
   "source": [
    "END OF YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d603783-a468-454d-a81f-ed7cc9093f07",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "2.4 Conceptual questions: \n",
    "\n",
    "- _(1 point)_ What does the learning rate hyperparameter control for the Adam optimizer?\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34bab82-dc0e-4375-8023-4a65966abc16",
   "metadata": {},
   "source": [
    "YOUR ANSWER IN THE CELL BELOW"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33384a82-13e6-4626-8f26-8544b2fe1bec",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "94107357cced9033bf4d6cc021594fbd",
     "grade": true,
     "grade_id": "cell-fcdda0404f28f831",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a14776-8b54-4e63-91cc-b4d86888fe6e",
   "metadata": {},
   "source": [
    "END OF YOUR ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c642182-4d65-402e-b888-c6bb33e30eea",
   "metadata": {},
   "source": [
    "---\n",
    "- _(1 point)_ What do you observe on the optimization trajectory when you change the learning rate? What do you see when we choose a learning rate that is too low or too high?\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d32cf50-62d8-474a-9d59-9ee02f8b737b",
   "metadata": {},
   "source": [
    "YOUR ANSWER IN THE CELL BELOW"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06944e72-1714-40e4-9c6d-cadf7b0ddb6a",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4646ecc498a43c35365a5804c30cf491",
     "grade": true,
     "grade_id": "cell-b9be54fad7d25a11",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31cd9ae5-0a41-43e0-b848-3a8e40b9e37e",
   "metadata": {},
   "source": [
    "END OF YOUR ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb586db6-5bc4-4e2c-b42d-881d2e8ac776",
   "metadata": {},
   "source": [
    "---\n",
    "- _(1 point)_ How can you get a suitable learning rate for the Adam optimizer on your optimization problem?\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "388af959-4f69-4b0f-be8a-b683929fe216",
   "metadata": {},
   "source": [
    "YOUR ANSWER IN THE CELL BELOW"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c67edce2-e85f-4621-879b-6685fa10844a",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f7f4ad070d39024ae7d1d5de7d8c431c",
     "grade": true,
     "grade_id": "cell-47690c142bdfda3e",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36d06cd-75a9-4dce-a85f-4aa7806a1729",
   "metadata": {},
   "source": [
    "END OF YOUR ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781eea18-8997-4e17-8464-7dfb6ad15136",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "2.5 _(2 points)_ Redo exercise 2.1 and 2.2 but know use the **gradient-free optimizer CMA-ES (Covariance Matrix Adaptation Evolution Strategy)**. Use the version implemented by the EvoSax library by using the `'ExoSaxCMAES'` built-in optimizer.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92882273-262f-4c27-9002-a9b40a47fdcf",
   "metadata": {},
   "source": [
    "YOUR CODE IN THE CELL BELOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ff0305-ed3d-4e09-8e70-9cc78e41a084",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "515f954dd14ed19689f1a9056620c535",
     "grade": true,
     "grade_id": "cell-cdaef63304ddca57",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab4e8de8-0757-47c3-812c-46481d515650",
   "metadata": {},
   "source": [
    "END OF YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5578123b-eb80-49a4-abf9-8fcec04716f4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Exercise 3\n",
    "\n",
    "Initial conditions play a significant role in the behavior and outcome of optimization algorithms. The starting point of an optimization process can influence the trajectory taken by the optimizer and determine whether it converges to a global or local minimum.\n",
    "\n",
    "In the `optimize_benchmark_fn` function, the initial starting point, $\\vec{x}_0$, is generated by sampling from a random uniform distribution. By using different `seed` values, we can alter the random number generation, creating new starting points for the optimization process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db99ec8c-6e4f-4f82-a306-fd3d18c7b828",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "3.1 _(3 points)_ Repeat exercise 2.1 but now with a few different seeds: $123$, $124$, $125$ and $126$. Create two figures:\n",
    "- Plot the objective value (y-axis) against the function evaluation count (x-axis) for each outcome in one figure.*\n",
    "- Plot all the trajectories in one contour plot.\n",
    "\n",
    "_\\* You might want to use a logarithmic scale on the objective value to observe the differences more clearly_\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52eab771-6e88-47db-b63a-890bed1cfa6a",
   "metadata": {},
   "source": [
    "YOUR CODE IN THE CELL BELOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c8a8fa-fd7b-4792-842c-f6d82b3b085f",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "26e54b833268ccc7ca31d99763679553",
     "grade": true,
     "grade_id": "cell-19c5f879e03b3c54",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30336a04-c904-4c35-85bb-16139278c196",
   "metadata": {},
   "source": [
    "END OF YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8cb462-730f-45a7-8257-b4ddbfcfd1eb",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "3.2 _(2 points)_ Repeat exercise 3.1 but now with a different benchmark function the `'Branin'` function (more information about this function can be found [here](https://www.sfu.ca/~ssurjano/branin.html))\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012e3649-d256-45eb-8c8d-89ad84a1f1d6",
   "metadata": {},
   "source": [
    "YOUR CODE IN THE CELL BELOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57614557-ca1a-4f4e-9a43-c540f8a7d5b8",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6748577888a8c2c0c4b4bd8529d7bf8b",
     "grade": true,
     "grade_id": "cell-3d58c03d9cd7ff92",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c18cce-67e0-40b5-9b0a-4f989577fdf7",
   "metadata": {},
   "source": [
    "END OF YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3151e45c-269b-4d2a-a947-49ffbd43b9c4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "3.3 Conceptual questions:\n",
    "\n",
    "- _(1 point)_ Can you explain in words what the influence is on the outcome of optimization if you are using different initial conditions on the two functions?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3042cfe-486f-4057-a934-ec89fabe12a4",
   "metadata": {},
   "source": [
    "YOUR ANSWER IN THE CELL BELOW"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6e4e65-e708-4c97-828f-24213bf762c2",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9d7296742f59ddb6ade7d01d770d9655",
     "grade": true,
     "grade_id": "cell-e7c61f6e9986ee3e",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7713c999-05fa-44bb-8452-199071e0413e",
   "metadata": {},
   "source": [
    "END OF YOUR ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f6ae882-bd7a-42be-8710-b1151ce10e9d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Exercise 4\n",
    "\n",
    "We will now evaluate the performance of various optimization algorithms using different benchmark functions. In this homework, we define the performance of an optimization algorithm as the **best objective value found** (cumulative minimum in the case of minimization problems) **after a specified number of function evaluations**. To account for the effect of initial conditions, we will report the **median result over multiple runs**, each with a different starting point $\\vec{x}_0$.\n",
    "\n",
    "We will consider default hyper-parameters for any given optimizer at this point\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd007fe-e64d-4d83-b69c-c86fe5c1c706",
   "metadata": {},
   "source": [
    "4.1 _(3 points)_ Complete the function `calc_performance` to calculate the performance value for a given optimizer and benchmark function.\n",
    "\n",
    "**Requirements:**\n",
    "- The function should call `optimize_benchmark_fn` multiple times using the provided arguments, each with a different `seed`.\n",
    "- It should compute the cumulative minimum\\* of the objective value over the number of function evaluations for each run.\n",
    "- Finally, the median value across all runs should be calculated and stored in `median_performance`, which will be returned as a one-dimensional `np.ndarray`.\n",
    "\n",
    "**Function Arguments:**\n",
    "- **`benchmark_fn`**: The name of a [built-in benchmark function](https://f3dasm.readthedocs.io/en/latest/rst_doc_files/defaults.html#implemented-benchmark-functions) from `f3dasm`.\n",
    "- **`optimizer`**: The name of a [built-in optimizer](https://bessagroup.github.io/f3dasm_optimize/rst_doc_files/optimizers.html#).\n",
    "- **`dimensionality`**: The number of continuous input dimensions.\n",
    "- **`noise`**: Magnitude of Gaussian noise added to the objective value. Value denotes the fraction of the objective value that will be used as the standard deviation in the Gaussian distribution.\n",
    "- **`iterations`**: The total number of function evaluations allowed for the optimization.\n",
    "- **`realizations`**: The number of runs performed with different initial conditions.\n",
    "- **`seed`**: The seed for generating initial candidate solutions. Successive realizations will use seeds incremented from this base, e.g., `seed=123` with `realizations=3` results in `optimize_benchmark_fn` being called with seeds $123$, $124$, and $125$.\n",
    "\n",
    "_\\* Hint: you might want to use the `numpy.minimum.accumulate` function!_ \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd70fc0b-8291-4ac5-84e5-a5b4552b1bf9",
   "metadata": {},
   "source": [
    "PUT YOUR CODE IN THE CELL BELOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ca2d41-3914-4385-9ee7-e93b881b1c9c",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "12f82b25dc9db3bc4bec56c5f7254a02",
     "grade": true,
     "grade_id": "cell-7280fe8f82f4f8ac",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def calc_performance(benchmark_fn: str, optimizer: str, dimensionality: int, noise: float, iterations: int, realizations: int, \n",
    "                     seed: int) -> np.ndarray:\n",
    "    seeds = np.arange(realizations) + seed\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return median_performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025c4a54-ec35-495e-99a5-01b3f2c424cf",
   "metadata": {},
   "source": [
    "END OF YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "261878eb-3793-4933-aec5-b325fe52b211",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "4.2 _(2 points)_ Plot the performance values with respect to the number of function evaluations for a 10D noiseless Sphere function for the `'EvoSaxCMAES'`, `'L-BFGS-B'` and `'Adam'` optimizer in one figure. Use $123$ as the seed and run for $10$ realizations. As a maximum budget, you can put $500$ function evaluations \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56834d88-cfb8-4598-9111-3ad320f4ccc9",
   "metadata": {},
   "source": [
    "YOUR CODE IN THE CELL BELOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0758c8-46d8-4dbb-bf8a-0db292adba15",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6c824fbc53ef58e64c01793cf27540f0",
     "grade": true,
     "grade_id": "cell-6a21f0562ceb8c53",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b759499-5689-4fbd-ae08-5f973df8f7f5",
   "metadata": {},
   "source": [
    "END OF YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dbde245-16b0-4493-af75-67f37dee9373",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "4.3 _(4 points)_ Repeat 4.2, but now consider the following different benchmark functions:\n",
    "- 10D Sphere function, noiseless (this is the function of exercise 4.2)\n",
    "- 2D Branin function, noiseless\n",
    "- 5D Rastrigin function, noiseless\n",
    "- 20D Rastrigin function, noisy (`noise=0.2`)\n",
    "\n",
    "Create a different figure for each of the 4 functions\n",
    "\n",
    "--- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a95877c4-54ca-45a4-bd17-a893d2a785f7",
   "metadata": {},
   "source": [
    "YOUR CODE IN THE CELL BELOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e212dd-a6e1-4cea-8573-0c4aa5b7265a",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2ed7186d1243b222b5762a022d1b033b",
     "grade": true,
     "grade_id": "cell-905b3045d7d3bfca",
     "locked": false,
     "points": 4,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b7969d-e48d-4bde-b7f8-7f1b57c6b41d",
   "metadata": {},
   "source": [
    "END OF YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1a0584-d18c-46b6-8330-a0931790faa1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "4.4 _(2 points)_ Conceptual question: \n",
    "\n",
    "- Can you reflect on the results from question 4.3 and explain why some optimizers having different performances on different benchmark functions?\n",
    "  In your answer, reflect on the influence of the input dimensionality, noisy/noiseless observations and the (absence of) local minima.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65db2713-6f53-4927-96e1-2b9b1f887981",
   "metadata": {},
   "source": [
    "YOUR ANSWER IN THE CELL BELOW"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "578a03ca-bf96-4d3f-8241-51d46a7861be",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9987f627f7ed70f8b743e0f91bd05ea2",
     "grade": true,
     "grade_id": "cell-5df693800bbf1217",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "882ad963-aec5-4eb9-8d6e-e5af504e9a65",
   "metadata": {},
   "source": [
    "END OF YOUR ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7cf5992-6906-4863-88c1-a5ddda50a946",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "End of the homework!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
