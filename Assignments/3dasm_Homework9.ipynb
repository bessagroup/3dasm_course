{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f29c19f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2aa7b7b11ddcdec4a758a8b25da72e89",
     "grade": false,
     "grade_id": "cell-bce01a551d8dabe6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# ENGN2350 Data-Driven Design and Analysis of Structures and Materials\n",
    "\n",
    "_Homeworks for fall semester 2025-2026_\n",
    "\n",
    "Coding exercises to explore the [`f3dasm`](https://f3dasm.readthedocs.io/en/latest/) package.\n",
    "\n",
    "**General instructions**:\n",
    "\n",
    "- Read the questions and answer in the cells under the \"PUT YOUR CODE IN THE CELL BELOW\" message.\n",
    "- Work through the notebook and make sure you fill in any place that says `YOUR CODE HERE` or `YOUR ANSWER HERE`. You can remove the `'raise NotImplementedError()'` code.\n",
    "- After \"END OF YOUR CODE\" , there is a cell that contains simple tests (with `assert` statements) to see if you did the exercises correctly. Not all exercises have tests! If you run the cell containing the tests and no error is given, you have succesfully solved the exercise!\n",
    "- Make sure you have the right version of `f3dasm` (2.1.0).\n",
    "\n",
    "> You can check your `f3dasm` version by running `pip show f3dasm`\n",
    "\n",
    "- **ONLY WORK ON THE EXERCISE IN A JUPYTER NOTEBOOK ENVIRONMENT**\n",
    "\n",
    "> The homework assignments are generated and automatically graded by the `nbgrader` extension. If you open and save the notebook in Google Colab, metadata from Colab will be added, and the `nbgrader` metadata will be altered. As a result, `nbgrader` will be unable to automatically grade your homework. Therefore, we kindly ask students to only work on the notebook in Jupyter Notebook.\n",
    "\n",
    "- **DO NOT ADD OR REMOVE CELLS IN THE NOTEBOOK**\n",
    "\n",
    "> Most cells containing tests are set to read-only, but VS Code can bypass this restriction. Modifying or removing cells in the notebook may disrupt the `nbgrader` system, preventing automatic grading of your homework.\n",
    "\n",
    "**Instructions for handing in the homework**\n",
    "\n",
    "- Upload the Jupyter Notebook (`.ipynb file`) to Canvas\n",
    "\n",
    "If there are any questions about the homework, send an email to Samik (samik_mukhopadhyay@brown.edu) or Elvis (elvis_alexander_aguero_vera@brown.edu)\n",
    "\n",
    "**Grading**\n",
    "\n",
    "- In each homework, you can obtain a maximum of 20 points\n",
    "- Next to each subquestion, the maximum amount of obtainable points is listed\n",
    "\n",
    "Good luck!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c450df",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "40ada260916a8fcb3dcb0a68252395fa",
     "grade": false,
     "grade_id": "cell-29cde56aff653320",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "You can put your name in the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41825f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a55bf51",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "45c2bf59ec09d762fd2d607eb147c1a4",
     "grade": false,
     "grade_id": "cell-925719e94f410246",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb91ac5-facd-4639-912d-e04c691659e9",
   "metadata": {},
   "source": [
    "## Homework 9\n",
    "\n",
    "In this homework, you will delve into optimization using the `f3dasm` package.\n",
    "\n",
    "By the end of this homework, you will:\n",
    "- Learn how to implement 3rd party libraries into the `f3dasm` framework\n",
    "- Gain a deeper understanding of the strengths and weaknesses of various optimization algorithms.\n",
    "- Learn how initial conditions and hyperparameters affect optimization performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f017b625-df69-4b46-97f2-f00984fe852f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import f3dasm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91add61b-ccfe-40ca-b1da-fe9f3eb642a1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Exercise 1\n",
    "\n",
    "`f3dasm` comes with only a few built-in functionalities - and that is intentional. Its main purpose is to provide interface classes, giving you the flexibility to implement your own algorithms and use them just like the defaults. If you’re already relying on third-party optimizers in your code, you can easily integrate them with f3dasm. In the following exercise, we’ll show you how to create an interface between `f3dasm` and a third-party implementation, so you can take advantage of the `f3dasm` interface in your workflow.\n",
    "\n",
    "In this case study, we focus on implementing the Covariance Matrix Adaptation Evolution Strategy (CMA-ES), a state-of-the-art gradient-free optimizer [[Hansen, 1996]](https://ieeexplore.ieee.org/abstract/document/542381?casa_token=t4_3OFlyxSMAAAAA:ekh3JAbGpzFA1hWkzA8jG0zyCh1AG_11EEpehwzG-CjFHqlkhV2F35SILK_rcxlHdUenAQ). Rather than coding the algorithm from scratch, we take advantage of an existing Python implementation provided by the [cmaes](https://github.com/CyberAgentAILab/cmaes) library. Our goal is to integrate this optimizer into the `f3dasm` framework.\n",
    "\n",
    "First we make sure to install the `cmaes` package from pip:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21cc0869-e898-4a0c-a870-4958af0d89f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install cmaes==0.12.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff92f87-0e45-492d-b133-1b1292d185d6",
   "metadata": {},
   "source": [
    "The README.md file of the GitHub repository gives you an idea how the library is used natively:\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "from cmaes import CMA\n",
    "\n",
    "def quadratic(x1, x2):\n",
    "    return (x1 - 3) ** 2 + (10 * (x2 + 2)) ** 2\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    optimizer = CMA(mean=np.zeros(2), sigma=1.3) # create the optimizer instance\n",
    "\n",
    "    for generation in range(50): # run for 50 iterations\n",
    "        solutions = [] # initialize empty list of solutions\n",
    "        for _ in range(optimizer.population_size): # run for each individual in the population\n",
    "            x = optimizer.ask() # ask the optimizer for the next query point\n",
    "            value = quadratic(x[0], x[1]) # evaluate the function (= data generation!)\n",
    "            solutions.append((x, value)) # add the solution to the list\n",
    "            print(f\"#{generation} {value} (x1={x[0]}, x2 = {x[1]})\")\n",
    "        optimizer.tell(solutions) # update the optimizer state by telling the newly acuired population\n",
    "```\n",
    "\n",
    "\n",
    "In order to make this compatible with `f3dasm`, we need to create class inheriting from `f3dasm.Optimizer` where we implement the `arm` and `call` method:\n",
    "\n",
    "```python\n",
    "class CMAESOptimizer(f3dasm.Optimizer):\n",
    "    def arm(self, data: ExperimentData, data_generator: DataGenerator, input_name: str, output_name: str) -> None:\n",
    "        ...\n",
    "        \n",
    "    def call(self, data: ExperimentData, n_iterations: int, **kwargs) -> ExperimentData:\n",
    "        ...\n",
    "```\n",
    "\n",
    "Notice that in order to initialize `optimizer`, we need to provide the shape of the input parameter.\n",
    "We can use the `data` argument provided by the `arm` call to to select the tunable parameter (`input_name`) in our `Domain` and inspect it's shape.\n",
    "Additionally, we can set our output parameter to optimizer (`output_name`).\n",
    "In the `call` method, we should run the CMA-ES update step iteratively. To make the code more readable, we can create a private `_step` method that only computes one iteration and call this `_step` function `n_iterations` number of times."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a615447-dafd-412c-a06f-1fcad804e92d",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "406367e6-87db-401a-a46e-0ca1f79de972",
   "metadata": {},
   "source": [
    "1.1 _(4 points)_ Fill in the missing code snippets in the `CMAESOptimizer` class below:\n",
    "\n",
    "`arm`\n",
    "- Extract the selected input parameter from the domain in the `arm` method\n",
    "- Define the bounds array (shape= (2, dimensionality)) for the `CMA` class\n",
    "- Initialize the CMA-ES optimizer\n",
    "\n",
    "`_step`\n",
    "- Evaluate the samples with the self.data_generator\n",
    "\n",
    "`call`\n",
    "- Run the optimizer for n_iterations steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f7c1297-c603-45fe-9e1c-6b5cec26136f",
   "metadata": {},
   "source": [
    "YOUR CODE IN THE CELL BELOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de76bc1f-7c8a-468c-836b-72cddc85cb7c",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3baab2f50c954e1416bc9281b6e98976",
     "grade": true,
     "grade_id": "cell-4fb3e45fb580764d",
     "locked": false,
     "points": 4,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# You might want to import some things here ..\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "from f3dasm import Optimizer, ExperimentData, DataGenerator\n",
    "from f3dasm.design import Domain\n",
    "\n",
    "class CMAESOptimizer(Optimizer):\n",
    "    def __init__(self, seed: int, sigma: float = 1.0):\n",
    "        \"\"\"\n",
    "        CMA-ES optimizer.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        seed : int\n",
    "            Random seed for reproducibility.\n",
    "        sigma : float\n",
    "            Initial standard deviation of the covariance matrix, default 1.0 \n",
    "        \"\"\"\n",
    "        self.seed = seed\n",
    "        self.sigma = sigma\n",
    "\n",
    "    def arm(self, data: ExperimentData, data_generator: DataGenerator,\n",
    "            input_name: str, output_name: str):\n",
    "        \"\"\"\n",
    "        Prepare the CMA-ES optimizer with the given data generator and parameters.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        data : ExperimentData\n",
    "            The experiment data containing the domain information.\n",
    "        data_generator : DataGenerator\n",
    "            The data generator to evaluate the samples.\n",
    "        input_name : str\n",
    "            The name of the input parameter to optimize.\n",
    "        output_name : str\n",
    "            The name of the output parameter to optimize.\n",
    "        \"\"\"\n",
    "        self.data_generator = data_generator\n",
    "        self.output_name = output_name\n",
    "        self.input_name = input_name\n",
    "\n",
    "        # Extract the selected input parameter from the domain\n",
    "        input_parameter = ...\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "        # Define the bounds array (shape= (2, dimensionality)) for CMA-ES\n",
    "        bounds = ...\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "        # Initialize the CMA-ES optimizer\n",
    "        self.optimizer = ...\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def _step(self, domain: Domain):\n",
    "        \"\"\"\n",
    "        Perform a single optimization step using CMA-ES.\n",
    "        Parameters\n",
    "        ----------\n",
    "        domain : Domain\n",
    "            The domain of the experiment.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        ExperimentData\n",
    "            The experiment data after performing one optimization step.\n",
    "        \"\"\"\n",
    "        individuals = []\n",
    "        for individual in range(self.optimizer.population_size):\n",
    "            x = self.optimizer.ask()\n",
    "            individuals.append({self.input_name: x})\n",
    "\n",
    "        experiment_data = ExperimentData(input_data=individuals, domain=domain)\n",
    "\n",
    "        # Evaluate the samples with the self.data_generator\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "        samples = [\n",
    "            (experiment_sample.input_data[self.input_name],\n",
    "          experiment_sample.output_data[self.output_name])\n",
    "          for idx, experiment_sample in experiment_data\n",
    "          ]\n",
    "        \n",
    "        self.optimizer.tell(samples)\n",
    "\n",
    "        return experiment_data\n",
    "\n",
    "    def call(self, data: ExperimentData, n_iterations: int, **kwargs):\n",
    "        \"\"\"\n",
    "        Run the CMA-ES optimization for a specified number of iterations.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        data : ExperimentData\n",
    "            The initial experiment data.\n",
    "        n_iterations : int\n",
    "            The number of optimization iterations to perform.\n",
    "        **kwargs\n",
    "            Additional keyword arguments (not used).\n",
    "        Returns\n",
    "        -------\n",
    "        ExperimentData\n",
    "            The accumulated experiment data after all optimization iterations.\n",
    "        \"\"\"\n",
    "        # Create a new ExperimentData object\n",
    "        solutions = ExperimentData(domain=data.domain)\n",
    "\n",
    "        # Run the optimizer for n_iterations steps\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        return solutions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864df664-43c7-4a43-90bb-1107b1f2973d",
   "metadata": {},
   "source": [
    "END OF YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "899b2b02-3653-4dbe-bee2-21ed49cda540",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3faf60c7-1fe8-4245-ad55-38b3f3db2d04",
   "metadata": {},
   "source": [
    "1.2 _(2 points)_ Optimize the a 10D Levy function with your CMAES optimizer for $50$ iterations within the bounds $x \\in [-10, 10]^d$. Use $123$ as the random seed and leave the initial standard deviation of the covariance matrix as $1.0$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7a76dd-f755-4b63-be0e-6e579f986a1a",
   "metadata": {},
   "source": [
    "YOUR CODE IN THE CELL BELOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63dfd39e-8cf1-40ca-be67-931e0ab86cf9",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "356c35fe6451f1c7a42ad32747833fbb",
     "grade": true,
     "grade_id": "cell-59c5a630a148cfa2",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72cb2aa-cefc-43eb-83a5-9dc915a80353",
   "metadata": {},
   "source": [
    "END OF YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad78178-4289-4983-864f-20fcf2768e74",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1789b5b-6c86-4c3d-bfb0-daac7f114a28",
   "metadata": {},
   "source": [
    "1.3 _(1 points)_ Plot the objective value versus the number of function evaluations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec52d9bd-b723-46de-a6d5-54d69df16b05",
   "metadata": {},
   "source": [
    "YOUR CODE IN THE CELL BELOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41bdc826-38f4-4601-a324-af585009fea6",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d3559fc916f86452b308f2c87123a568",
     "grade": true,
     "grade_id": "cell-0877978f7fe9d010",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d596343b-207c-4168-a1fe-51594efc5d88",
   "metadata": {},
   "source": [
    "END OF YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76871f11-9b6f-4731-918e-2c87d6c85682",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a6f82b-6c47-4053-aeb7-7251aa2d5d5b",
   "metadata": {},
   "source": [
    "1.3 _(1 point)_ Repeat exercise 1.2, but now use the built-in L-BFGS-B optimizer. Plot the objective value versus number of function evaluations for both optimizers in one plot."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513dc60c-bf5d-43ea-83e3-e4fa4eccd519",
   "metadata": {},
   "source": [
    "YOUR CODE IN THE CELL BELOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f08a4f9-23f2-489e-94bb-e7deb65c5dc5",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "21de4fe25d16abc0a48595bafe107e56",
     "grade": true,
     "grade_id": "cell-2b46289cc58fc2ab",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3bf87fb-db9e-4fa7-9904-084cf4e7eeab",
   "metadata": {},
   "source": [
    "END OF YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67baad43-16af-487d-b2b5-2466f48cbb85",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Exercise 2\n",
    "\n",
    "Now we are going to compare different optimization techniques on different benchmark functions. You have implemented the gradient-free optimizer CMAES in the previous exercise. The other optimizer is the gradient-descent optimizer Adam. In the code block below, I have implemented this optimizer with numpy. The gradients are calculated using finiste differences:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36981e24-dd2a-4136-ba59-086f27e6cacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finite difference gradient\n",
    "def numerical_grad(f, x, eps=1e-8):\n",
    "    grad = np.zeros_like(x)\n",
    "    for i in range(len(x)):\n",
    "        x_pos = x.copy()\n",
    "        x_neg = x.copy()\n",
    "        x_pos[i] += eps\n",
    "        x_neg[i] -= eps\n",
    "        grad[i] = (f(x_pos) - f(x_neg)) / (2 * eps)\n",
    "    return grad\n",
    "\n",
    "class AdamOptimizer(Optimizer):\n",
    "    def __init__(self, learning_rate: float = 1e-3, beta1: float = 0.9, beta2 = 0.999, eps: float = 1e-8):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.eps = eps\n",
    "\n",
    "    def arm(self, data: ExperimentData, data_generator: DataGenerator,\n",
    "            input_name: str, output_name: str):\n",
    "        self.data_generator = data_generator\n",
    "        self.input_name = input_name\n",
    "        self.output_name = output_name\n",
    "\n",
    "        # Extract the last ExperimentSample as the starting point of optimization\n",
    "        experiment_sample = data.get_experiment_sample(data.index[-1])\n",
    "        self.x = experiment_sample.input_data[input_name]\n",
    "\n",
    "        # Initialize the momentum term and the second moment estimate\n",
    "        self.m = np.zeros_like(self.x)\n",
    "        self.v = np.zeros_like(self.x)\n",
    "\n",
    "        # Initialize the timestep\n",
    "        self.t = 1\n",
    "\n",
    "    def _step(self, domain: Domain):\n",
    "        g = numerical_grad(self.data_generator.f, self.x)  # finite-difference gradient\n",
    "        \n",
    "        # Update biased first moment estimate\n",
    "        self.m = self.beta1 * self.m + (1 - self.beta1) * g\n",
    "        \n",
    "        # Update biased second raw moment estimate\n",
    "        self.v = self.beta2 * self.v + (1 - self.beta2) * (g**2)\n",
    "        \n",
    "        # Bias correction\n",
    "        m_hat = self.m / (1 - self.beta1**self.t)\n",
    "        v_hat = self.v / (1 - self.beta2**self.t)\n",
    "        \n",
    "        # Update parameters\n",
    "        self.x -= self.learning_rate * m_hat / (np.sqrt(v_hat) + self.eps)\n",
    "\n",
    "        # Update the timestep\n",
    "        self.t += 1\n",
    "\n",
    "        experiment_data = ExperimentData(input_data=[{self.input_name: self.x}], domain=domain)\n",
    "        self.data_generator.arm(experiment_data)\n",
    "        experiment_data = self.data_generator.call(experiment_data)\n",
    "\n",
    "        return experiment_data\n",
    "    \n",
    "    def call(self, data: ExperimentData, n_iterations: int, **kwargs):\n",
    "        \"\"\"\n",
    "        Run the CMA-ES optimization for a specified number of iterations.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        data : ExperimentData\n",
    "            The initial experiment data.\n",
    "        n_iterations : int\n",
    "            The number of optimization iterations to perform.\n",
    "        **kwargs\n",
    "            Additional keyword arguments (not used).\n",
    "        Returns\n",
    "        -------\n",
    "        ExperimentData\n",
    "            The accumulated experiment data after all optimization iterations.\n",
    "        \"\"\"\n",
    "        # Create a new ExperimentData object\n",
    "        solutions = ExperimentData(domain=data.domain)\n",
    "\n",
    "        # Run the optimizer for n_iterations steps\n",
    "        for epoch in range(n_iterations):\n",
    "            solutions += self._step(data.domain)\n",
    "        return solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06cb9d78-a07c-4c72-a6b2-0b553a78b726",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6921a349-355c-462c-8de4-dadd031d4e45",
   "metadata": {},
   "source": [
    "We are going to evaluate the performance of twi optimizers on various benchmark functions. In order to do this, we construct a function that initializes a benchmark function with a given dimensionality and optimizes it with the requested optimizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8559a2-af3f-4362-b7c1-5d16ebc12e8f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "be9a6ab80721365a0194625f7741ee40",
     "grade": false,
     "grade_id": "cell-0928e48c04692141",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "from f3dasm import create_sampler\n",
    "\n",
    "def optimize_benchmark_fn(benchmark_fn: str, optimizer: str, dimensionality: int, seed: int, n_iterations: int,\n",
    "                          lower_bound: float = 0.0, upper_bound: float = 1.0,\n",
    "                          hyperparameters: Optional[dict] = None) -> ExperimentData:\n",
    "\n",
    "    # If no overwriting hyperparameters are given, create an empty dictionary\n",
    "    if hyperparameters is None:\n",
    "        hyperparameters = {}\n",
    "\n",
    "    # Create a single-objective, continuous domain with bounds [0., 1.]\n",
    "    domain = Domain()\n",
    "    domain.add_array(name='x', shape=(dimensionality,), low=lower_bound, high=upper_bound)\n",
    "    domain.add_output(name='y')\n",
    "\n",
    "    # Create an empty ExperimentData object from the domain\n",
    "    data = ExperimentData(domain=domain)\n",
    "\n",
    "    # Randomly sample an initial solution\n",
    "    sampler = create_sampler('random', seed=seed)\n",
    "    data = sampler.call(data, n_samples=1)\n",
    "\n",
    "    # Create the benchmark function\n",
    "    data_generator = create_datagenerator(benchmark_fn, output_names=['y'])\n",
    "\n",
    "    # Evaluate the sample\n",
    "    data = data_generator.call(data)\n",
    "\n",
    "    # Create the optimizer\n",
    "\n",
    "    if optimizer.lower() == 'adam':\n",
    "        optimizer = AdamOptimizer(**hyperparameters)\n",
    "    elif optimizer.lower() == 'cmaes':\n",
    "        optimizer = CMAESOptimizer(**hyperparameters, seed=seed)\n",
    "    else:\n",
    "        optimizer = create_optimizer(optimizer, **hyperparameters)\n",
    "        \n",
    "    # optimizer = create_optimizer(optimizer, **hyperparameters, seed=seed)\n",
    "    optimizer.arm(data, data_generator, input_name='x', output_name='y')\n",
    "\n",
    "    # Optimize the function\n",
    "    data += optimizer.call(data=data, n_iterations=n_iterations)\n",
    "\n",
    "    # Return the data\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f791bd7-138b-41fc-a2dd-57d6c29c0a83",
   "metadata": {},
   "source": [
    "The function takes the following inputs:\n",
    "\n",
    "- **`benchmark_fn`**: The name of a [built-in benchmark function](https://f3dasm.readthedocs.io/en/latest/rst_doc_files/defaults.html#implemented-benchmark-functions) from `f3dasm`.\n",
    "- **`optimizer`**: The name of a [built-in optimizer](https://f3dasm.readthedocs.io/en/latest/rst_doc_files/defaults.html#implemented-optimizers).\n",
    "- **`dimensionality`**: The number of continuous input dimensions.\n",
    "- **`seed`**: The seed for generating initial candidate solutions (this is done with random uniform sampling).\n",
    "- **`n_iterations`**: The total number of function evaluations allowed for optimizing the function.\n",
    "- **`lower_bound`**: The lower box-constraint of the feasible area, by default $0.0$\n",
    "- **`upper_bound`**: The upper box-constraint of the feasible area, by default $1.0$\n",
    "- **`hyperparameters`**: _(optional)_ A dictionary to set non-default hyperparameters for the optimizer.\n",
    "\n",
    "The function tries to minimize the benchmark function $f(\\vec{x})$ with a finite budget while changing the input $\\vec{x} = (x^0, x^1, x^2 \\dots x^D)$ with $D$ the dimensionality of the search space. We constrain $\\vec{x}$ to be in the domain, $[low, high]^D$, \n",
    "\n",
    "The output of the `optimize_benchmark_fn` is an `ExperimentData` object containing the entire history of function evaluations during optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b03d02-41ff-4d8b-bae3-d361a9e58949",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "2.1 _(1 points)_ Optimize the noiseless 2D `'Sphere'` function with the `'Adam'` optimizer with a learning rate of $10^{-3}$ for $500$ iterations by using the `optimize_benchmark_fn` in the domain $\\vec{x} \\in [-1.0, 1.0]^D$. Use $123$ as the random seed. \n",
    "\n",
    "Make a plot where you show the progression of the objective values (y-axis) w.r.t the number of function evaluations (x-axis).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de9fc97-b6d9-4378-96ba-0cfacb568653",
   "metadata": {},
   "source": [
    "YOUR CODE IN THE CELL BELOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb04b7ea-fb9a-41d3-932b-2b6d0005d097",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "97601e5af98f095a67f2be30004b4559",
     "grade": true,
     "grade_id": "cell-e065cf5645652534",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f9b0ff-96de-4334-93f8-cdd1fa9ba1a8",
   "metadata": {},
   "source": [
    "END OF YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f3dfce-96fb-4bc7-9b41-57b2e2003715",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "In addition to tracking how the objective function value evolves with each update step, visualizing the input parameters throughout the iterations can provide valuable insights. This visualization, referred to as the optimizer's trajectory, can be achieved by plotting it over a contour plot of the benchmark function.\n",
    "\n",
    "A **contour plot** is a two-dimensional visualization of a three-dimensional surface. In this plot, we represent constant values of the function on a 2D plane using contour lines, or \"level curves.\" Each line on the plot connects points where the function value is the same. This allows us to easily see how the function changes over a domain, highlighting areas of increase and decrease.\n",
    "\n",
    "In optimization, contour plots are especially useful for understanding the shape of the function landscape, which can reveal minima, maxima, and saddle points.\n",
    "\n",
    "We can generate a contour plot of benchmark functions using the following function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0338ad-f8f3-443a-bfec-1de33d6c417a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d748c1d385f2bbb56ffbf089c1fad1c9",
     "grade": false,
     "grade_id": "cell-b67a712ca5f25d69",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "def plot_function(data_generator: str, resolution: int = 31, lower_bound: float = 0.0, upper_bound: float = 1.0):\n",
    "    # Step 1: Create the input data by creating a grid of points, equally spaced\n",
    "    points = np.linspace(lower_bound, upper_bound, resolution)\n",
    "    \n",
    "    # 2D grid (meshgrid)\n",
    "    x, y = np.meshgrid(points, points)\n",
    "    grid = np.column_stack([x.ravel(), y.ravel()])\n",
    "    input_data = [{'x': xi} for xi in grid]\n",
    "    \n",
    "    # Step 2: Create the experiment data\n",
    "    domain = Domain()\n",
    "    domain.add_array('x', shape=(2,), low=lower_bound, high=upper_bound) \n",
    "    experiment_data = ExperimentData(input_data=input_data, domain=domain)\n",
    "    \n",
    "    # Step 3: Evaluate the grid of points on the data generator\n",
    "    data_generator = create_datagenerator(data_generator, output_names='y')\n",
    "    experiment_data = data_generator.call(data=experiment_data)\n",
    "    \n",
    "    # Step 3.1: Retrieve the best found point in the grid, estimating the 'global minimum'\n",
    "    x_min = np.array([es.input_data['x'] for _, es in experiment_data.get_n_best_output(1)])\n",
    "    \n",
    "    array_in = np.stack([es.input_data['x'] for _, es in experiment_data])\n",
    "    array_out = np.stack([es.output_data['y'] for _, es in experiment_data])\n",
    "    \n",
    "    # Step 4: Create the contour plot\n",
    "    X = array_in[:, 0].reshape(resolution, resolution) # Reshape in order to make the contour plot\n",
    "    Y = array_in[:, 1].reshape(resolution, resolution) # Reshape in order to make the contour plot\n",
    "    Z = array_out.reshape(resolution, resolution)\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    contour = ax.contour(X, Y, Z, levels=50, cmap='viridis', zorder=0)\n",
    "    ax.contourf(X, Y, Z, levels=50, cmap='viridis', alpha=0.7, zorder=0)\n",
    "    ax.scatter(x_min[:, 0], x_min[:, 1], color='k', marker='x', label='global minimum') # Display the global minimum estimate with a black cross\n",
    "    fig.colorbar(contour)\n",
    "    ax.set_xlabel(\"$x_0$\")\n",
    "    ax.set_ylabel(\"$x_1$\")\n",
    "    return fig, ax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36203bd5-e1ac-43cc-8206-fccbd5705d90",
   "metadata": {},
   "source": [
    "The function takes the following inputs:\n",
    "\n",
    "- **`benchmark_fn`**: The name of a [built-in benchmark function](https://f3dasm.readthedocs.io/en/latest/rst_doc_files/defaults.html#implemented-benchmark-functions) from `f3dasm`.\n",
    "- **`resolution`**: _(optional)_ The number of equally spaced points per dimension used for constructing the contour plot, by default $31$.\n",
    "- **`lower_bound`**: The lower box-constraint of the feasible area, by default $0.0$\n",
    "- **`upper_bound`**: The upper box-constraint of the feasible area, by default $1.0$\n",
    "\n",
    "The function returns a Matplotlib figure (`plt.Figure`) and axis (`plt.Axes`) object, allowing you to use the axis to overlay the optimization trajectory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f62104-fe2d-4e13-87fc-20a73ca3af72",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "2.2 _(1 points)_ Create a contour plot of the 'Sphere' function using the `plot_function` function and plot the trajectory (= 2D input dimension) of the Adam optimizer created in the previous exercise on top of this landscape.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "205bba01-bc6c-4c55-ac9e-1258814349d5",
   "metadata": {},
   "source": [
    "YOUR CODE IN THE CELL BELOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a93558-b813-41a5-a5f1-c677fa16b681",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6cae9643f4e10e9f9b84af4a6b41a467",
     "grade": true,
     "grade_id": "cell-c3a81ec05858fcba",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63bff1e0-cc35-4732-8bee-f972e0186d20",
   "metadata": {},
   "source": [
    "END OF YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d18c0723-3b1f-4884-8c65-d12cce1af58f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "2.3 _(2 points)_ Optimize the same benchmark function in exercise 2.1, but now use the Adam optimizer with **4 different values of the learning rate hyperparameter**: $10^{-3}$, $10^{-2}$, $10^{-1}$ and $10^{0}$. Use the same seed $123$ and run for a maximum of $500$ function evaluations.\n",
    "- Create a contour plot with the trajectories of the four Adam optimizers\n",
    "- Make one figure where you plot the objective value w.r.t. the number of function evaluations for the four runs.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d73e1dfe-d579-4ee6-a089-190681f40412",
   "metadata": {},
   "source": [
    "YOUR CODE IN THE CELL BELOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85cfbf47-e5cc-4f5c-b634-ac75e26648be",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "81430ace5cdc7cf2edcf69cd8553bfb8",
     "grade": true,
     "grade_id": "cell-4919b68e0ab870a5",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9261adcd-efb6-4104-bee7-38b7663c0ea7",
   "metadata": {},
   "source": [
    "END OF YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d603783-a468-454d-a81f-ed7cc9093f07",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "2.4 _(1 point)_ Answer the following conceptual question:\n",
    "\n",
    "- What does the learning rate hyperparameter control for the Adam optimizer?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34bab82-dc0e-4375-8023-4a65966abc16",
   "metadata": {},
   "source": [
    "YOUR ANSWER IN THE CELL BELOW"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33384a82-13e6-4626-8f26-8544b2fe1bec",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "94107357cced9033bf4d6cc021594fbd",
     "grade": true,
     "grade_id": "cell-fcdda0404f28f831",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a14776-8b54-4e63-91cc-b4d86888fe6e",
   "metadata": {},
   "source": [
    "END OF YOUR ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c642182-4d65-402e-b888-c6bb33e30eea",
   "metadata": {},
   "source": [
    "---\n",
    "2.5 _(1 point)_ Answer the following conceptual questions:\n",
    "\n",
    "- What do you observe on the optimization trajectory when you change the learning rate? What do you see when we choose a learning rate that is too low or too high?\n",
    "- How can you get a suitable learning rate for the Adam optimizer on your optimization problem?\\\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d32cf50-62d8-474a-9d59-9ee02f8b737b",
   "metadata": {},
   "source": [
    "YOUR ANSWER IN THE CELL BELOW"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06944e72-1714-40e4-9c6d-cadf7b0ddb6a",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4646ecc498a43c35365a5804c30cf491",
     "grade": true,
     "grade_id": "cell-b9be54fad7d25a11",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31cd9ae5-0a41-43e0-b848-3a8e40b9e37e",
   "metadata": {},
   "source": [
    "END OF YOUR ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781eea18-8997-4e17-8464-7dfb6ad15136",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "2.5 _(2 points)_ Redo exercise 2.1 and 2.2 but know use the **gradient-free optimizer CMA-ES (Covariance Matrix Adaptation Evolution Strategy)**. Use the version implemented in exercise 1.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92882273-262f-4c27-9002-a9b40a47fdcf",
   "metadata": {},
   "source": [
    "YOUR CODE IN THE CELL BELOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ff0305-ed3d-4e09-8e70-9cc78e41a084",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "515f954dd14ed19689f1a9056620c535",
     "grade": true,
     "grade_id": "cell-cdaef63304ddca57",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab4e8de8-0757-47c3-812c-46481d515650",
   "metadata": {},
   "source": [
    "END OF YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5578123b-eb80-49a4-abf9-8fcec04716f4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Exercise 3\n",
    "\n",
    "Initial conditions play a significant role in the behavior and outcome of optimization algorithms. The starting point of an optimization process can influence the trajectory taken by the optimizer and determine whether it converges to a global or local minimum.\n",
    "\n",
    "In the `optimize_benchmark_fn` function, the initial starting point, $\\vec{x}_0$, is generated by sampling from a random uniform distribution. By using different `seed` values, we can alter the random number generation, creating new starting points for the optimization process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db99ec8c-6e4f-4f82-a306-fd3d18c7b828",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "3.1 _(1 points)_ Repeat exercise 2.1 but now with a few different seeds: $123$, $124$, $125$ and $126$. Create two figures:\n",
    "- Plot the objective value (y-axis) against the function evaluation count (x-axis) for each outcome in one figure.*\n",
    "- Plot all the trajectories in one contour plot.\n",
    "\n",
    "_\\* You might want to use a logarithmic scale on the objective value to observe the differences more clearly_\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52eab771-6e88-47db-b63a-890bed1cfa6a",
   "metadata": {},
   "source": [
    "YOUR CODE IN THE CELL BELOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c8a8fa-fd7b-4792-842c-f6d82b3b085f",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4c67e2d9367882c481669728f3b51e34",
     "grade": true,
     "grade_id": "cell-19c5f879e03b3c54",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30336a04-c904-4c35-85bb-16139278c196",
   "metadata": {},
   "source": [
    "END OF YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8cb462-730f-45a7-8257-b4ddbfcfd1eb",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "3.2 _(1 point)_ Repeat exercise 3.1 but now with a different benchmark function the `'Branin'` for $2000$ iterations and $\\vec{x} \\in [-5.0, 15.0]$ function (more information about this function can be found [here](https://www.sfu.ca/~ssurjano/branin.html))\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012e3649-d256-45eb-8c8d-89ad84a1f1d6",
   "metadata": {},
   "source": [
    "YOUR CODE IN THE CELL BELOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57614557-ca1a-4f4e-9a43-c540f8a7d5b8",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6dbef811be0ea9f4040381804f2d7c47",
     "grade": true,
     "grade_id": "cell-3d58c03d9cd7ff92",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c18cce-67e0-40b5-9b0a-4f989577fdf7",
   "metadata": {},
   "source": [
    "END OF YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d134aae5-527b-45a4-9a3f-185b87384f1d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "3.3 _(1 point)_ Repeat exercise 3.2 but now with the CMAES optimizer. Make sure you divide the number of iterations by the population size, since one iterations of CMAES creates multiple function evaluations!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d567e2-d2a6-45c4-8116-08eb097598f6",
   "metadata": {},
   "source": [
    "YOUR CODE IN THE CELL BELOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb08d40c-c0b0-43b4-ae95-1602fb0b9c21",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b97841aff7b7377f66cff652929366a3",
     "grade": true,
     "grade_id": "cell-c51d3a527dc87ae0",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa0e159-2347-46f4-bdcc-14bc1244ff82",
   "metadata": {},
   "source": [
    "END OF YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3151e45c-269b-4d2a-a947-49ffbd43b9c4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "3.3 _(1 point)_ Answer the following conceptual question:\n",
    "\n",
    "- Can you explain in words what the influence is on the outcome of optimization if you are using different initial conditions on the two functions?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3042cfe-486f-4057-a934-ec89fabe12a4",
   "metadata": {},
   "source": [
    "YOUR ANSWER IN THE CELL BELOW"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6e4e65-e708-4c97-828f-24213bf762c2",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9d7296742f59ddb6ade7d01d770d9655",
     "grade": true,
     "grade_id": "cell-e7c61f6e9986ee3e",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14193905-8f1b-4d12-b0cd-07fba7b49b36",
   "metadata": {},
   "source": [
    "END OF YOUR ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a46493-2605-4e4a-a2b1-4f201e4cdad7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "End of the homework!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
